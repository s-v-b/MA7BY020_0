[
  {
    "objectID": "labs/lab-r-intro.html#packages",
    "href": "labs/lab-r-intro.html#packages",
    "title": "R language: a tour",
    "section": "Packages",
    "text": "Packages\nBase R can do a lot. But the full power of R comes from a fast growing collection of packages.\nPackages are first installed (that is downloaded from cran and copied somewhere on the hard drive), and if needed, loaded during a session.\n\nInstallation can usually be performed using command install.packages(). In some circumstances, ad hoc installation commands (often from packages devtools) are needed\nPackage pak offers an insteresting alternative to base R install.packages()\n\nOnce a package has been installed/downloaded on your drive\n\nif you want all objects exported by the package to be available in your session, you should load the package, using library() or require() (what’s the difference?). Technically, this loads the NameSpace defined by the package.\nif you just want to pick some objects exported from the package, you can use qualified names like package_name::object_name to access the object (function, dataset, …).\n\n\n\nFor example. when we write\ngapminder &lt;- gapminder::gapminder\nwe assign dataframe/tibble gapminder from package gapminder to identifier \"gapminder\" in global environment .\nFunction p_load() from pacman (package manager) blends installation and loading: if the package named in the argument of p_load() is not installed (not among the installed.packages()), p_load() attempts to install the package. If installation is successful, the package is loaded.\n\nif (! require(pak)){\n  install.packages(\"pak\")\n}\n\n\nto_be_loaded &lt;- c(\"devtools\",\n                  \"tidyverse\", \n                  \"lobstr\",\n                  \"ggforce\",\n                  \"nycflights13\",\n                  \"patchwork\", \n                  \"glue\",\n                  \"DT\", \n                  \"kableExtra\",\n                  \"viridis\")\n\nfor (pck in to_be_loaded) {\n  if (!require(pck, character.only = T)) {\n    pak::pkg_install(pck, repos=\"http://cran.rstudio.com/\")\n    stopifnot(require(pck, character.only = T))\n  }  \n}\n\n\n\n\n\n\n\nOptional arguments\n\n\n\nA very nice feature of R is that functions from base R as well as from packages have optional arguments with sensible default values. Look for example at documentation of require() using expression ?require.\nOptional settings may concern individual functions or the collection of functions exported by some packages. In the next chunk, we reset the default color scales used by graphical functions from ggplot2.\n\nopts &lt;- options()  # save old options\n\noptions(ggplot2.discrete.colour=\"viridis\")\noptions(ggplot2.continuous.colour=\"viridis\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou shall not confuse installing (on your hard-drive) and loading (in session) a package.\n\n\n\n\n\n\n\n\nQuestion for Pythonistas\n\n\n\n\nIn  what is the analogue of install.packages()?\nIn  what is the analogue of require()/library()?"
  },
  {
    "objectID": "labs/lab-r-intro.html#vector-creation-and-assignment",
    "href": "labs/lab-r-intro.html#vector-creation-and-assignment",
    "title": "R language: a tour",
    "section": "Vector creation and assignment",
    "text": "Vector creation and assignment\nThe next three lines create three numerical atomic vectors.\nIn IDE Rstudio, have a look at the environment pane on the right before running the chunk, and after.\nUse ls() to investigate the environment before and after the execution of the three assignments.\n\nls()\nx &lt;- c(1, 2, 12)\ny &lt;- 5:7\nz &lt;- 10:1\nx ; y ; z \nls()\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat are the identifiers known in the global environment before execution of lines 2-4?\nWhat are the identifiers known in the global environment after execution of lines 2-4?\nWhich objects are attached to identifiers x, y, and z?\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat does the next chunk?\n\nls()\nw &lt;- y\nls()\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nIs the content of object denoted by y copied to a new object bound to w?\nInterpret the result of w == y.\nInterpret the result of identical(w,y) (use help(\"identical\") if needed).\n\n\nw == y \nidentical(w,y)"
  },
  {
    "objectID": "labs/lab-r-intro.html#indexation-slicing-modification",
    "href": "labs/lab-r-intro.html#indexation-slicing-modification",
    "title": "R language: a tour",
    "section": "Indexation, slicing, modification",
    "text": "Indexation, slicing, modification\nSlicing a vector can be done in two ways:\n\nproviding a vector of indices to be selected. Indices need not be consecutive\nproviding a Boolean mask, that is a logical vector to select a set of positions\n\n\nx &lt;- c(1, 2, 12) ; y &lt;- 5:7 ; z &lt;- 10:1\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next lines\n\nz[1]   # slice of length 1\nz[0]   # What did you expect?\nz[x]   # slice of length ??? index error ?\nz[y]\nz[x %% 2]   # what happens with x[0] ?\nz[0 == (x %% 2)] # masking\nz[c(2, 1, 1)]\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIf the length of mask and and the length of the sliced vector do not coincide, what happens?\n\n\n\n\n\n\n\n\n\n\n\n\nA scalar is just a vector of length \\(1\\)!\n\nclass(z)\n\n[1] \"integer\"\n\nclass(z[1])\n\n[1] \"integer\"\n\nclass(z[c(2,1)])\n\n[1] \"integer\"\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next lines\n\ny[2:3] &lt;- z[2:3]\ny == z[-10]\n\nz[-11]\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next line\n\nz[-(1:5)]\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you select the last element from a vector (say z)?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nReverse the entries of a vector. Find two ways to do that.\n\n\nIn statistics, machine learning, we are often faced with the task of building grid of regularly spaced elements (these elements can be numeric or not). R offers a collection of tools to perform this. The most basic tool is rep().\n\n\n\n\n\n\nQuestion\n\n\n\n\nRepeat a vector \\(2\\) times\nRepeat each element of a vector twice\n\n\n\nLet us remove objects from the global environment.\n\nrm(w, x, y ,z)"
  },
  {
    "objectID": "labs/lab-r-intro.html#numbers",
    "href": "labs/lab-r-intro.html#numbers",
    "title": "R language: a tour",
    "section": "Numbers",
    "text": "Numbers\nSo far, we told about numeric vectors. Numeric vectors are vectors of floating point numbers. R distinguishes several kinds of numbers.\n\nIntegers\nFloating point numbers (double)\n\nTo check whether a vector is made of numeric or of integer, use is.numeric() or is.integer(). Use as.integer, as.numeric() to enforce type conversion.\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the outcome of the next chunks\n\nclass(113L) ; class(113) ; class(113L + 113) ; class(2 * 113L) ; class(pi) ; as.integer(pi)\n\n[1] \"integer\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] 3\n\n\n\nclass(as.integer(113))\n\n[1] \"integer\"\n\n\n\npi ; class(pi)\n\n[1] 3.141593\n\n\n[1] \"numeric\"\n\n\n\nfloor(pi) ; class(floor(pi)) # mind the floor\n\n[1] 3\n\n\n[1] \"numeric\""
  },
  {
    "objectID": "labs/lab-r-intro.html#integer-arithmetic",
    "href": "labs/lab-r-intro.html#integer-arithmetic",
    "title": "R language: a tour",
    "section": "Integer arithmetic",
    "text": "Integer arithmetic\n\n29L * 31L ; 899L %/% 32L ; 899L %% 30L\n\n[1] 899\n\n\n[1] 28\n\n\n[1] 29\n\n\n\n\n\n\n\n\nR integers are not the natural numbers from Mathematics\nR numerics are not the real numbers from Mathematics\n\n.Machine$double.eps\n\n[1] 2.220446e-16\n\n.Machine$double.xmax\n\n[1] 1.797693e+308\n\n.Machine$sizeof.longlong\n\n[1] 8\n\nu &lt;- double(19L)\nv &lt;- numeric(5L)\nw &lt;- integer(7L)\nlapply(list(u, v, w), typeof)\n\n[[1]]\n[1] \"double\"\n\n[[2]]\n[1] \"double\"\n\n[[3]]\n[1] \"integer\"\n\nlength(c(u, v, w))\n\n[1] 31\n\ntypeof(c(u, v, w))\n\n[1] \"double\"\n\n\n\n\n\nR is (sometimes) able to make sensible use of Infinite.\n\nlog(0)\n\n[1] -Inf\n\nlog(Inf)\n\n[1] Inf\n\n1/0\n\n[1] Inf\n\n0/0\n\n[1] NaN\n\nmax(c( 0/0,1,10))\n\n[1] NaN\n\nmax(c(NA,1,10))\n\n[1] NA\n\nmax(c(-Inf,1,10))\n\n[1] 10\n\nis.finite(c(-Inf,1,10))\n\n[1] FALSE  TRUE  TRUE\n\nis.na(c(NA,1,10))\n\n[1]  TRUE FALSE FALSE\n\nis.nan(c(NaN,1,10))\n\n[1]  TRUE FALSE FALSE"
  },
  {
    "objectID": "labs/lab-r-intro.html#computing-with-vectors",
    "href": "labs/lab-r-intro.html#computing-with-vectors",
    "title": "R language: a tour",
    "section": "Computing with vectors",
    "text": "Computing with vectors\nSumming, scalar multiplication\n\nx &lt;- 1:3\ny &lt;- 9:7\n\nsum(x) ; prod(x)\n\n[1] 6\n\n\n[1] 6\n\nz &lt;- cumsum(1:3)\nw &lt;- cumprod(3:5)\n\nx + y\n\n[1] 10 10 10\n\nx + z\n\n[1] 2 5 9\n\n2 * w\n\n[1]   6  24 120\n\n2 + w\n\n[1]  5 14 62\n\nw / 2\n\n[1]  1.5  6.0 30.0\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you compute a factorial?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nApproximate \\(\\sum_{n=1}^\\infty 1/n^2\\) within \\(10^{-3}\\)?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you compute the inner product between two (atomic numeric) vectors?\n\n\n\n\n\n\n\n\nWhat we have called vectors so far are indeed atomic vectors.\n\nRead Chapter on Vectors in R advanced Programming\n\nKeep an eye on package vctrs for getting insights into the R vectors."
  },
  {
    "objectID": "labs/lab-r-intro.html#creation-transposition-and-reshaping",
    "href": "labs/lab-r-intro.html#creation-transposition-and-reshaping",
    "title": "R language: a tour",
    "section": "Creation, transposition and reshaping",
    "text": "Creation, transposition and reshaping\nA vector can be turned into a column matrix.\n\nv &lt;- as.matrix(1:5)\nv\n\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n[4,]    4\n[5,]    5\n\n\nA matrix can be transposed\n\nt(v)  # transpose \n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n\ncat(dim(v), ' ', dim(t(v)), '\\n')\n\n5 1   1 5 \n\n\n\nA &lt;- matrix(1, nrow=5, ncol=2) ; A\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n[3,]    1    1\n[4,]    1    1\n[5,]    1    1\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nlobstr::mem_used() allows us to keep track of the amount of memory used by our R session. lobstr::obj_size() tells us the amount of memory used by the representation of an object.\nComment the next chunk\n\nm1 &lt;-lobstr::mem_used()\nA &lt;- matrix(rnorm(100000L), nrow=1000L)\nm2 &lt;- lobstr::mem_used()\nlobstr::obj_size(A)\n\n800,216 B\n\nB &lt;- t(A)\nlobstr::obj_size(B)\n\n800,216 B\n\nm3 &lt;- lobstr::mem_used()\nm2-m1 ; m3-m2\n\n795,664 B\n\n\n986,984 B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nIs there a difference between the next two assignments?\nHow would you assign value to all entries of a matrix?\n\n\nA &lt;- matrix(rnorm(16), nrow=4)\nA[] &lt;- 0 ; A\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    0    0    0    0\n\nA   &lt;- 0 ; A\n\n[1] 0\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the final shape of A?\n\nA &lt;- matrix(1, nrow=5, ncol=2) \nA\nA[] &lt;- 1:15 \nA\n\n\n\nWe can easily generate diagonal matrices and constant matrices.\n\ndiag(1, 3)  # building identity matrix\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\nmatrix(0, 3, 3) # building null matrix\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0    0\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs there any difference between the next two assignments?\n\nB &lt;- A[]\nB ; A\n\n[1] 0\n\n\n[1] 0\n\nlobstr::obj_addr(B) ; lobstr::obj_addr(A)\n\n[1] \"0x5bada50b8c58\"\n\n\n[1] \"0x5bada5ef7a00\"\n\nB &lt;- A"
  },
  {
    "objectID": "labs/lab-r-intro.html#indexation-slicing-modification-1",
    "href": "labs/lab-r-intro.html#indexation-slicing-modification-1",
    "title": "R language: a tour",
    "section": "Indexation, slicing, modification",
    "text": "Indexation, slicing, modification\nIndexation consists in getting one item from a vector/list/matrix/array/dataframe.\nSlicing and subsetting consists in picking a substructure:\n\nsubsetting a vector returns a vector\nsubsetting a list returns a list\nsubsetting a matrix/array returns a matrix/array (beware of implicit simplifications and dimension dropping)\nsubsetting a dataframe returns a dataframe or a vector (again, beware of implicit simplifications).\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next results\n\nA &lt;- matrix(1, nrow=5, ncol=2)\n\ndim(A[sample(5, 3), -1])\ndim(A[sample(5, 3), 1])\nlength(A[sample(5, 3), 1])\nis.vector(A[sample(5, 3), 1])\nA[10:15]\nA[60]\ndim(A[])\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you create a fresh copy of a matrix?"
  },
  {
    "objectID": "labs/lab-r-intro.html#computing-with-matrices",
    "href": "labs/lab-r-intro.html#computing-with-matrices",
    "title": "R language: a tour",
    "section": "Computing with matrices",
    "text": "Computing with matrices\n\n\n* versus %*%\n\n\n%*% stands for matrix multiplication. In order to use it, the two matrices should have conformant dimensions.\n\n\n\nt(v) %*% A\n\n          [,1]     [,2]\n[1,] -8.325958 6.187704\n\n\nThere are a variety of reasonable products around. Some of them are available in R.\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you compute the Hilbert-Schmidt inner product between two matrices?\n\\[\\langle A, B\\rangle_{\\text{HS}} = \\text{Trace} \\big(A \\times B^\\top\\big)\\]\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow can you invert a square (invertible) matrix?"
  },
  {
    "objectID": "labs/lab-r-intro.html#handling-three-valued-logic",
    "href": "labs/lab-r-intro.html#handling-three-valued-logic",
    "title": "R language: a tour",
    "section": "Handling three-valued logic",
    "text": "Handling three-valued logic\n\n\n\n\n\n\nQuestion\n\n\n\n\nTRUE &  (1&gt; (0/0))\n(1&gt; (0/0)) | TRUE\n(1&gt; (0/0)) | FALSE\nTRUE || (1&gt; (0/0))\nTRUE |  (1&gt; (0/0))\nTRUE || stopifnot(4&lt;3)\n# TRUE |  stopifnot(4&lt;3)  \nFALSE && stopifnot(4&lt;3)\n# FALSE & stopifnot(4&lt;3)\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the difference between logical operators || and | ?\n\n\n\n\n\n\n\n\n\n\n\n\nRemark: favor &, | over &&, ||."
  },
  {
    "objectID": "labs/lab-r-intro.html#all-and-any",
    "href": "labs/lab-r-intro.html#all-and-any",
    "title": "R language: a tour",
    "section": "\nall and any\n",
    "text": "all and any\n\nLook at the definition of all and any.\n\n\n\n\n\n\nQuestion\n\n\n\n\nHow would you check that a square matrix is symmetric?\nHow would you check that a matrix is diagonal?"
  },
  {
    "objectID": "labs/lab-r-intro.html#if-then-else",
    "href": "labs/lab-r-intro.html#if-then-else",
    "title": "R language: a tour",
    "section": "If () then {} else {}",
    "text": "If () then {} else {}\nIf expressions yes_expr and no_expr are complicated it makes sense to use the if (...) {...} else {...} construct\nThere is also a conditional statement with an optional else {}\n#| eval: false\n#| collapse: false\nif (condition) {\n  ...\n} else {\n  ...\n}\n\n\n\n\n\n\nQuestion\n\n\n\nIs there an elif construct in R?\n\n\n R also offers a switch\nswitch (object,\n  case1 = {action1}, \n  case2 = {action2}, \n  ...\n)\n\n\n\n\n\n\nThere exists a selection function ifelse(test, yes_expr, no_expr).\n\nifelse(test, yes, no)\n\nNote that ifelse(...) is vectorized.\n\nx &lt;-  1L:6L\ny &lt;-  rep(\"odd\", 6)\nz &lt;- rep(\"even\", 6)\n\nifelse(x %% 2L, y, z)\n\n[1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n\n\n This is a vectorized function"
  },
  {
    "objectID": "labs/lab-r-intro.html#iterations-for-it-in-iterable-...",
    "href": "labs/lab-r-intro.html#iterations-for-it-in-iterable-...",
    "title": "R language: a tour",
    "section": "Iterations for (it in iterable) {...}\n",
    "text": "Iterations for (it in iterable) {...}\n\nHave a look at Iteration section in R for Data Science\n\n\n\n\n\n\nQuestion\n\n\n\nCreate a lower triangular matrix which represents the 5 first lines of the Pascal triangle.\n\n\nRecall\n\\[\\binom{n}{k} = \\binom{n-1}{k-1} + \\binom{n-1}{k}\\]\n\n\n\n\n\n\nQuestion\n\n\n\nLocate the smallest element in a numerical vector"
  },
  {
    "objectID": "labs/lab-r-intro.html#while-condition",
    "href": "labs/lab-r-intro.html#while-condition",
    "title": "R language: a tour",
    "section": "While (condition) {…}",
    "text": "While (condition) {…}\n\n\n\n\n\n\nQuestion\n\n\n\nFind the location of the minimum in a vector v\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a loop that checks whether vector v is non-decreasing."
  },
  {
    "objectID": "labs/lab-r-intro.html#operators-purrrmap_",
    "href": "labs/lab-r-intro.html#operators-purrrmap_",
    "title": "R language: a tour",
    "section": "Operators purrr::map_???\n",
    "text": "Operators purrr::map_???\n\n\n\n\n\n\n\nQuestion\n\n\n\nWrite truth tables for &, |, &&, ||, ! and xor\nHint: use purrr::map, function outer()\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a function that takes as input a square matrix and returns TRUE if it is lower triangular.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nUse map , choose and proper use of pronouns to deliver the n first lines of the Pascal triangle using one line of code.\nAs far as the total number of operations is concerned, would you recommend this way of computing the Pascal triangle?\n\n\n\n\n\n\n\n\nRead Chapter on Functional Programming in Advanced R"
  },
  {
    "objectID": "labs/lab-lee-carter.html",
    "href": "labs/lab-lee-carter.html",
    "title": "Life tables, Lee-Carter Modeling",
    "section": "",
    "text": "Codestopifnot(\n  require(patchwork),\n  require(httr),\n  require(glue),\n#  require(ineq),\n  require(here),\n  require(skimr),\n  require(magrittr),\n  require(plotly),\n  require(tidyverse)\n)\n\nold_theme &lt;- theme_set(theme_minimal())"
  },
  {
    "objectID": "labs/lab-lee-carter.html#data-sources",
    "href": "labs/lab-lee-carter.html#data-sources",
    "title": "Life tables, Lee-Carter Modeling",
    "section": "Data sources",
    "text": "Data sources\nLife data tables are downloaded from https://www.mortality.org.\nSee also https://www.lifetable.de.\nIf you install and load package https://cran.r-project.org/web/packages/demography/index.html, you will also find life data tables.\nWe investigate life tables describing countries from Western Europe (France, Great Britain –actually England and Wales–, Italy, the Netherlands, Spain, and Sweden) and the United States.\nWe load the one-year lifetables for female, male and whole population for the different countries.\n\nCodelife_table |&gt;\n  dplyr::mutate(Country = forcats::as_factor(Country)) |&gt;\n  dplyr::mutate(Country = forcats::fct_relevel(Country, \"Spain\", \"Italy\", \"France\", \"England & Wales\", \"Netherlands\", \"Sweden\", \"USA\")) |&gt;\n  dplyr::mutate(Gender = forcats::as_factor(Gender)) -&gt; life_table\n\nlife_table |&gt;\n  dplyr::mutate(Area =forcats::fct_collapse(Country, \n                                            SE = c(\"Spain\", \"Italy\", \"France\"), \n                                            NE = c(\"England & Wales\", \"Netherlands\", \"Sweden\"), \n                                            USA=\"USA\")) -&gt; life_table\n\n\nCheck on http://www.mortality.org the meaning of the different columns:\nDocument Tables de mortalité françaises pour les XIXe et XXe siècles et projections pour le XXIe siècle contains detailed information on the construction of Life Tables for France.\nTwo kinds of Life Tables can be distinguished: Table du moment which contain for each calendar year, the mortality risks at different ages for that very year; and Tables de génération which contain for a given birthyear, the mortality risks at which an individual born during that year has been exposed.\nThe life tables investigated in this homework are Table du moment. According to the document by Vallin and Meslé, building the life tables required ,decisions and doctoring.\nSee (among other things)\n\np. 19 Abrupt changes in mortality quotients at some ages for a given calendar year\nEstimating mortality quotients at great age.\n\nHave a look at Lexis diagram.\nDefinitions can be obtained from www.lifeexpectancy.org. We translate it into mathematical (rather than demographic) language. Recall that the quantities define a probability distribution over \\(\\mathbb{N}\\). This probability distribution is a construction that reflects the health situation in a population at a given time. This probability distribution does not describe the sequence of sanitary situations experienced by a cohort (people born during a specific year).\n\nOne works with a period, or current, life table (table du moment). This summarizes the mortality experience of persons across all ages in a short period, typically one year or three years. More precisely, the death probabilities \\(q(x)\\) for every age \\(x\\) are computed for that short period, often using census information gathered at regular intervals. These \\(q(x)\\)’s are then applied to a hypothetical cohort of \\(100 000\\) people over their life span to produce a life table.\n\n\nCodelife_table |&gt; \n  filter(Country=='France', Year== 2010, Gender=='Female', Age &lt; 10 | Age &gt; 80)\n\n# A tibble: 39 × 13\n    Year   Age      mx      qx    ax     lx    dx    Lx      Tx    ex Country\n   &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;dbl&gt; &lt;fct&gt;  \n 1  2010     0 0.00325 0.00324  0.14 100000   324 99722 8465207  84.6 France \n 2  2010     1 0.00032 0.00032  0.5   99676    32 99660 8365484  83.9 France \n 3  2010     2 0.00015 0.00015  0.5   99645    15 99637 8265824  83.0 France \n 4  2010     3 0.00011 0.00011  0.5   99630    11 99624 8166187  82.0 France \n 5  2010     4 0.00008 0.00008  0.5   99619     8 99615 8066563  81.0 France \n 6  2010     5 0.00005 0.00005  0.5   99611     5 99608 7966948  80.0 France \n 7  2010     6 0.00008 0.00008  0.5   99606     8 99602 7867339  79.0 France \n 8  2010     7 0.00008 0.00008  0.5   99598     8 99594 7767737  78.0 France \n 9  2010     8 0.00008 0.00008  0.5   99590     8 99586 7668143  77   France \n10  2010     9 0.00007 0.00007  0.5   99582     7 99578 7568557  76   France \n# ℹ 29 more rows\n# ℹ 2 more variables: Gender &lt;fct&gt;, Area &lt;fct&gt;\n\n\nIn the sequel, we denote by \\(F_{t}\\) the cumulative distribution function for year \\(t\\). We agree on \\(\\overline{F}_t = 1 - F_t\\) and \\(F_t(-1)=0\\).\n\nCodelife_table |&gt; \n  filter( Year&gt;=1948) |&gt; \n  group_by(Country, Year, Gender) |&gt; \n  summarise(m1 =max(abs(lx -dx -lead(lx)), na.rm = T), \n            m2 =max(abs(lx * qx -dx), na.rm=T),\n            m3 =max(abs(Lx -lx * (1 + qx * (ax-1))), na.rm=T),\n            m4 =max(abs(1-exp(-mx)-qx), na.rm=T)) |&gt; \n  select(Year, Country, Gender, m1, m2, m3, m4) |&gt;  \n  ungroup() |&gt; \n  group_by(Country, Gender) |&gt; \n  slice_max(order_by = desc(m4), n = 1)\n\n# A tibble: 21 × 7\n# Groups:   Country, Gender [21]\n    Year Country         Gender    m1    m2    m3      m4\n   &lt;int&gt; &lt;fct&gt;           &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1  1948 Spain           Both       1 0.874 2.20  0.00838\n 2  1948 Spain           Female     1 0.789 1.56  0.00816\n 3  1952 Spain           Male       1 0.802 5.5   0.0119 \n 4  2004 Italy           Both       1 0.836 0.968 0.0150 \n 5  2004 Italy           Female     1 0.875 1.03  0.0149 \n 6  1984 Italy           Male       1 0.774 5.56  0.0146 \n 7  2007 France          Both       1 0.887 0.976 0.0152 \n 8  2007 France          Female     1 0.890 0.980 0.0151 \n 9  1979 France          Male       1 0.764 4.97  0.0161 \n10  1992 England & Wales Both       1 0.898 2.42  0.0135 \n# ℹ 11 more rows\n\n\n\nqx\n\n(age-specific) risk of death at age \\(x\\), or mortality quotient at given age \\(x\\) for given year \\(t\\): \\(q_{t,x} = \\frac{\\overline{F}_t(x) - \\overline{F}_t(x+1)}{\\overline{F}_t(x)}\\).\nFor each year, each age, \\(q_{t,x}\\) is determined by data. We also have \\[\\overline{F}_{t}(x+1) = \\overline{F}_{t}(x) \\times (1-q_{t,x+1})\\, .\\]\n\nmx\n\ncentral death rate at age \\(x\\) during year \\(t\\). This is connected with \\(q_{t,x}\\) by \\[m_{t,x} = -\\log(1- q_{t,x}) \\,,\\]\n\n\nor equivalently \\(q_{t,x} = 1 - \\exp(-m_{t,x})\\).\n\nlx\n\nthe so-called survival function: the scaled proportion of persons alive at age \\(x\\). These values are computed recursively from the \\(q_{t,x}\\) values using the formula \\[l_t(x+1) = l_t(x) \\times (1-q_{t,x}) \\, ,\\] with \\(l_{t,0}\\), the “radix” of the table, arbitrarily set to \\(100000\\). Function \\(l_{t,\\cdot}\\) and \\(\\overline{F}_t\\) are connected by \\[l_{t,x + 1} = l_{t,0} \\times \\overline{F}_t(x)\\,.\\] Note that in Probability theory, \\(\\overline{F}\\) is also called the survival or tail function.\n\ndx\n\n\\(d_{t,x} = q_{t,x} \\times l_{t,x}\\)\n\nTx\n\nTotal number of person-years lived by the cohort from age \\(x\\) to \\(x+1\\). This is the sum of the years lived by the \\(l_{t, x+1}\\) persons who survive the interval, and the \\(d_{t,x}\\) persons who die during the interval. The former contribute exactly \\(1\\) year each, while the latter contribute, on average, approximately half a year, so that \\(L_{t,x} = l_{t,x+1} + 0.5 \\times d_{t,x}\\). This approximation assumes that deaths occur, on average, half way in the age interval x to x+1. Such is satisfactory except at age 0 and the oldest age, where other approximations are often used; We will stick to a simplified vision \\(L_{t,x}= l_{t,x+1}\\)\n\n\nex:\n\nResidual Life Expectancy at age \\(x\\) and year \\(t\\)"
  },
  {
    "objectID": "labs/lab-lee-carter.html#loading-life_table-onto-an-in-memory-database",
    "href": "labs/lab-lee-carter.html#loading-life_table-onto-an-in-memory-database",
    "title": "Life tables, Lee-Carter Modeling",
    "section": "Loading life_table onto an in memory database",
    "text": "Loading life_table onto an in memory database\nWe load life_table into an in memory database, unleashing the full power of SQL. This is helpful if we have to use window functions.\n\n\n&lt;SQL&gt;\nSELECT `dbplyr_PPNQrffoC7`.*\nFROM `dbplyr_PPNQrffoC7`\nWHERE (`Gender` = 'Female') AND (`Country` = 'USA') AND ('Year' = 1948.0)\n\n\nObject lt can be queried like any other data frame.\n\nCodecon &lt;- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\nsrc &lt;- dbplyr::src_dbi(con, auto_disconnect = TRUE)\n\ndplyr::copy_to(src, lt)\n\n\nComputing residual life expectancies at all ages can also be completed using SQL queries."
  },
  {
    "objectID": "labs/lab-lee-carter.html#western-countries-in-1948",
    "href": "labs/lab-lee-carter.html#western-countries-in-1948",
    "title": "Life tables, Lee-Carter Modeling",
    "section": "Western countries in 1948",
    "text": "Western countries in 1948\nSeveral pictures share a common canvas: we plot central death rates against ages using a logarithmic scale on the \\(y\\) axis. Countries are identified by aesthetics (shape, color, linetypes). Abiding to the DRY principle, we define a prototype ggplot (alternatively plotly) object. The prototype will be fed with different datasets and decorated and arranged for the different figures.\n\nCodedummy_data &lt;- dplyr::filter(life_table, FALSE)\n\nproto_plot &lt;- ggplot(dummy_data,\n                     aes(x=Age,\n                         y=qx,\n                         col=Area,\n                         linetype=Country,\n                         shape=Country)) +\n              scale_y_log10() +\n              scale_x_continuous(breaks = c(seq(0, 100, 10), 109)) +\n              ylab(\"Mortality quotients\") +\n              labs(linetype=\"Country\") +\n              theme_bw()\n\n\n\nPlot qx of all Countries at all ages for years 1948 and 2013.\n\n\n\n\n\n\n\n\nCodeproto_plt2 &lt;-\n  ggplot() +\n  aes(x=Age, y=qx, colour=Area, frame=Year, linetype=Country) +\n  geom_point(size=.1) +\n  geom_line(size=.1) +\n  scale_y_log10() +\n  labs(linetype=c(\"Country\")) +\n  scale_x_continuous(breaks = c(seq(0, 100, 10), 109)) +\n  xlab(\"Age\") +\n  ylab(\"Central death rates\") +\n  facet_grid(cols=vars(Gender))\n\nwith(params,\n  (proto_plt2 %+%\n    (life_table |&gt; \n      filter(between(Year, year_p, year_e), \n             Gender != 'Both', \n             Age &lt; 90))  +\n    ggtitle(\"Central death rates 1948-2013: Europe catches up\"))) |&gt;\n  plotly::ggplotly()\n\n\n\n\n\n\n\n\n\n\n\nThe animated plot allows to spot more details. It is useful to use color so as to distinguish threee areas: USA; Northern Europe (NE) comprising England and Wales, the Netherlands, and Sweden; Southern Europe (SE) comprising Spain, Italy, and France. In 1948, NE and the USA exhibit comparable central death reates at all ages for the two genders, the USA looking like a more dangerous place for young adults. Spain lags behind, Italy and Frane showing up at intermediate positions.\nBy year 1962, SE has almost caught up the USA. Italy and Spain still have higher infant mortality while central death rates in the USA and France are almost identical at all ages for both genders. Central death rates attain a minimum around 10-12 for both genders. In Spain the minium central death rate has been divided by almost ten between 1948 and 1962.\nIf we dig further we observe that the shape of the male central death rates curve changes over time. In 1962, in the USA and France, central death rates exhibit a sharp increase between years 12 and 18, then remain almost constant between 20 and 30 and afterwards increase again. This pattern shows up in other countries but in a less spectacular way.\nTwenty years afterwards, during years 1980-1985, death rates at age 0 have decreased at around \\(1\\%\\) in all countries while it was \\(7\\%\\) in Spain in 1948. The male central death curve exhibits a plateau between ages 20 and 30. Central death rates at this age look higher in France and the USA.\nBy year 2000, France is back amongst European countries (at least with respect to central death rates). Young adult mortality rates are higher in the USA than in Europe. This phenomenon became more pregnant during the last decade.\n\n\n\nPlot ratios between central death rates (qx) in European countries and central death rates in the USA in 1948.\n\nCodewith(params,\n(eur_us_table  |&gt;\n  ggplot(aes(x=Age,\n             y=Ratio,\n             col=Area,\n             frame=Year,\n             linetype=Country)) +\n  scale_y_log10() +\n  scale_x_continuous(breaks = c(seq(0, 100, 10), 109)) +\n  geom_point(size=.1) +\n  geom_smooth(method=\"loess\", se=FALSE, span=.1, size=.1) +\n  ylab(\"Ratio of mortality quotients with respect to US\") +\n  labs(linetype=\"Country\", color=\"Area\") +\n  # scale_colour_brewer(direction=-1) +\n  ggtitle(label = stringr::str_c(\"European countries with respect to US,\", year_p,'-', year_e, sep = \" \"), subtitle = \"Sweden consistently ahead\") +\n  facet_grid(rows = vars(Gender)))) |&gt;\n  ggplotly()"
  },
  {
    "objectID": "labs/lab-babynames.html",
    "href": "labs/lab-babynames.html",
    "title": "Babynames I",
    "section": "",
    "text": "Code\nrequire(patchwork)\nrequire(httr)\nrequire(glue)\nrequire(ineq)\nrequire(here)\nrequire(skimr)\nrequire(magrittr)\nrequire(tidyverse)\n\nold_theme &lt;- theme_set(theme_minimal())"
  },
  {
    "objectID": "labs/lab-babynames.html#french-data",
    "href": "labs/lab-babynames.html#french-data",
    "title": "Babynames I",
    "section": "French data",
    "text": "French data\nThe French data are built and made available by INSEE (French Governement Statistics Institute)\n\nhttps://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip\n\nThis dataset has been growing for a while. It has been considered by social scientists for decades. Given names are meant to give insights into a variety of phenomena, including religious observance.\nA glimpse at that body of work can be found in L’archipel français by Jérome Fourquet, Le Seuil, 2019\nRead the File documentation\n\n\nCode\npath_data &lt;- 'DATA'\nfname &lt;- 'nat2021_csv.zip'\nfpath &lt;- here(path_data, fname)\n\nif (!file.exists(fpath)){\n  url &lt;- \"https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip\"\n  download.file(url, fpath, mode=\"wb\")\n}   \n\ndf_fr &lt;- readr::read_csv2(fpath)\n\n# df_fr |&gt; glimpse()"
  },
  {
    "objectID": "labs/lab-babynames.html#us-data",
    "href": "labs/lab-babynames.html#us-data",
    "title": "Babynames I",
    "section": "US data",
    "text": "US data\nUS data may be gathered from\nBaby Names USA from 1910 to 2021 (SSA)\nSee https://www.ssa.gov/oact/babynames/background.html\nIt can also be obtained by installing and loading the “babynames” package.\nFull baby name data provided by the SSA. This includes all names with at least 5 uses.\n\n\nCode\nif (!require(\"babynames\")){\n  install.packages(\"babynames\")\n  stopifnot(require(\"babynames\"), \"Couldn't install and load package 'babynames'\")\n}\n\n\n\n\nCode\n?babynames"
  },
  {
    "objectID": "labs/lab-babynames.html#tidy-the-french-data",
    "href": "labs/lab-babynames.html#tidy-the-french-data",
    "title": "Babynames I",
    "section": "Tidy the French data",
    "text": "Tidy the French data\nRename columns according to the next lookup table:\n\n\nCode\nlkp &lt;- list(year=\"annais\",\n  sex=\"sexe\",\n  name=\"preusuel\",\n  n=\"nombre\")\n\n\n\n\nCode\ndf_fr &lt;- df_fr |&gt;\n1  rename(!!!lkp) |&gt;\n  mutate(country='fr') |&gt;\n  mutate(sex=as_factor(sex)) |&gt;\n  mutate(sex=fct_recode(sex, \"M\"=\"1\", \"F\"=\"2\")) |&gt;\n  mutate(sex=fct_relevel(sex, \"F\", \"M\")) |&gt; \n  mutate(year=ifelse(year==\"XXXX\", NA, year)) |&gt;\n  mutate(year=as.integer(year)) \n  \ndf_fr  |&gt;\n  sample(5) |&gt;\n  glimpse()\n\n\n\n1\n\n!!! (bang-bang-bang) is offered by rlang package. Here, we use it to perform list unpacking (with the same intent and purposes we use dictionary unpacking in Python)\n\n\n\n\nRows: 686,538\nColumns: 5\n$ n       &lt;dbl&gt; 1249, 1342, 1330, 1286, 1430, 1472, 1451, 1514, 1509, 1526, 16…\n$ country &lt;chr&gt; \"fr\", \"fr\", \"fr\", \"fr\", \"fr\", \"fr\", \"fr\", \"fr\", \"fr\", \"fr\", \"f…\n$ name    &lt;chr&gt; \"_PRENOMS_RARES\", \"_PRENOMS_RARES\", \"_PRENOMS_RARES\", \"_PRENOM…\n$ year    &lt;int&gt; 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 19…\n$ sex     &lt;fct&gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M,…\n\n\nDownload ‘Naissances totales par sexe’ from URL https://www.ined.fr/fichier/s_rubrique/168/t35.fr.xls from INED.\n\n\nCode\nbirths_fr_path &lt;- here(path_data, 't35.fr.xls')\nbirths_fr_url &lt;- 'https://www.ined.fr/fichier/s_rubrique/168/t35.fr.xls'\n\nif (!file.exists(births_fr_path)) {\n  download.file(births_fr_url, births_fr_path)\n}\n\n\n\n\nCode\nbirths_fr &lt;-  readxl::read_excel(births_fr_path, skip = 3)\n\nbirths_fr &lt;- births_fr[-1, ] \n\n\nbirths_fr |&gt; \n  glimpse()\n\n\nRows: 130\nColumns: 10\n$ `Répartition par sexe et vie`                &lt;chr&gt; \"1901\", \"1902\", \"1903\", \"…\n$ `Ensemble des nés vivants`                   &lt;dbl&gt; 917075, 904434, 884498, 8…\n$ `Nés vivants - Garçons`                      &lt;dbl&gt; 468125, 462097, 451510, 4…\n$ `Nés vivants - Filles`                       &lt;dbl&gt; 448950, 442337, 432988, 4…\n$ `Ensemble des enfants sans vie`              &lt;dbl&gt; 32410, 32000, 31076, 3067…\n$ `Enfants sans vie - Garçons`                 &lt;chr&gt; \"18522\", \"18172\", \"17875\"…\n$ `Enfants sans vie - Filles`                  &lt;chr&gt; \"13888\", \"13828\", \"13201\"…\n$ `Garçons vivants pour 100 nés\\nvivants`      &lt;dbl&gt; 51.0, 51.1, 51.0, 51.0, 5…\n$ `Garçons vivants pour 100\\nfilles vivantes`  &lt;dbl&gt; 104.3, 104.5, 104.3, 104.…\n$ `Garçons sans vie pour 100\\nfilles sans vie` &lt;chr&gt; \"133.40000000000001\", \"13…\n\n\n\n\n\n\n\n\nIf you have problems with the excel reader, feel free to download an equivalent csv file from url\n\n\n\n\n\nCode\nnames(births_fr)[1] &lt;- \"year\"\n\n\n\n\nCode\nbirths_fr &lt;- births_fr |&gt;\n  mutate(year=as.integer(year)) |&gt;\n  drop_na()\n\n\n\n\nCode\nbirths_fr |&gt;\n  ggplot() +\n  aes(x=year, y=`Ensemble des nés vivants`) +\n  geom_col() +\n  labs(title=\"Births in France\")"
  },
  {
    "objectID": "labs/lab-babynames.html#tidy-the-american-data",
    "href": "labs/lab-babynames.html#tidy-the-american-data",
    "title": "Babynames I",
    "section": "Tidy the American data",
    "text": "Tidy the American data\n\n\nCode\nbabynames &lt;- babynames |&gt;\n  mutate(country='us') |&gt;\n  mutate(sex=as_factor(sex))\n  \nbabynames |&gt;\n  glimpse()\n\n\nRows: 1,924,665\nColumns: 6\n$ year    &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 18…\n$ sex     &lt;fct&gt; F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F,…\n$ name    &lt;chr&gt; \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Id…\n$ n       &lt;int&gt; 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 12…\n$ prop    &lt;dbl&gt; 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.…\n$ country &lt;chr&gt; \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"u…\n\n\n\n\nCode\nbirths_us &lt;- births\n\nbirths_us  |&gt; \n  ggplot() +\n  aes(x=year, y=births) +\n  geom_col() +\n  labs(title=\"Births in USA\")"
  },
  {
    "objectID": "labs/lab-babynames.html#sex-ratios",
    "href": "labs/lab-babynames.html#sex-ratios",
    "title": "Babynames I",
    "section": "Sex ratios",
    "text": "Sex ratios\n\n\n\n\n\n\nQuestion\n\n\n\nIn dataset df_fr compute the total number of reported male and female births per year. Compute and plot the sex ratio.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCompare with sex ratio as given in dataset from INED\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nConsider the fluctuations of the sex ratio through the years.\nAre they consistent with the hypothesis: the sex of newborns are independently. identically distributed with the probability of getting a girl equal to \\(.48\\)?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nConsider again the fluctuations of the sex ratio through the years.\nAssume that for each year the sex of newborns are independently. identically distributed with the probability of getting a girl depending on the year.\nAre the data consistent with the hypothesis: the probability of getting a girl remains constant thoughout the years?"
  },
  {
    "objectID": "labs/lab-babynames.html#grouping-names-by-patterns-of-popularity",
    "href": "labs/lab-babynames.html#grouping-names-by-patterns-of-popularity",
    "title": "Babynames I",
    "section": "Grouping names by patterns of popularity",
    "text": "Grouping names by patterns of popularity"
  },
  {
    "objectID": "labs/lab-babynames.html#fitting-a-zipf-distribution",
    "href": "labs/lab-babynames.html#fitting-a-zipf-distribution",
    "title": "Babynames I",
    "section": "Fitting a Zipf distribution",
    "text": "Fitting a Zipf distribution\n\n\n\n\n\n\nChoosing scales\n\n\n\n\n\n\nAnimation"
  },
  {
    "objectID": "labs/lab-babynames.html#classifying-names-according-to-their-pattern-of-popularity",
    "href": "labs/lab-babynames.html#classifying-names-according-to-their-pattern-of-popularity",
    "title": "Babynames I",
    "section": "Classifying names according to their pattern of popularity",
    "text": "Classifying names according to their pattern of popularity\nNow, we focus on names that made it to the top \\(300\\) at least once since year 1948. We attempt to classify them according to their pattern of popularity,"
  },
  {
    "objectID": "labs/lab-univariate-categorical.html#setup",
    "href": "labs/lab-univariate-categorical.html#setup",
    "title": "Univariate analysis II",
    "section": "Setup",
    "text": "Setup\nTry to load (potentially) useful packages in a chunk at the beginning of your file.\n\nCodestopifnot(\n  require(lobstr), \n  require(rlang),\n  require(ggforce),\n  require(patchwork), \n  require(glue),\n  require(magrittr),\n  require(DT), \n  require(gt),\n  require(kableExtra),\n  require(viridis),\n  require(vcd),\n  require(skimr),\n  require(tidyverse)\n\n)\n\n\nSet the (graphical) theme\n\nCodeold_theme &lt;- theme_set(theme_minimal())\n\n\n\n\n\n\n\n\nIn this lab, we load the data from the hard drive. The data are read from some file located in our tree of directories. Loading requires the determination of the correct filepath. This filepath is often a relative filepath, it is relative to the directory where the R session/the R script has been launched. Base R offers functions that can help you to find your way the directories tree.\n\nCodegetwd() # Where are we? \n## [1] \"/home/boucheron/Documents/MA7BY020/labs\"\nhead(list.files())  # List the files in the current directory\n## [1] \"_lab-in-memory.qmd\"     \"_lab-nycflights-pq.qmd\" \"_metadata.yml\"         \n## [4] \"lab-babynames.qmd\"      \"lab-bivariate_cache\"    \"lab-bivariate_files\"\nhead(list.dirs())   # List sub-directories\n## [1] \".\"                                 \"./lab-bivariate_cache\"            \n## [3] \"./lab-bivariate_cache/html\"        \"./lab-bivariate_cache/pdf\"        \n## [5] \"./lab-bivariate_files\"             \"./lab-bivariate_files/figure-html\"\n\n\n\n\n\n\n\n\n\n\n\nUse package fs for files maniulations\n\n\n\n\n\n\nSummarizing univariate categorical samples amounts to counting the number of occurrences of levels in the sample.\nVisualizing categorical samples starts with\n\nBar plots\nColumn plots\n\nThis exploratory work seldom makes it to the final report. Nevertheless, it has to be done in an efficient, reproducible way.\nThis is an opportunity to introduce the DRY principle.\nAt the end, we shall see that skimr::skim() can be very helpful."
  },
  {
    "objectID": "labs/lab-univariate-categorical.html#counting",
    "href": "labs/lab-univariate-categorical.html#counting",
    "title": "Univariate analysis II",
    "section": "Counting",
    "text": "Counting\nUse table, prop.table from base R to compute the frequencies and proportions of the different levels. In statistics, the result of table() is a (one-way) contingency table.\nWhat is the class of the object generated by table? Is it a vector, a list, a matrix, an array ?\n\n\n\n\n\n\nas.data.frame() (or as_tibble) can transform a table object into a dataframe.\n\nCodeta &lt;-  rename(as.data.frame(ta), SEXE=`.`)\n\nta\n\n  SEXE Freq\n1    F  297\n2    M  302\n\n\n\n\n\nYou may use knitr::kabble(), possibly knitr::kable(., format=\"markdown\") to tweak the output.\nIn order to feed ggplot with a contingency table, it is useful to build contingency tables as dataframes. Use dplyr::count() to do this.\n\n\n\n\n\n\nskimr::skim() allows us to perform univariate categorical analysis all at once.\n\nCodedf %&gt;% \n  skimr::skim(where(is.factor)) %&gt;% \n  print(n=50)\n\n── Data Summary ────────────────────────\n                           Values    \nName                       Piped data\nNumber of rows             599       \nNumber of columns          11        \n_______________________              \nColumn type frequency:               \n  factor                   9         \n________________________             \nGroup variables            None      \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique\n1 SEXE                  0             1 FALSE          2\n2 REGION                0             1 FALSE          4\n3 STAT_MARI             0             1 FALSE          5\n4 SYNDICAT              0             1 FALSE          2\n5 CATEGORIE             0             1 FALSE         10\n6 NIV_ETUDES            0             1 FALSE         15\n7 NB_PERS               0             1 FALSE          9\n8 NB_ENF                0             1 FALSE          7\n9 REV_FOYER             0             1 FALSE         16\n  top_counts                           \n1 M: 302, F: 297                       \n2 S: 200, W: 148, NE: 129, NW: 122     \n3 M: 325, C: 193, D: 61, S: 14         \n4 non: 496, oui: 103                   \n5 Lib: 133, Ser: 125, Adm: 94, Sel: 48 \n6 12 : 187, Col: 148, Bac: 114, Ass: 45\n7 2: 196, 4: 130, 3: 122, 1: 63        \n8 0: 413, 1: 86, 2: 76, 3: 18          \n9 [60: 89, [75: 77, [50: 71, [40: 70   \n\n\nThe output can be tailored to your specific objectives and fed to functions that are geared to displaying large tables (see packages knitr, DT, and gt)"
  },
  {
    "objectID": "labs/lab-univariate-categorical.html#using-a-for-loop",
    "href": "labs/lab-univariate-categorical.html#using-a-for-loop",
    "title": "Univariate analysis II",
    "section": "Using a for loop",
    "text": "Using a for loop\nWe have to build a barplot for each categorical variable. Here, we just have nine of them. We could do this using cut and paste, and some editing. In doing so, we would not comply with the DRY (Don’t Repeat Yourself) principle.\nIn order to remain DRY, we will attempt to abstract the recipe we used to build our first barplot.\nThis recipe is pretty simple:\n\nBuild a ggplot object with df as the data layer.\nAdd an aesthetic mapping a categorical column to axis x\n\nAdd a geometry using geom_bar\n\nAdd labels explaining the reader which column is under scrutiny\n\nWe first need to gather the names of the categorical columns. The following chunk does this in a simple way.\nIn the next chunk, we shall build a named list of ggplot objects consisting of barplots. The for loop body is almost obtained by cutting and pasting the recipe for the first barplot.\n\n\n\n\n\n\nNote an important difference: instead of something aes(x=col) where col denotes a column in the dataframe, we shall write aes(x=.data[[col]]) where col is a string that matches a column name. Writing aes(x=col) would not work.\nThe loop variable col iterates over the column names, not over the columns themselves.\nWhen using ggplot in interactive computations, we write aes(x=col), and, under the hood, the interpreter uses the tidy evaluation mechanism that underpins R to map df$col to the x axis.\nggplot functions like aes() use data masking to alleviate the burden of the working Statistician.\nWithin the context of ggplot programming, pronoun .data refers to the data layer of the graphical object.\n\n\n\nIf the labels on the x-axis are not readable, we need to tweak them. This amounts to modifying the theme layer in the ggplot object, and more specifically the axis.text.x attribute."
  },
  {
    "objectID": "labs/lab-univariate-categorical.html#using-functional-programming-lapply-purrr...",
    "href": "labs/lab-univariate-categorical.html#using-functional-programming-lapply-purrr...",
    "title": "Univariate analysis II",
    "section": "Using functional programming (lapply, purrr::...)",
    "text": "Using functional programming (lapply, purrr::...)\nAnother way to compute the list of graphical objects replaces the for loop by calling a functional programming tool. This mechanism relies on the fact that in R, functions are first-class objects.\n\n\n\n\n\n\nPackage purrr offers a large range of tools with a clean API. Base R offers lapply().\n\n\n\nWe shall first define a function that takes as arguments a datafame, a column name, and a title. We do not perform any defensive programming. Call your function foo.\nFunctional programmming makes code easier to understand.\nUse foo, lapply or purrr::map() to build the list of graphical objects.\nWith purrr::map(), you may use either a formula or an anonymous function. With lapply use an anonymous function.\nPackage patchwork offers functions for displaying collections of related plots."
  },
  {
    "objectID": "projects/homework01.html",
    "href": "projects/homework01.html",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "",
    "text": "Due date : 2023-02-24 @23h55 (this is a hard deadline)\n\n\n\nName, First Name, Informatique/Mathématique-Informatique\nName, First Name, Informatique/Mathématique-Informatique\n\n\n\n\nIf you don’t: no evaluation!\nWrite in English or French\nThe deliverable is a file\n\nxxx_yyy.ipynb file (jupyter notebook) or\nxxx_yyy.py file (if you are using jupytext) or\nxxx_yyy.qmd file (if you are using quarto)\n\nwhere xxx and yyy are your names, for example lagarde_michard.ipynb.\nThe deliverable is not meant to contain cell outputs.\nThe data files used to execute cells are meant to sit in the same directory as the deliverable. Use relative filepaths or urls to denote the data files.\nWe will execute the code in your notebook: make sure that running all the cells works well.\n\n\n\nHere is the way we’ll assess your work\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nSpelling and syntax\n3\nEnglish/French\n\n\nPlots correction\n3\nClarity / answers the question\n\n\nPlot style and cleanliness\n3\nTitles, legends, labels, breaks …\n\n\nTable wrangling\n4\nETL, SQL like manipulations\n\n\nComputing Statistics\n5\nSQL goup by and aggregation\n\n\nDRY compliance\n2\nDRY principle at Wikipedia\n\n\n\nIf we see a single (or more) for loop in your code: -5 points. Everything can be done using high-level pandas methods"
  },
  {
    "objectID": "projects/homework01.html#fill-this-cell-with-your-names",
    "href": "projects/homework01.html#fill-this-cell-with-your-names",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "",
    "text": "Name, First Name, Informatique/Mathématique-Informatique\nName, First Name, Informatique/Mathématique-Informatique"
  },
  {
    "objectID": "projects/homework01.html#carefully-follow-instructions",
    "href": "projects/homework01.html#carefully-follow-instructions",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "",
    "text": "If you don’t: no evaluation!\nWrite in English or French\nThe deliverable is a file\n\nxxx_yyy.ipynb file (jupyter notebook) or\nxxx_yyy.py file (if you are using jupytext) or\nxxx_yyy.qmd file (if you are using quarto)\n\nwhere xxx and yyy are your names, for example lagarde_michard.ipynb.\nThe deliverable is not meant to contain cell outputs.\nThe data files used to execute cells are meant to sit in the same directory as the deliverable. Use relative filepaths or urls to denote the data files.\nWe will execute the code in your notebook: make sure that running all the cells works well."
  },
  {
    "objectID": "projects/homework01.html#grading",
    "href": "projects/homework01.html#grading",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "",
    "text": "Here is the way we’ll assess your work\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nSpelling and syntax\n3\nEnglish/French\n\n\nPlots correction\n3\nClarity / answers the question\n\n\nPlot style and cleanliness\n3\nTitles, legends, labels, breaks …\n\n\nTable wrangling\n4\nETL, SQL like manipulations\n\n\nComputing Statistics\n5\nSQL goup by and aggregation\n\n\nDRY compliance\n2\nDRY principle at Wikipedia\n\n\n\nIf we see a single (or more) for loop in your code: -5 points. Everything can be done using high-level pandas methods"
  },
  {
    "objectID": "projects/homework01.html#notebooks-modus-operandi",
    "href": "projects/homework01.html#notebooks-modus-operandi",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Notebooks: Modus operandi",
    "text": "Notebooks: Modus operandi\n\nThis is a Jupyter Notebook.\nWhen you execute code within the notebook, the results appear beneath the code.\nJupytext\nQuarto"
  },
  {
    "objectID": "projects/homework01.html#packages",
    "href": "projects/homework01.html#packages",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Packages",
    "text": "Packages\n\nBase Python can do a lot. But the full power of Python comes from a fast growing collection of packages/modules.\nPackages are first installed (that is using pip install or conda install), and if needed, imported during a session.\nThe docker image you are supposed to use already offers a lot of packages. You should not need to install new packages.\nOnce a package has been installed on your drive, if you want all objects exported by the package to be available in your session, you should import the package, using from pkg import *.\nIf you just want to pick some subjects from the package, you can use qualified names like pkg.object_name to access the object (function, dataset, class…)\n\n\n\nCode\n# importing basic tools\nimport numpy as np\nimport pandas as pd\n\nfrom pandas.api.types import CategoricalDtype\n\nimport os            # file operations\nimport requests      # networking\nimport zipfile\nimport io\nfrom pathlib import Path\n\nfrom datetime import date  # if needed\n\n\n\n\nCode\n# importing plotting packages\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\n\n\n\n\nCode\n# make pandas plotly-friendly\nnp.set_printoptions(precision=2, suppress=True)\n%matplotlib inline\npd.options.plotting.backend = \"plotly\""
  },
  {
    "objectID": "projects/homework01.html#french-data",
    "href": "projects/homework01.html#french-data",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "French data",
    "text": "French data\nThe French data are built and made available by INSEE (French Governement Statistics Institute)\nPrénoms: - https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip\nThis dataset has been growing for a while. It has been considered by social scientists for decades. Given names are meant to give insights into a variety of phenomena, including religious observance.\n\nA glimpse at the body of work can be found in L’archipel français by Jérome Fourquet, Le Seuil, 2019\nRead the File documentation"
  },
  {
    "objectID": "projects/homework01.html#us-data",
    "href": "projects/homework01.html#us-data",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "US data",
    "text": "US data\nUS data may be gathered from\nBaby Names USA from 1910 to 2021 (SSA)\nSee https://www.ssa.gov/oact/babynames/background.html"
  },
  {
    "objectID": "projects/homework01.html#british-data",
    "href": "projects/homework01.html#british-data",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "British data",
    "text": "British data\nEnglish and Welsh data can be gathered from\nhttps://www.ons.gov.uk/"
  },
  {
    "objectID": "projects/homework01.html#download-the-french-data",
    "href": "projects/homework01.html#download-the-french-data",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Download the French data",
    "text": "Download the French data\nQUESTION: Download the data into a file which relative path is './nat2021_csv.zip'\nHints:\n\nHave a look at package requests.\nUse magic commands to navigate across the file hierarchy and create subdirectories when needed\n\n\n\nCode\n# for Frenc data \n\nparams = dict(\n    url = 'https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip',\n    dirpath = './',\n    timecourse = '',\n    datafile = 'nat2021.hdf',\n    fpath = 'nat2021_csv.zip'\n)\n\n\n\n\nCode\n# modify location  make sure you are in the right directory\n# %cd\n# %pwd  #\n# %ls\n# %mkdir # if needed\n\n\n\n\nCode\nurl = params['url']      # 'https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip'\nfpath = params['fpath']  # './nat2021_csv.zip'\n\n\n\n\nCode\n# your code here\n\nif not Path(params['fpath']).exists():\n    r = requests.get(params['url'])                # What is the type of `r` ?\n    z = zipfile.ZipFile((io.BytesIO(r.content)))   # What is the type of `z` ? \n    z.extractall(path='./')"
  },
  {
    "objectID": "projects/homework01.html#download-us-and-british-data",
    "href": "projects/homework01.html#download-us-and-british-data",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Download US and British data",
    "text": "Download US and British data\n\n\nCode\n# your code here"
  },
  {
    "objectID": "projects/homework01.html#load-the-french-data-in-memory",
    "href": "projects/homework01.html#load-the-french-data-in-memory",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Load the French data in memory",
    "text": "Load the French data in memory\nQUESTION: Load the data in a pandas DataFrame called data\nHints:\n\nYou should obtain a Pandas dataframe with 4 columns.\nMind the conventions used to build the csv file.\nPackage pandas provides the convenient tools.\nThe dataset, though not too large, is already demanding.\nDon’t hesitate to test your methods on a sample of rows method sample() from class DataFrame can be helpful.\n\n\n\nCode\n# your code here"
  },
  {
    "objectID": "projects/homework01.html#load-us-and-british-data-in-memory",
    "href": "projects/homework01.html#load-us-and-british-data-in-memory",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Load US and British data in memory",
    "text": "Load US and British data in memory\n\n\nCode\n# your code here"
  },
  {
    "objectID": "projects/homework01.html#explore-the-data",
    "href": "projects/homework01.html#explore-the-data",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Explore the data",
    "text": "Explore the data\nQUESTION: Look at the data, Use the attributes columns, dtypes and the methods head, describe, to get a feeling of the data.\n\nThis dataset is supposed to report all given names used for either sex during a year in France since 1900\nThe file is made of 652 056 lines and 4 columns.\n\n|-- preusuel : object\n|-- nombre: int64\n|-- sexe: int64\n|-- annais: object\nEach row indicates for a given preusuel (prénom usuel, given name), sexe (sex), and annais (année naissance, birthyear) the nombre (number) of babies of the given sex who were given that name during the given year.\n\n\n\nsexe\npreusuel\nannais\nnombre\n\n\n\n\n2\nSYLVETTE\n1953\n577\n\n\n1\nBOUBOU\n1979\n4\n\n\n1\nNILS\n1959\n3\n\n\n2\nNICOLE\n2003\n36\n\n\n1\nJOSÉLITO\n2013\n4\n\n\n\nQUESTION: Compare memory usage and disk space used by data\nHints:\n\nThe method info prints a concise summary of a DataFrame.\nWith optional parameter memory_usage, you can get an estimate of the amount of memory used by the DataFrame.\nBeware that the resulting estimate depends on the argument fed.\n\n\n\nCode\n# your code here\n\n\nQUESTION: Display the output of .describe() with style.\n\n\nCode\n# your code here\n\n\nQUESTION: For each column compute the number of distinct values\n\n\nCode\n# your code here"
  },
  {
    "objectID": "projects/homework01.html#improving-the-data-types",
    "href": "projects/homework01.html#improving-the-data-types",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Improving the data types",
    "text": "Improving the data types\nQUESTION: Make sexe a category with two levels Female and Male. Call the new column gender. Do you see any reason why this factor should be ordered?\nHint: Read Pandas and categorical variables\n\n\nCode\n# your code here\n\n\nQUESTION: Compare memory usage of columns sexe and gender\n\n\nCode\n# your code here\n\n\nQUESTION: Would it be more memory-efficient to recode sexe using modalities F and M instead of Male and Female ?\nInsert your answer here\n\n…"
  },
  {
    "objectID": "projects/homework01.html#dealing-with-missing-values",
    "href": "projects/homework01.html#dealing-with-missing-values",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Dealing with missing values",
    "text": "Dealing with missing values\nQUESTION: Variable annais class is object. Make annais of type float. Note that missing years are encoded as “XXXX”, find a way to deal with that.\nHint: As of releasing this Homework (2023-01-18), Pandas is not very good at managing missing values, see roadmap. Don’t try to convert annais into an integer column.\n\n\nCode\n# your code here"
  },
  {
    "objectID": "projects/homework01.html#rename-and-remove-columns",
    "href": "projects/homework01.html#rename-and-remove-columns",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Rename and remove columns",
    "text": "Rename and remove columns\nQUESTION: Remove useless columns (now that you’ve created new ones, and rename them). You should end up with a dataframe with columns called \"gender\", \"year\", \"count\", \"firstname” with the following dtypes:\ngender        category\nfirstname     object\ncount         int64\nyear          float64\n\n\nCode\n# your code here\n\n\nQuestion: Do the same thing for British and US data. You should eventually obtain dataframes with the same schema.\nQUESTION: How many missing values (NA) have been introduced? How many births are concerned?\n\n\nCode\n# your code here\n\n\nQUESTION: Read the documentation and describe the origin of rows containing the missing values.\n\n\nCode\n# your code here"
  },
  {
    "objectID": "projects/homework01.html#checkpointing-save-your-transformed-dataframes",
    "href": "projects/homework01.html#checkpointing-save-your-transformed-dataframes",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Checkpointing: save your transformed dataframes",
    "text": "Checkpointing: save your transformed dataframes\nQUESTION: Save the transformed dataframe (retyped and renamed) to ./nat2021_csv.zip. Try several compression methods.\n\n\nCode\n# your code here\n\n\nQUESTION: Save the transformed dataframes (retyped and renamed) to ./nat2021.hdf using .hdf format\n\n\nCode\n# your code here\n\n\nAt that point your working directory should look like:\n├── homework01.py      # if you use `jupytext`\n|── homework01.qmd     # if you use `quarto`\n├── homework01.ipynb   # if you use `jupyter` `notebook`\n├── babies-fr.hdf\n├── babies-fr.zip\n├── babies-us.hdf\n├── babies-us.zip\n├── babies-ew.hdf\n├── babies-ew.zip\n├── births-fr.csv\n├── births-fr.hdf\nQUESTION: Reload the data using read_hdf(...) so that the resulting dataframes are properly typed with meaningful and homogeneous column names.\nHint: use try: ... except to handle exceptions such as FileNotFoundError\n\n\nCode\n# your code here"
  },
  {
    "objectID": "projects/homework01.html#some-data-analytics-and-visualization",
    "href": "projects/homework01.html#some-data-analytics-and-visualization",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Some data “analytics” and visualization",
    "text": "Some data “analytics” and visualization\nQUESTION: For each year, compute the total number of Female and Male births and the proportion of Female births among total births\nHints:\n\nGroupby operations using several columns for the groups return a dataframe with a MultiIndex index see Pandas advanced\nHave a look at MultiIndex, reset_index, pivot, columns.droplevel\n\n\n\nCode\n# your code here\n\n\nQUESTION: Plot the proportion of female births as a function of year and French, US, en British babynames data. Compare with what you get from births-fr.hdf.\nDon’t forget: title, axes labels, ticks, scales, etc.\nBecause of what we did before, the plot method of a DataFrame with be rendered using plotly, so you can use this. But you can use also seaborn or any other available plotting library that you want.\nHint: Mind the missing values in the year column\n\n\nCode\n# your code here\n\n\nQUESTION: Make any sensible comment about these plots.\nInsert your answer here\n\n…\n\nQUESTION: Explore the fluctuations of sex ratio around its mean value since 1945 in the US, in France and in the Great Britain.\nPlot deviations of sex ratio around its mean since 1945 as a function of time.\n\n\nCode\n# your code here\n\n\nQUESTION: Assume that baby gender is chosen at random according to a Bernoulli distribution with success probability \\(.48\\), that baby genders are i.i.d. Perform simulations for sex ratios for French and US data since 1945.\nPlot the results, compare with your plots above."
  },
  {
    "objectID": "projects/homework01.html#rare-firstnames",
    "href": "projects/homework01.html#rare-firstnames",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Rare firstnames",
    "text": "Rare firstnames\nQUESTION: In the French data, for each sex, plot the proportion of births given _PRENOMS_RARES as a function of the year.\n\n\nCode\n# your code here"
  },
  {
    "objectID": "projects/hmw-glm-2024.html",
    "href": "projects/hmw-glm-2024.html",
    "title": "MA7BY020 - Spring 2025",
    "section": "",
    "text": "Homework 2 (2023-24): Linear regression and Gaussian Linear Modeling\nDue date : 2024-03-15 23:55 (hard deadline)\n\nM1 MIDS & MFA\nUniversité Paris Cité\nAnnée 2023-2024\nCourse Homepage\n\nMoodle\n\n\n\n\n\n\n\n\n Objectives\nThis homework is concerned with Gaussian Linear Models. The objective consists of working with simulated data and visualizing/illustrating the main constructions and theorems from the sections of the Statistical Inference course dedicated to Gaussian Linear Models.\nWe start from the whiteside dataset from MASS package (R).\nFit a linear model with formula Gas ~ poly(Temp, degree=2, raw=T) * Insul to the whiteside data.\n\nlm2 &lt;- lm(Gas ~ poly(Temp, degree=2, raw=T) * Insul, \n          data=whiteside)\n\n\nExtract the coefficients vector (\\(\\widehat{\\beta}\\)) and the model matrix (\\(X\\)).\nExtract the estimate \\(\\widehat{\\sigma}\\) of Gaussian noise standard deviation from the model.\n\nYou may rename the components of \\(\\beta\\) and the columns of \\(X\\) up to your convenience.\n\n\n# A tibble: 6 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 Int           6.76      0.151      44.8   4.85e-42\n2 Temp^1       -0.318     0.0630     -5.04  6.36e- 6\n3 Temp^2       -0.00847   0.00662    -1.28  2.07e- 1\n4 After        -2.26      0.220     -10.3   6.52e-14\n5 After:Temp^1  0.180     0.0964      1.86  6.82e- 2\n6 After:Temp^2 -0.00651   0.00997    -0.653 5.17e- 1\n\n\n\n\nSimulate random data conforming to GLM with fixed design\nGenerate \\(N = 1000\\) instances of the Gaussian Linear Model defined by \\[\n\\begin{bmatrix} \\vdots \\\\\nY \\\\ \\vdots \\end{bmatrix}\n= \\begin{bmatrix} \\mbox{} & & \\mbox{} \\\\\n& X & \\\\\n&   & \\end{bmatrix} \\times \\widehat{\\beta} + \\widehat{\\sigma} \\times \\begin{bmatrix} \\vdots \\\\\n\\epsilon\\\\ \\vdots \\end{bmatrix}\n\\] where \\(\\epsilon \\sim \\mathcal{N}(0, \\text{Id}_{56})\\) and \\(X, \\widehat{\\beta}\\), and \\(\\widehat{\\sigma}\\) have been extracted above from the linear fit to the whiteside data.\n\n\n\n\n\n\nCaution\n\n\n\nTry to avoid unnecessary computations.\n\n\nFor each simulated instance, fit a linear model.\nNow you should have \\(N\\) identically distributed, independent realizations of the response vector \\(Y\\) (and with some more work, realizations of prediction vectors \\(\\widehat{Y}\\)). Denote the \\(N\\) independent realizations of the response vectors by \\(Y^*_1, \\ldots, Y^*_N\\), and denote the \\(N\\) realizations of the estimators \\(\\widehat{\\beta}^*_1, \\ldots, \\beta^*_{N}\\) and \\(\\widehat{\\sigma}^*_1, \\ldots, \\widehat{\\sigma}^*_N\\). Use the same style of notation for predictions and residuals.\nThe Statistical Inference course tells us a lot of things about the distribution of the response vectors, the prediction vectors, the residuals, and so on. Those theoretical results are used by function lm(), method summary.lm(), and diagnostic plot methods plot.lm() (and also by aov(), anova(), stepAIC(), …)\n\n\nDistribution of estimators of noise variance\nPlot your sample of estimators of the noise variance \\(\\widehat{\\sigma}^*_1, \\ldots, \\widehat{\\sigma}^*_N\\). Compare with the theoretical density of the distribution of these estimators (histograms, CDF, quantile plots).\nFor \\(\\alpha=5\\%, 1\\%\\), compute the \\(N\\) confidence regions for \\(\\beta\\) (\\(N\\) ellipsoids), and compute the empirical coverage of your confidence regions (the number of times the true parameter belongs to the confidence region). How should this empirical coverage be distributed? What is its expectation? its departure from expectation?\n\n\nFluctuations of coefficients estimates (I)\nAccording to the GLM, the distribution of the coefficients estimates \\(\\widehat{\\beta}^*_1, \\ldots, \\widehat{\\beta}^*_N\\) is known if the noise variance is known.\nVisualize the empirical joint distribution of \\((\\widehat{\\beta}^*_i[1], \\widehat{\\beta}^*_i[2])\\). Compare with theoretical distribution.\n\n\nFluctuations of coefficients estimates (II) : Studentized statistics\nIf the noise variance is not known, according to the GLM theory, we can use the noise variance estimator to build confidence regions.\nInvestigate and illustrate (histograms, CDF and quantiles plots) the distribution of the coefficients \\(\\frac{1}{\\widehat{\\sigma}}A \\times (\\widehat{\\beta}^*-\\widehat{\\beta})\\) where \\(A\\) is a well-chosen matrix (that may depend on the design).\n\n\nRegression of \\(\\widehat{\\beta}^*[6]\\) with respect to all other estimated coefficients \\(\\widehat{\\beta}^*[1,..,5]\\)\nThe sample of \\(N\\) realizations of \\(\\widehat{\\beta}^*\\) : \\(\\widehat{\\beta}^*_1, \\ldots, \\widehat{\\beta}^*_N\\) may be considered as an instance of linear regression with respect to a random design where the response variable is \\(\\widehat{\\beta}^*[6]\\) while the explanatory variables are \\(\\widehat{\\beta}^*[1], \\ldots, \\widehat{\\beta}^*[5]\\).\nCompute the optimal regression coefficients. What is the distribution of \\(\\widehat{\\beta}^*[6] - \\mathbb{E}\\left[ \\widehat{\\beta}^*[6] \\mid \\widehat{\\beta}^*[1], \\ldots, \\widehat{\\beta}^*[5] \\right]\\)? Investigate graphically.\n\n\nDiagnostic plots when the GLM assumptions hold\nPick \\(1\\) linear fit amongst the \\(N\\) linear fits performed on the simulated data. Draw the four diagnostic plots. Comment (briefly).\n\n\nOverparametrized model\nDefine \\(\\widehat{\\theta} \\in \\mathbb{R}^6\\) by zeroing the coefficients of \\(\\widehat{\\beta}\\) corresponding to the quadratic terms (with respect to Temp)\nGenerate \\(N = 1000\\) instances of the Gaussian Linear Model defined by \\[\n\\begin{bmatrix} \\vdots \\\\\nY \\\\ \\vdots \\end{bmatrix}\n= \\begin{bmatrix} \\mbox{} & & \\mbox{} \\\\\n& X & \\\\\n&   & \\end{bmatrix} \\times \\widehat{\\theta} + \\widehat{\\sigma} \\times \\begin{bmatrix} \\vdots \\\\\n\\epsilon\\\\ \\vdots \\end{bmatrix}\n\\] where \\(\\epsilon \\sim \\mathcal{N}(0, \\text{Id}_{56})\\).\n\n\nEstimators of noise variance\nFit all \\(N\\) realizations with the same formula as above. Compute the new estimators of the noise variance.\n\n\nStudent’s tests for coefficients\nPerform student’s tests for the coefficient vectors. How many times do you reject the null hypothesis concerning the coefficients of \\(\\widehat{\\beta}\\) corresponding to the quadratic terms (with respect to Temp) if you choose a size/level equal to \\(5\\%\\). Plot the sample of the \\(|t|\\) values for the coefficients of \\(\\widehat{\\beta}\\) corresponding to the quadratic terms (with respect to Temp).\nPlot the histogram of the empirical distrinbution of \\(p\\)-values. Compare with theoretical distribution of \\(p\\) values.\n\n\nFisher’s test(s)\nWe aim at testing\n\n\\(H_0\\) : \\(\\widehat{\\theta}[3] = \\widehat{\\theta}[6] = 0\\) (null hypothesis, assuming that the third and the sixth coefficients represent Temp^2 and Temp^2:InsulAfter)\n\nversus\n\n\\(H_1\\) : \\(\\widehat{\\theta}[3] \\neq 0 \\quad \\text{or} \\quad \\widehat{\\theta}[6] \\neq 0\\) (alternative)\n\nCompute the Fisher statistics for the \\(N\\) simulated response vectors (when the null hypothesis is true). Plot the Fisher statistics and compare to the theoretical distribution under the the null hypothesis. If you choose a level/size equal to \\(1\\%\\), how many times do you reject the null hypothesis?\nCompute the Fisher statistics for the \\(N\\) simulated response vectors (when the null hypothesis is not true). Plot the Fisher statistics and compare to the theoretical distribution under the the null hypothesis. If you choose a level/size equal to \\(1\\%\\), how many times do you reject the null hypothesis?\nAgain plot a histogram of the empirical distribution of \\(p\\)-values\n\n\nPerformance of stepAIC\nRun stepAIC() on the overparametrized models you obtain. Describe graphically the distribution of outcomes, and the distribution of the AIC criteria for the selected models.\n\n\nDeparting from the Gaussian Linear Model assumptions\nReplay the above described simulations but replace the Gaussian noise with Student’s noise with three degrees of freedom rt(N, df=3, ncp=0) (with and without overparametrizatio). Recompute the Fisher statistics for testing \\(H_0\\) against \\(H_1\\). Visualize the distributions compare to the theoretical distribution of the Fisher statistics. If you choose a level/size equal to \\(1\\%\\), how many times do you reject the null hypothesis?\n\n\nReferences\n\nPoly. S. Boucheron\nPoly. S. Coste\n\n\n\n Grading criteria\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nSpelling and syntax\n20%\nEnglish/French \n\n\nPlots correction\n25%\nchoice of aesthetics, geom, scale … \n\n\nComputing Statistics\n30%\nAggregations, LR, PCA, CA, … \n\n\nDRY compliance\n25%\nDRY principle at  Wikipedia"
  },
  {
    "objectID": "labs/lab-vctrs.html",
    "href": "labs/lab-vctrs.html",
    "title": "R programming: vectors",
    "section": "",
    "text": "Vectors in R\nvctrs package"
  },
  {
    "objectID": "labs/lab-vctrs.html#basic-atomic-vectors",
    "href": "labs/lab-vctrs.html#basic-atomic-vectors",
    "title": "R programming: vectors",
    "section": "Basic atomic vectors",
    "text": "Basic atomic vectors\nBasic atomic vectors are sequences of objects with the simplest storage modes.\n\n\n\n\n\nFigure 2: Common atomic vectors"
  },
  {
    "objectID": "labs/lab-vctrs.html#null-values",
    "href": "labs/lab-vctrs.html#null-values",
    "title": "R programming: vectors",
    "section": "Null values",
    "text": "Null values\n\n\n\n\n\n\nQuestion\n\n\n\nTry to determine which items in a vector are NULL\n\nx &lt;- c(NA, 3, NA, 7, 13)\nx == NA\n\nExplain the output. Fix it.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat happens when you combine (with c()) atomic vectors with different base types?\nx &lt;- c(1L:3L)\ny &lt;- letters[5:9]\nz &lt;- rep(c(TRUE, FALSE), 2)[1:3]\nx ; y ; c(x,y) ;  c(x,z) ; c(y,z)"
  },
  {
    "objectID": "labs/lab-vctrs.html#attributes",
    "href": "labs/lab-vctrs.html#attributes",
    "title": "R programming: vectors",
    "section": "Attributes",
    "text": "Attributes\n\n\n\n\n\n\nQuestion\n\n\n\nAttributes are metadata.\n\nx &lt;- as_date(\"2024-08-06\") + 1:7\nis_vector(x) ; is_atomic(x) ; class(x) ; typeof(x)\n\n[1] TRUE\n\n\n[1] TRUE\n\n\n[1] \"Date\"\n\n\n[1] \"double\"\n\nattributes(x)\n\n$class\n[1] \"Date\"\n\nnames(x) &lt;- wday(x, label=T, abbr=F)\n\nx\n\n    mercredi        jeudi     vendredi       samedi     dimanche        lundi \n\"2024-08-07\" \"2024-08-08\" \"2024-08-09\" \"2024-08-10\" \"2024-08-11\" \"2024-08-12\" \n       mardi \n\"2024-08-13\" \n\nattributes(x)\n\n$class\n[1] \"Date\"\n\n$names\n[1] \"mercredi\" \"jeudi\"    \"vendredi\" \"samedi\"   \"dimanche\" \"lundi\"    \"mardi\"   \n\nx[[\"mercredi\"]]\n\n[1] \"2024-08-07\"\n\nattr(x, \"names\")\n\n[1] \"mercredi\" \"jeudi\"    \"vendredi\" \"samedi\"   \"dimanche\" \"lundi\"    \"mardi\""
  },
  {
    "objectID": "labs/lab-vctrs.html#examples",
    "href": "labs/lab-vctrs.html#examples",
    "title": "R programming: vectors",
    "section": "Examples",
    "text": "Examples\nFactors have basetype integer and attribute factor and levels.\n\nctr_names &lt;- factor(ISOcodes::ISO_3166_1$Name)\n\nctr_names |&gt; \n  str()\n\n Factor w/ 249 levels \"Afghanistan\",..: 13 1 7 8 2 3 6 234 11 12 ...\n\n\n\nclass(ctr_names); str(attributes(ctr_names))\n\n[1] \"factor\"\n\n\nList of 2\n $ levels: chr [1:249] \"Afghanistan\" \"Åland Islands\" \"Albania\" \"Algeria\" ...\n $ class : chr \"factor\"\n\n\n\nctr_names |&gt; \n  unclass()  |&gt; \n  str()\n\n int [1:249] 13 1 7 8 2 3 6 234 11 12 ...\n - attr(*, \"levels\")= chr [1:249] \"Afghanistan\" \"Åland Islands\" \"Albania\" \"Algeria\" ...\n\n\nExamples of important S3 classes\n\nlm\nkmeans\nprcomp\nhclust\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain\n\nctr_names |&gt;\n  str_to_upper() |&gt;\n  str()\n\n chr [1:249] \"ARUBA\" \"AFGHANISTAN\" \"ANGOLA\" \"ANGUILLA\" \"ÅLAND ISLANDS\" ..."
  },
  {
    "objectID": "labs/lab-vctrs.html#history",
    "href": "labs/lab-vctrs.html#history",
    "title": "R programming: vectors",
    "section": "History",
    "text": "History"
  },
  {
    "objectID": "labs/lab-vctrs.html#relevance-s3-generics",
    "href": "labs/lab-vctrs.html#relevance-s3-generics",
    "title": "R programming: vectors",
    "section": "Relevance: S3 generics",
    "text": "Relevance: S3 generics\nAn S3 object behaves differently from its underlying base type when it is passed to a generic function.\nWhat is a generic?\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat happens if an S3 object is passed to a generic?\nWhat is method dispatch?\nWhat kind of MRO (Method Resolution Order) is used?\nHow would you register a new method for a generic?\nHow do you define a generic?\nGive examples of generics in base R.\nGet the list of base R functions which are generics.\n\n\n\n\n\n\n\n\n\nQuestion"
  },
  {
    "objectID": "labs/lab-vctrs.html#preserving-attributes",
    "href": "labs/lab-vctrs.html#preserving-attributes",
    "title": "R programming: vectors",
    "section": "Preserving attributes",
    "text": "Preserving attributes"
  },
  {
    "objectID": "labs/lab-vctrs.html#combining-vectors-using-c",
    "href": "labs/lab-vctrs.html#combining-vectors-using-c",
    "title": "R programming: vectors",
    "section": "Combining vectors using c()\n",
    "text": "Combining vectors using c()"
  },
  {
    "objectID": "labs/lab-vctrs.html#creating-a-new-s3-vector-class",
    "href": "labs/lab-vctrs.html#creating-a-new-s3-vector-class",
    "title": "R programming: vectors",
    "section": "Creating a new S3 vector class",
    "text": "Creating a new S3 vector class\nIn package nycflights13, in tibble flights, columns with names ending with dep_time or arr_time have basetype integer.\n\nstopifnot(\n  require(nycflights13)\n)\n\nLoading required package: nycflights13\n\nflights |&gt; \n  select(ends_with('_time')) |&gt;\n  glimpse()\n\nRows: 336,776\nColumns: 5\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n\n\nNevertheless, these columns encode time information (hour, minute, second) in an unusual way. The last two digits represent minutes, the leading digits represent hours. In the sequel, we define an S3 vector class with basetype integer that will allow us to handle these columns in a transparent way. Desirable properties are\n\nReadable display: 517 should be displayed as 5h17m\n\nSome time arithmetics should be possible: we should be able either to add difftime or to compute the difference between dep_time and sched_dep_time\n\nSome validation should be possible: 2517 is not a valid value for dep_time\n\nCasting to datetime should be easy\nCasting from datetime should be easy as well\n…\n\nWe use the tools from article S3 vectors\n\n\n\n\n\n\nQuestion\n\n\n\nCreate a new S3 vector class called weird_tm. Endow it with a constructor new_weird_tm(), an helper weird_tm(), a test is_weird_tm().\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDefine a format() function for class weird_tm. Mind NAs.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCasting and coercion\nThe next piece of code does not work\nc(weird_tm(flights$dep_time[1:5]), flights$dep_time[1:5])\nWe need to define casting methods for generics vec_cast() and vec_ptype2() at least for casting to integer and character.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nTransform the tibble flights so that columns with name ending with _time (except air_time) have type weird_time. Is it still possible to filter rows with dep_time is a prescribed time interval.\n\n\nWe will use tools from vctrs to define differences between weird_tm objects.\n\n\n\n\n\n\nQuestion\n\n\n\nDouble dispatch.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDefine the difference - operator for two vectors of class weird_tm. The result is expected to be an integer vector."
  },
  {
    "objectID": "labs/lab-vctrs.html#section-1",
    "href": "labs/lab-vctrs.html#section-1",
    "title": "R programming: vectors",
    "section": "",
    "text": "Object Oriented Programming in R part 1 to …"
  },
  {
    "objectID": "labs/lab-ca-mortality.html",
    "href": "labs/lab-ca-mortality.html",
    "title": "Correspondence Analysis of Mortality Data",
    "section": "",
    "text": "Code\nstopifnot(\n  require(tidyverse),\n  require(patchwork),\n  require(httr),\n  require(glue),\n  require(broom),\n  require(DT),\n  require(GGally),\n  require(ggforce),\n  require(ggfortify),\n  require(testthat),\n  require(viridisLite)\n)\n\ntidymodels::tidymodels_prefer(quiet = TRUE)\nCode\nold_theme &lt;- theme_set(\n  theme_minimal(base_size=9, \n                base_family = \"Helvetica\")              \n  )"
  },
  {
    "objectID": "labs/lab-ca-mortality.html#the-mortality-dataset",
    "href": "labs/lab-ca-mortality.html#the-mortality-dataset",
    "title": "Correspondence Analysis of Mortality Data",
    "section": "The mortality dataset",
    "text": "The mortality dataset\nThe goal is to investigate a possible link between age group and cause of death. We work with dataset mortality from package FactoMineR\n\n\nCode\nstopifnot(\n  require(FactoMineR),\n  require(factoextra),\n  require(FactoInvestigate)\n)\n## Loading required package: FactoMineR\n## Loading required package: factoextra\n## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n## Loading required package: FactoInvestigate\n\ndata(\"mortality\", package = \"FactoMineR\")\n\n\n\nCode\n#help(mortality)\n\n\nA data frame with 62 rows (the different causes of death) and 18 columns. Each column corresponds to an age interval (15-24, 25-34, 35-44, 45-54, 55-64, 65-74, 75-84, 85-94, 95 and more) in a year. The 9 first columns correspond to data in 1979 and the 9 last columns to data in 2006. In each cell, the counts of deaths for a cause of death in an age interval (in a year) is given.\n\n\nSource\n\nCentre d’épidemiologie sur les causes de décès médicales\n\n\nSee also EuroStat:\n\nCauses of death (hlth_cdeath) Reference Metadata in Single Integrated Metadata Structure (SIMS)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nRead the documentation of the mortality dataset. Is this a sample? an aggregated dataset?\nIf you consider mortality as an agregated dataset, can you figure out the organization of the sample mortality was built from?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCompute the marginal counts for each year (1979, 2006). Compare."
  },
  {
    "objectID": "labs/lab-ca-mortality.html#correspondance-analysis",
    "href": "labs/lab-ca-mortality.html#correspondance-analysis",
    "title": "Correspondence Analysis of Mortality Data",
    "section": "Correspondance Analysis",
    "text": "Correspondance Analysis\n\n\n\n\n\n\nCA executive summary\n\n\n\n\nStart from a 2-way contingency table \\(X\\) with \\(\\sum_{i,j} X_{i,j}=N\\)\nNormalize \\(P = \\frac{1}{N}X\\) (correspondance matrix)\nLet \\(r\\) (resp. \\(c\\)) be the row (resp. column) wise sums vector\nLet \\(D_r=\\text{diag}(r)\\) denote the diagonal matrix with row sums of \\(P\\) as coefficients\nLet \\(D_c=\\text{diag}(c)\\) denote the diagonal matrix with column sums of \\(P\\) as coefficients\nThe row profiles matrix is \\(D_r^{-1} \\times P\\)\nThe standardized residuals matrix is \\(S = D_r^{-1/2} \\times \\left(P - r c^T\\right) \\times D_c^{-1/2}\\)\n\nCA consists in computing the SVD of the standardized residuals matrix \\(S =  U  \\times D \\times V^T\\)\nFrom the SVD, we get - \\(D_r^{-1/2} \\times U\\) standardized coordinates of rows - \\(D_c^{-1/2} \\times V\\) standardized coordinates of columns - \\(D_r^{-1/2} \\times U \\times D\\) principal coordinates of rows - \\(D_c^{-1/2} \\times V \\times D\\) principal coordinates of columns - Squared singular values: the principal inertia\nWhen calling svd(.), the argument should be \\[D_r^{1/2}\\times \\left(D_r^{-1} \\times P \\times D_c^{-1}- \\mathbf{I}\\times \\mathbf{I}^T  \\right)\\times D_c^{1/2}\\]\n\n\n\n\n\n\n\n\nCA and extended SVD\n\n\n\nAs \\[D_r^{-1} \\times P \\times D_c^{-1} - \\mathbf{I}\\mathbf{I}^T = (D_r^{-1/2} \\times U)\\times D \\times (D_c^{-1/2}\\times V)^T\\]\n\\((D_r^{-1/2} \\times U)\\times D \\times (D_c^{-1/2}\\times V)^T\\) is the extended SVD of \\[D_r^{-1} \\times P \\times D_c^{-1} - \\mathbf{I}\\mathbf{I}^T\\] with respect to \\(D_r\\) and \\(D_c\\)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPerform CA on the two contingency tables.\n\n\n\n\n\n\n\n\nYou may use FactoMineR::CA(). It is interesting to compute the correspondence analysis in your own way, by preparing the matrix that is handled to svd() and returning a named list containing all relevant information.\n\nDo the Jedi and Sith build their own light sabers? Jedi do. It’s a key part of the religion to have a kyber crystal choose you, to build the saber through the power of the force creating a blade unique and in tune with them\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIf you did use FactoMineR::CA(), explain the organization of the result."
  },
  {
    "objectID": "labs/lab-ca-mortality.html#screeplots",
    "href": "labs/lab-ca-mortality.html#screeplots",
    "title": "Correspondence Analysis of Mortality Data",
    "section": "Screeplots",
    "text": "Screeplots\n\n\n\n\n\n\nQuestion\n\n\n\nDraw screeplots. Why are they useful? Comment briefly."
  },
  {
    "objectID": "labs/lab-ca-mortality.html#row-profiles-analysis",
    "href": "labs/lab-ca-mortality.html#row-profiles-analysis",
    "title": "Correspondence Analysis of Mortality Data",
    "section": "Row profiles analysis",
    "text": "Row profiles analysis\n\n\n\n\n\n\nQuestion\n\n\n\nPerform row profiles analysis.\nWhat are the classical plots? How can you build them from the output of FactoMiner::CA?\nBuild the table of row contributions (the so-called \\(cos^2\\))"
  },
  {
    "objectID": "labs/lab-ca-mortality.html#column-profiles-analysis",
    "href": "labs/lab-ca-mortality.html#column-profiles-analysis",
    "title": "Correspondence Analysis of Mortality Data",
    "section": "Column profiles analysis",
    "text": "Column profiles analysis"
  },
  {
    "objectID": "labs/lab-ca-mortality.html#symmetric-plots",
    "href": "labs/lab-ca-mortality.html#symmetric-plots",
    "title": "Correspondence Analysis of Mortality Data",
    "section": "Symmetric plots",
    "text": "Symmetric plots\n\n\n\n\n\n\nQuestion\n\n\n\nBuild the symmetric plots (biplots) for correspondence analysis of Mortalitity data"
  },
  {
    "objectID": "labs/lab-ca-mortality.html#mosaicplots",
    "href": "labs/lab-ca-mortality.html#mosaicplots",
    "title": "Correspondence Analysis of Mortality Data",
    "section": "Mosaicplots",
    "text": "Mosaicplots\n\n\n\n\n\n\nQuestion\n\n\n\nMosaic plots provide an alternative way of exploring contingency tables. They are particularly handy when handling 2-way contingency tables.\nDraw mosaic plots for the two contingency tables living inside mortality datasets.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAre you able to deliver an interpretation of this Correspondence Analysis?"
  },
  {
    "objectID": "labs/lab-ca-mortality.html#hierarchical-clusetring-of-row-profiles",
    "href": "labs/lab-ca-mortality.html#hierarchical-clusetring-of-row-profiles",
    "title": "Correspondence Analysis of Mortality Data",
    "section": "Hierarchical clusetring of row profiles",
    "text": "Hierarchical clusetring of row profiles\n\n\n\n\n\n\nQuestion\n\n\n\nBuild the standardized matrix for row profiles analysis. Compute the pairwise distance matrix using the \\(\\chi^2\\) distances. Should you work centered row profiles?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPerform hierarchical clustering of row profiles with method/linkage \"single\". Check the definition of the method. Did you know the underlying algorithm? If yes, in which context did you get acquainted with this algorithm?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nChoose the number of classes (provide justification).\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCan you xxplain the size of the different classes in the partition?"
  },
  {
    "objectID": "labs/lab-ca-mortality.html#atypical-row-profiles",
    "href": "labs/lab-ca-mortality.html#atypical-row-profiles",
    "title": "Correspondence Analysis of Mortality Data",
    "section": "Atypical row profiles",
    "text": "Atypical row profiles\n\n\n\n\n\n\nQuestion\n\n\n\nRow profiles that do not belong to the majority class are called atypical.\n\nCompute the share of inertia of atypical row profiles.\nDraw a symmetric plot (biplot) outlining the atypical row profiles."
  },
  {
    "objectID": "labs/lab-ca-mortality.html#investigating-independenceassociation",
    "href": "labs/lab-ca-mortality.html#investigating-independenceassociation",
    "title": "Correspondence Analysis of Mortality Data",
    "section": "Investigating independence/association",
    "text": "Investigating independence/association\n\n\n\n\n\n\nQuestion\n\n\n\n\nCalculate the theoretical population table for deces. Do you possible to carry out a chi-squared test?\nPerform a hierarchical classification of the line profiles into two classes.\nMerge the rows of deces corresponding to the same class (you can use the the tapply function), and perform a chi-square test. chi-square test. What’s the conclusion?\nWhy is it more advantageous to carry out this grouping into two classes compared to arbitrarily grouping two classes, in order to prove the dependence between these two variables?"
  },
  {
    "objectID": "labs/lab-ca-mortality.html#about-the-average-profile",
    "href": "labs/lab-ca-mortality.html#about-the-average-profile",
    "title": "Correspondence Analysis of Mortality Data",
    "section": "About the “average profile”",
    "text": "About the “average profile”\n\n\n\n\n\n\nQuestion\n\n\n\n\nRepresent individuals from the majority class. Do they all seem to you to correspond to an average profile?\nTry to explain this phenomenon considering the way in which hierarchical classification uses the Single Linkage method.\n\n\n\n\n\n\n\n\n\nCaveat\n\n\n\nThe mortality dataset should be taken with grain of salt. Assigning a single cause to every death is not a trivial task. It is even questionable: if somebody dies from some infection because she could not be cured using an available drug due to another preexisting pathology, who is the culprit?"
  },
  {
    "objectID": "labs/lab-pca-misc.html",
    "href": "labs/lab-pca-misc.html",
    "title": "PCA I: Up and running",
    "section": "",
    "text": "Codestopifnot(\n  require(broom),\n  require(FactoInvestigate),\n  require(FactoMineR),\n  require(ggfortify),\n  require(ggrepel),\n  require(glue),\n  require(httr),\n  require(patchwork),\n  require(tidyverse)\n)\n\nold_theme &lt;- theme_set(theme_minimal())\nCodeopts &lt;- options()  # save old options\n\noptions(ggplot2.discrete.colour=\"viridis\")\noptions(ggplot2.discrete.fill=\"viridis\")\noptions(ggplot2.continuous.fill=\"viridis\")\noptions(ggplot2.continuous.colour=\"viridis\")"
  },
  {
    "objectID": "labs/lab-pca-misc.html#two-perspectives-on-pca-and-svd",
    "href": "labs/lab-pca-misc.html#two-perspectives-on-pca-and-svd",
    "title": "PCA I: Up and running",
    "section": "Two perspectives on PCA (and SVD)",
    "text": "Two perspectives on PCA (and SVD)\n\n\n\n\n\n\nFaithful projection\n\n\n\nLet \\(X\\) be a \\(n \\times p\\) matrix, a reasonable task consists in finding a \\(k \\lll p\\)-dimensional subspace \\(E\\) of \\(\\mathbb{R}^p\\) such that\n\\[X \\times \\Pi_E\\]\nis as close as possible from \\(X\\). By as close as possible, we mean that the sum of squared Eudlidean distances between the rows of \\(X\\) and the rows of \\(X \\times \\Pi_E\\) is as small as possible\nPicking \\(E\\) as the subspace generated by the first \\(k\\) right-singular vectors of \\(X\\) solves the problem\n\n\n\n\n\n\n\n\nMaximizing the projected variance\n\n\n\nLet \\(X\\) be a \\(n \\times p\\) matrix, a reasonable task consists in finding a \\(k \\lll p\\)-dimensional subspace \\(E\\) of \\(\\mathbb{R}^p\\) such that\n\\[X \\times \\Pi_E\\]\nretains as much variance as possible.\nThis means maximizing\n\\[\\operatorname{var}\\left(X \\times \\Pi_E\\right)\\]"
  },
  {
    "objectID": "labs/lab-pca-misc.html#pca-up-and-running",
    "href": "labs/lab-pca-misc.html#pca-up-and-running",
    "title": "PCA I: Up and running",
    "section": "PCA up and running",
    "text": "PCA up and running\nWe will walk through Principal Component Analysis using historical datasets\n\nUSArrests: crime data across states (individuals) in USA\niris: morphological data of 150 flowers (individuals) from genus iris\ndecathlon: scores of athletes at the ten events making a decathlon\nLife tables for a collection of Western European countries and the USA\n\n\n\n\n\n\n\nSome datasets have non-numerical columns.\nSuch columns are not used when computing the SVD that underpins PCA\nBut non-numerical columns may be incorporated into PCA visualization\n\n\n\nWhen PCA is used as a feature engineering device to prepare the dataset for regression, supervised classification, clustering or any other statistical/machine learning treatment, visualizing the interplay between non-numerical columns and the numerical columns constructed by PCA is crucial\nIris flower dataset\nA showcase for Principal Component Analysis\n\nThe Iris flower data set or Fisher’s Iris data set is a multivariate data set introduced by Ronald Fisher in 1936. Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the Gaspé Peninsula “all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus”\n\n\nThe data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters\n\nWikipedia\n\nUSArrests\n\nCodedata(USArrests)\n\nUSArrests %&gt;%\n  sample_n(10) %&gt;%\n  knitr::kable()\n\n\n\n\nMurder\nAssault\nUrbanPop\nRape\n\n\n\nNevada\n12.2\n252\n81\n46.0\n\n\nCalifornia\n9.0\n276\n91\n40.6\n\n\nColorado\n7.9\n204\n78\n38.7\n\n\nIdaho\n2.6\n120\n54\n14.2\n\n\nTennessee\n13.2\n188\n59\n26.9\n\n\nVirginia\n8.5\n156\n63\n20.7\n\n\nArizona\n8.1\n294\n80\n31.0\n\n\nOregon\n4.9\n159\n67\n29.3\n\n\nFlorida\n15.4\n335\n80\n31.9\n\n\nKansas\n6.0\n115\n66\n18.0\n\n\n\n\n\n\nCodeUSArrests %&gt;%\n  rownames_to_column() %&gt;%\n  ggplot() +\n  aes(x = Assault, y = Murder) +\n  aes(colour = UrbanPop, label=rowname) +\n  geom_point(size = 5) +\n  geom_text_repel(box.padding = unit(0.75, \"lines\"))\n\nWarning: ggrepel: 22 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\nDecathlon\n\nCodedata(decathlon)\n\ndecathlon %&gt;%\n  rownames_to_column() %&gt;%\n  dplyr::select(-c(`400m`, `110m.hurdle`, `Shot.put`,`Javeline`, `1500m`, `Rank`, `Competition`, `Points`)) %&gt;%\n  sample_n(10) %&gt;%\n  knitr::kable()\n\n\n\nrowname\n100m\nLong.jump\nHigh.jump\nDiscus\nPole.vault\n\n\n\nHERNU\n11.37\n7.56\n1.86\n44.99\n4.82\n\n\nKARPOV\n11.02\n7.30\n2.04\n48.95\n4.92\n\n\nTuri\n11.08\n6.91\n2.03\n39.83\n4.80\n\n\nWARNERS\n11.11\n7.60\n1.98\n41.10\n4.92\n\n\nUldal\n11.23\n6.99\n1.85\n43.01\n4.50\n\n\nMARTINEAU\n11.64\n6.81\n1.95\n47.60\n4.92\n\n\nKarlivans\n11.33\n7.26\n1.97\n43.34\n4.50\n\n\nNOOL\n11.33\n7.27\n1.98\n37.92\n4.62\n\n\nWarners\n10.62\n7.74\n1.97\n43.73\n4.90\n\n\nCasarsa\n11.36\n6.68\n1.94\n48.66\n4.40\n\n\n\n\n\n\nCodedecathlon %&gt;%\n  rownames_to_column() %&gt;%\n  ggplot() +\n  aes(x = `100m`, y = Long.jump) +\n  aes(colour = Points, label=rowname) +\n  geom_point(size = 5) +\n  geom_smooth(se = FALSE) +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  geom_text_repel(box.padding = unit(0.75, \"lines\"))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation: colour\nand label.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation: colour\nand label.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: ggrepel: 9 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\nUsing prcomp from base \n\n\nThe calculation is done by a singular value decomposition of the (centered and possibly scaled) data matrix, not by using eigen on the covariance matrix.\n\n\nThis is generally the preferred method for numerical accuracy.\n\n\nThe print method for these objects prints the results in a nice format and the plot method produces a scree plot\n\nAt first glance, performing Principal Component Analysis consists in applying SVD to the \\(n  \\times p\\) data matrix.\nRaw PCA on iris dataset\n\nCodepca_iris &lt;- iris %&gt;%\n  select(- Species) %&gt;%\n  prcomp(x = .,          #&lt;&lt;\n         center = FALSE, #&lt;&lt;\n         scale. = FALSE) #&lt;&lt;\n\nclass(pca_iris) ; is.list(pca_iris)\n\n[1] \"prcomp\"\n\n\n[1] TRUE\n\nCodenames(pca_iris)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\nResult has 5 components:\n\nsdev: singular values\nrotation: orthogonal matrix \\(V\\) in SVD\ncenter: FALSE or centering row vector\nscale: FALSE of scaling row vector\nx: matrix \\(U \\times D\\) from SVD\nFrom data to PCA and back\nLet\n\n\n\\(Z\\) be the \\(n \\times p\\) matrix fed to prcomp\n\n\n\\(X\\) be the \\(n \\times p\\) matrix forming component x of the result\n\n\\(V\\) be the \\(p \\times p\\) matrix forming component rotation of the result\n\n\\(\\mu\\) be the centering vector, \\(\\mathbf{0}\\) if center=FALSE\n\n\n\\(\\sigma\\) be the scaling vector, \\(\\mathbf{1}\\) is scale.=FALSE\n\n\nthen\n\\[\\text{diag}(\\sigma) \\times (Z - \\mathbf{1} \\times \\mu^T) =  X \\times V^T\\]\nor\n\\[Z  =  \\text{diag}(\\sigma)^{-1} \\times  X \\times V^T +  \\mathbf{1} \\times \\mu^T\\]\nPCA can be pictured as follows\n\noptional centering and scaling\nperforming SVD\ngathering the byproducts of SVD so as to please statisticians\npossibly truncating the result so as to obtain a truncated SVD\nGlossary\nThe columns of components x are called the principal components of the dataset\n Note that the precise connection between the principal components and the dataset depend on centering and scaling\nConventions\n\n\nPCA vocabulary\nSVD vocabulary\n\n\n\nPrincipal axis\nLeft singular vector\n\n\nPrincipal component\nScaled left singular vector\n\n\nFactorial axis\nRight singular vector\n\n\nComponent of inertia\nSquared singular value\n\n\nModus operandi\n\nRead the data.\nChoose the active individuals and variables (cherrypick amongs numeric variables)\nChoose if the variables need to be standardised\nChoose the number of dimensions you want to keep (the rank)\nAnalyse the results\nAutomatically describe the dimensions of variability\nBack to raw data.\nImpact of standardization and centering\nWhen performing PCA, once the active variables have been chosen, we wonder whether we should centre and/or standardize the columns\n\n\n\n\n\n\nPCA investigates the spread of the data not their location\nCentering is performed by default when using prcomp\n\n\n\nCentering/Scaling: iris dataset\nOn iris dataset Centering and standardizing the columns slightly spreads out the eigenvalues\n\nCodeiris &lt;- datasets::iris\niris_num &lt;- dplyr::select(iris, -Species)\n\nlist_pca_iris &lt;-\n  map2(.x=rep(c(FALSE, TRUE), 2),\n       .y=rep(c(FALSE, TRUE), c(2,2)),\n       ~ prcomp(iris_num, center= .x, scale.=.y)\n)\n\nnames(list_pca_iris) &lt;- stringr::str_c(\"pca_iris\",\n    c(\"\", \"c\", \"s\", \"c_s\"),\n    sep=\"_\")\n\n\n\nCodeconfig_param &lt;- as.vector(outer(c(\"_\", \"center\"),\n                          c(\"_\", \"scale\"), FUN=paste)) %&gt;%\n                rep( c(4,4,4,4))\n\nlist_pca_iris %&gt;%\n  purrr::map_dfr(~ broom::tidy(., matrix=\"pcs\")) %&gt;%\n  mutate(Parameter=config_param) -&gt; df_pca_iris_c_s\n\np_pca_c_s &lt;- df_pca_iris_c_s %&gt;%\n  ggplot() +\n  aes(x=PC, y=percent, fill=Parameter) +\n  geom_col(position=\"dodge\")\n\np_pca_c_s +\n  labs(title=\"Iris : share of inertia per PC\",\n       subtitle = \"Centering, scaling or not\")\n\n\n\n\n\n\n\nMore on the centering/scaling dilemma\n\nCodeusarrests_num &lt;- USArrests\ndata(decathlon)\n\nlist_pca_usarrests &lt;-\n  map2(.x=rep(c(FALSE, TRUE), 2),\n       .y=rep(c(FALSE, TRUE), c(2,2)),\n       ~ prcomp(USArrests, center= .x, scale.=.y)\n)\n\nlist_pca_decathlon &lt;-\n  map2(.x=rep(c(FALSE, TRUE), 2),\n       .y=rep(c(FALSE, TRUE), c(2,2)),\n       ~ prcomp(decathlon[, 1:10], center= .x, scale.=.y)\n)\n\nnames(list_pca_usarrests) &lt;- c(\"pca_usarrests\", \"pca_usarrests_c\", \"pca_usarrests_s\", \"pca_usarrests_c_s\")\n\nnames(list_pca_decathlon) &lt;- c(\"pca_decathlon\", \"pca_decathlon_c\", \"pca_decathlon_s\", \"pca_decathlon_c_s\")\n\n\n\n\n\n\n\n\nFor USArrests, scaling is mandatory\n\n\n\n\nCodelist_pca_usarrests %&gt;%\n  purrr::map_dfr(~ broom::tidy(., matrix=\"pcs\")) %&gt;%\n  mutate(Parameter=config_param) -&gt; df_pca_usarrests_c_s\n\np_pca_c_s %+%\n  df_pca_usarrests_c_s  +\n  labs(title=\"USArrests : share of inertia per PC\",\n       subtitle = \"Centering, scaling or not\")\n\n\n\n\n\n\n\nFor decathlon, centering is mandatory, scaling is recommended\n\nCodeconfig_param_deca &lt;- as.vector(outer(c(\"_\", \"center\"),\n                          c(\"_\", \"scale\"), FUN=paste)) %&gt;%\n                rep(rep(10,4))\n\nlist_pca_decathlon %&gt;%\n  purrr::map_dfr(~ broom::tidy(., matrix=\"pcs\")) %&gt;%\n  mutate(Parameter=config_param_deca) -&gt; df_pca_decathlon_c_s\n\np_pca_c_s %+%\n  df_pca_decathlon_c_s  +\n  labs(title=\"Decathlon : share of inertia per PC\",\n       subtitle = \"Centering, scaling or not\")\n\n\n\n\n\n\n\nIn order to investigate the columns means, there is no need to use PCA. Centering is almost always relevant.\nThe goal of PCA is to understand the fluctuations of the data and, when possible, to show that most fluctuations can be found in a few priviledged directions\nScaling is often convenient: it does not alter correlations. Second, high variance columns artificially capture an excessive amount of inertia, without telling us much about the correlations between variables"
  },
  {
    "objectID": "labs/lab-pca-misc.html#visualizing-pca-1",
    "href": "labs/lab-pca-misc.html#visualizing-pca-1",
    "title": "PCA I: Up and running",
    "section": "Visualizing PCA",
    "text": "Visualizing PCA\nRecall Joliffe’s big picture\n\nVisualize the correlation plot\nVisualize the share of inertia per principal component (screeplot)\nVisualize the individuals in the factorial planes generated by the leading principal axes\nVisualize the (scaled) orginal variables in the basis defined by the factorial axes (correlation circle)\nVisualize simultaneously individuals and variables (biplot)\n\nInspecting the correlation matrix\nUsing corrr::...\n\nCodeiris %&gt;%\n  dplyr::select(where(is.numeric)) %&gt;%\n  corrr::correlate() -&gt; tb #&lt;&lt;\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\nCodetb %&gt;%\n  corrr::shave() %&gt;%       #&lt;&lt;\n  corrr::fashion() %&gt;%     #&lt;&lt;\n  knitr::kable(format=\"markdown\")\n\n\n\nterm\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\n\n\n\nSepal.Length\n\n\n\n\n\n\nSepal.Width\n-.12\n\n\n\n\n\nPetal.Length\n.87\n-.43\n\n\n\n\nPetal.Width\n.82\n-.37\n.96\n\n\n\n\n\n\nAbout corrr::...\nPackage corrr\n\ncorrelate explores pairwise correlations just as cor\nIt outputs a dataframe (a tibble)\nThe corrr API is designed with data pipelines in mind\ncorrr offers functions for manipulating correlation matrices\n\nOther plots\n\nCodetb %&gt;%\n  corrr::rearrange(absolute = FALSE) %&gt;%\n  corrr::shave(upper = FALSE) %&gt;%\n  corrr::rplot()  #&lt;&lt;\n\n\n\n\n\n\n\nCorrelation plot for decathlon\n\n\nCodedecathlon  %&gt;%\n  rownames_to_column(\"Name\") %&gt;%     #&lt;&lt; tidyverse does not like rownames\n  mutate(across(-c(Name, Rank, Points, Competition),\n                ~ (.x -mean(.x))/sd(.x))) -&gt; decathlonR2\n\ndecathlonR2 %&gt;%\n  dplyr::select(-c(Name, Rank, Points, Competition)) %&gt;%\n  corrr::correlate(quiet = TRUE, method=\"pearson\") %&gt;%\n  mutate(across(where(is.numeric), abs)) %&gt;%\n  corrr::rearrange() %&gt;%\n  corrr::shave() -&gt; df_corr_decathlon\n\ndf_corr_decathlon %&gt;%\n  corrr::rplot() +\n  coord_fixed() +\n  ggtitle(\"Absolute correlation plot for Decathlon data\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\nIf our goal is to partitition the set of columns into tightly connected groups, this correlation plot already tells us a story:\n\nshort running races 100m, …, 400m along long jump form a cluster,\nshot.put, discus, and to a lesser extent high jump form another cluster\npole.vault, 1500m and javeline look pretty isolated\nPCA Visualization : first goals\n\nLook at the data in PC coordinates (Factorial plane)\nLook at the rotation matrix (Factorial axes)\nLook at the variance explained by each PC (Screeplot)\nInspect eigenvalues\n\nCodeiris_pca &lt;- list_pca_iris[[4]]\n\niris_pca %&gt;%\n  broom::tidy(matrix=\"pcs\") %&gt;%\n  knitr::kable(format=\"markdown\", digits=2)\n\n\n\nPC\nstd.dev\npercent\ncumulative\n\n\n\n1\n1.71\n0.73\n0.73\n\n\n2\n0.96\n0.23\n0.96\n\n\n3\n0.38\n0.04\n0.99\n\n\n4\n0.14\n0.01\n1.00\n\n\n\n\nCodeiris_plus_pca &lt;- broom::augment(iris_pca, iris)\n\n\nScatterplots\n\nCodeshare_variance &lt;- broom::tidy(iris_pca, \"pcs\")[[\"percent\"]]\n\niris_plus_pca %&gt;%\n  ggplot() +\n  aes(x=.fittedPC1, y=.fittedPC2) +\n  aes(colour=Species) +\n  geom_point() +\n  ggtitle(\"Iris data projected on first two principal axes\") +\n  xlab(paste(\"PC1 (\", share_variance[1], \"%)\", sep=\"\")) +\n  ylab(paste(\"PC2 (\", share_variance[2], \"%)\", sep=\"\")) -&gt; p\n\niris_plus_pca %&gt;%\n  ggplot() +\n  aes(y=Sepal.Width, x=Petal.Length) +\n  aes(colour=Species) +\n  geom_point() +\n  ggtitle(\"Iris data projected on Petal length and Sepal width\") -&gt; q\n\n\n\nCodep ; q\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiplot : overplotting the original variables in the factorial plane\n\nCodetotal_inertia &lt;- sum(share_variance)\n\np +\n  aes(label=.rownames) +\n  geom_text_repel(verbose = FALSE) +\n  coord_fixed()\n\nWarning: ggrepel: 70 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\nSee plot_pca\nThis is a scatterplot on the plane spanned by the two leading principal components\n\nCodeiris %&gt;%\n  select(where(is.numeric)) %&gt;%\n  prcomp(.scale=TRUE) %&gt;%\n  autoplot(                 #&lt;&lt;\n   data = iris,\n   colour = 'Species',\n   label = TRUE,            #&lt;&lt;\n   loadings = TRUE,         #&lt;&lt;\n   loadings.colour = 'blue',\n   loadings.label = TRUE\n  ) +\n  ggtitle(\"Iris dataset biplot\")\n\nWarning: In prcomp.default(., .scale = TRUE) :\n extra argument '.scale' will be disregarded\n\n\n\n\n\n\n\n\nScatterplot of the factorial plane is overplotted with loadings corresponding to native variables\n!\n\nCoderequire(plotly)\n\n(\nautoplot(prcomp(iris[,-5]),\n         data = iris,\n         colour = 'Species',\n         label = TRUE,\n         loadings = TRUE,\n         loadings.label = TRUE) +\n  ggtitle(\"Iris dataset \")\n) %&gt;% ggplotly()\n\n\nOfficial plots for PCA on iris dataset\n\n\n\nCodeplot(list_pca_iris[[4]])\n\n\n\n\n\n\n\n\n\nCodebiplot(list_pca_iris[[4]])\n\n\n\n\n\n\n\n\n\nPlotting PCA on decathlon data\n\nCodedecathlon_pca &lt;- list_pca_decathlon[[4]]\ninertias &lt;- decathlon_pca$sdev^2\nshare_variance &lt;- round((100*(inertias)/\n                         sum(inertias)), 2)\n\np %+% broom::augment(decathlon_pca, decathlon) +\n  aes(colour=Points) +\n  ggtitle(\"Decathlon data\") +\n  xlab(paste(\"PC1 (\", share_variance[1], \"%)\", sep=\"\")) +\n  ylab(paste(\"PC2 (\", share_variance[2], \"%)\", sep=\"\")) +\n  aes(label=.rownames) +\n  geom_text_repel(verbose = FALSE) +\n  coord_fixed()\n\n\n\n\n\n\n\nDecathlon correlation circle\n\nCodedecathlon_pca$rotation %&gt;%\n  as_tibble(rownames=\"Name\") %&gt;%\n  ggplot() +\n  aes(x=PC1, y=PC2, label=Name) +\n  geom_segment(xend=0, yend=0, arrow = grid::arrow(ends = \"first\")) +\n  geom_text_repel() +\n  coord_fixed() +\n  xlim(-.7, .7) +\n  ylim(-.7, .7) +\n  stat_function(fun = ~  sqrt(.7^2 - .^2), alpha=.5, color=\"grey\") +\n  stat_function(fun = ~ - sqrt(.7^2 - .^2), alpha=.5, color=\"grey\") +\n  geom_segment(x = 0, y = -1, xend = 0, yend = 1, linetype = \"dashed\", color = \"grey\") +\n  geom_segment(x = -1, y = 0, xend = 1, yend = 0, linetype = \"dashed\", color = \"grey\") +\n  ggtitle(\"Decathlon correlation circle\")\n\nWarning: The following aesthetics were dropped during statistical transformation: label.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\nThe following aesthetics were dropped during statistical transformation: label.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\nUsing broom::augment\n\n\nCodepca_fit &lt;- prcomp(iris[,-5])\n\npca_fit %&gt;%\n  broom::augment(iris) %&gt;% #&lt;&lt;  add original dataset back in\n  ggplot(aes(x= .fittedPC1, y=.fittedPC2, color = Species)) +\n  geom_point(size = 1.5) +\n  ggtitle(\"Iris PCA with broom::augment\") -&gt; p\n\np\n\n\n\n\n\n\n\nScreeplot\n\nCodezpca &lt;- list_pca_iris[[4]]\n\ntibble(x=1:4, y=zpca$sdev^2) |&gt;\n  ggplot() + \n  aes(x=x, y=y) +\n  geom_col() +\n  xlab(\"Principal components\") +\n  ylab(\"Variance\") +\n  ggtitle(\"Screeplot of standardized/centered Iris dataset\") \n\n\n\n\n\n\n\nVariations on screeplot\n\nCodezetitle &lt;-  \"PCA on Iris: Projected variance on the first PCs\"\n\np &lt;-list_pca_iris[[4]] %&gt;%\n  broom::tidy() %&gt;%\n  ggplot() +\n  aes(x= as_factor(PC), y= value) +\n  labs(xlab=\"PC\", ylab=\"Coord\") +\n  ggtitle(zetitle)\n\n\n\n\n\n\n\n\nWe center and standardize columns, and then perform a change of basis\n\n\n\n\nCodep + geom_boxplot()  ; p + geom_violin()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo packages\nSeveral packages handle the field of factorial analysis\nADE4\n\n2002-…\nDuality diagrams: dudi.pca\n\nEcology oriented\nADE4 Homepage\nFactoMineR\n\n2003-…\nFactoshiny\nFactoExtra\nFactoInvestigate\nA nice book\nA MOOC\nFactoMineR homepage\nhttp://factominer.free.fr\nFactoMineR on PCA\nhttp://factominer.free.fr/factomethods/principal-components-analysis.html\nFactoMineR : scatterplot and correlation circle\n\nCodeiris %&gt;%\n  dplyr::select(where(is.numeric)) %&gt;%\n  FactoMineR::PCA() %&gt;%\n  plot(graph.type=\"ggplot\",\n       title=\"Iris data with FactoMineR\")\n\n\nRevisiting Decathlon data\nAd hoc transformation of running scores: average speed matters\n\nCodedecathlonTime &lt;- mutate(decathlon,\n                        `100m` = 100/`100m`,\n                        `400m` = 400/`400m`,\n                        `110m.hurdle` = 110/`110m.hurdle`,\n                        `1500m` = 1500/`1500m`)\n\n# rownames(decathlonTime)  &lt;- rownames(decathlon)\n\nPCAdecathlontime &lt;- PCA(decathlonTime[,1:10],  graph = FALSE)\n\n\n\nCodefactoextra::fviz_pca_ind(PCAdecathlontime, repel = TRUE)\n\n\n\n\n\n\nCodefactoextra::fviz_pca_var(PCAdecathlontime, repel = TRUE)\n\n\n\n\n\n\n\nWhen inspecting decathlon some columns are positively correlated with column Points, others are not. The negatively correlated columns contain running races times. In order to handle all scores in an homogenous way, running races scores are converted into average speeds. The larger, the better.\nThe correlation circle becomes more transparent: columns can be clustered into three subsets: - running races and long jump, p - puts, javeline and high jump - 1500m and pole vault which are poorly correlated with first two principal components\nWe can do it with tidyverse\n\nCodedecathlonTime[, 1:10] %&gt;%\n  prcomp(scale.=TRUE) -&gt; pc_decathlonTime\n\nshare_variance &lt;-  round(broom::tidy(pc_decathlonTime, matrix=\"pcs\")[[\"percent\"]], 2)\n\npc_decathlonTime  %&gt;%\n  broom::augment(data=decathlonTime) %&gt;%\n  ggplot() +\n  aes(x=.fittedPC1, y=.fittedPC2) +\n  aes(color=Points, label=.rownames) +\n  geom_point() +\n  geom_text_repel() +\n  xlab(paste(\"Dim1 (\", share_variance[1]  ,\"%)\", sep=\"\")) +\n  ylab(paste(\"Dim2 (\", share_variance[2]  ,\"%)\", sep=\"\"))\n\n\n\n\n\n\nCoderadius &lt;-  sqrt(broom::tidy(pc_decathlonTime, matrix=\"pcs\")[[\"cumulative\"]][2])\n\npc_decathlonTime[[\"rotation\"]] %&gt;%\n  as_tibble(rownames=\"Name\") %&gt;%\n  ggplot() +\n  aes(x=PC1, y=PC2, label=Name) +\n  geom_segment(xend=0, yend=0, arrow = grid::arrow(ends = \"first\") ) +\n  geom_text_repel() +\n  coord_fixed() +\n  xlim(-radius, radius) +\n  ylim(-radius, radius) +\n  stat_function(fun = ~  sqrt(radius^2 - .^2), alpha=.5, color=\"grey\") +\n  stat_function(fun = ~ - sqrt(radius^2 - .^2), alpha=.5, color=\"grey\") +\n  geom_segment(x = 0, y = -1, xend = 0, yend = 1, linetype = \"dashed\", color = \"grey\") +\n  geom_segment(x = -1, y = 0, xend = 1, yend = 0, linetype = \"dashed\", color = \"grey\") +\n  ggtitle(\"Decathlon correlation circle (bis)\") +\n  xlab(paste(\"Dim1 (\", share_variance[1]  ,\"%)\", sep=\"\")) +\n  ylab(paste(\"Dim2 (\", share_variance[2]  ,\"%)\", sep=\"\"))\n\nWarning: The following aesthetics were dropped during statistical transformation: label.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\nThe following aesthetics were dropped during statistical transformation: label.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\nPCA and the tidy approach\nThe output of prcomp (or of any contender) is complex object represented by a list with five named elements: x, sdev, rotation, center scale\nIn , the list is assigned a dedicated S3 class: prcomp\nThe prcomp class is not ggplot2 friendly\nIn the tidyverse framework, visualization abides to the grammar of graphics framework: plots are built on dataframes (usually a single dataframe)\nThe different plots that serve to understand the output of PCA (screeplot, correlation circle, biplot, …) have to be built on dataframes that themselves have to be built from the output of PCA and possibly also from the original dataframe\nPackage broom () offers off-the-shelf components for building PCA graphical pipelines (and many other things)\n\nbroom is an attempt to bridge the gap from untidy outputs of predictions and estimations to the tidy data we want to work with. It centers around three S3 methods, each of which take common objects produced by  statistical functions (lm, t.test, nls, prcomp, etc) and convert them into a tibble\n\n\nbroom is particularly designed to work with Hadley’s dplyr package (see the broom+dplyr vignette for more)\n\n\n Three (generic) functions\n\ntidy\n\nconstructs a tibble that summarizes the model’s statistical findings. This includes coefficients and p-values for each term in a regression, per-cluster information in clustering applications, or per-test information for multtest functions\n\naugment\n\nadd columns to the original data that was modeled. This includes predictions, residuals, and cluster assignments\n\nglance\n\nconstruct a concise one-row summary of the model. There is no such method for class prcomp\n\n\n tidy for class prcomp\n\n\nCoderequire(broom)\n\n\n\nCodepc &lt;- prcomp(USArrests, scale = TRUE)\n\n# information about samples  (U x D matrix)\n# Default behaviour\n\nbroom::tidy(pc, matrix=\"samples\") %&gt;%\n  head(5) %&gt;%\n  knitr::kable()\n\n\n\nrow\nPC\nvalue\n\n\n\nAlabama\n1\n-0.9756604\n\n\nAlabama\n2\n-1.1220012\n\n\nAlabama\n3\n0.4398037\n\n\nAlabama\n4\n0.1546966\n\n\nAlaska\n1\n-1.9305379\n\n\n\n\n\nThe output of tidy.prcomp is always a tibble\nDepending on argument matrix, the output gathers information on different components of the SVD factorization\ntidy(pc, \"samples\") provide a long format tibble version of \\(U \\times D\\)\nThe tibble has three columns\n\n\nrow: rownames in the dataframe that served as input to prcomp\n\n\nPC: index of principal components\n\nvalue: score of individual indexed by row on principal components PC, \\((U \\times D)[\\text{row}, \\text{PC}]\\)\n\n\nThe x component of pc is a wide format version of the output of tidy(pc, \"samples\")\nDocumentation\n\nCodeas_tibble(pc$x) %&gt;%\n  add_column(row=rownames(pc$x)) %&gt;%\n  pivot_longer(starts_with(\"PC\"),\n               names_to=\"PC\",\n               names_prefix=\"PC\") %&gt;%\n  head(5)\n\n# A tibble: 5 × 3\n  row     PC     value\n  &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;\n1 Alabama 1     -0.976\n2 Alabama 2     -1.12 \n3 Alabama 3      0.440\n4 Alabama 4      0.155\n5 Alaska  1     -1.93 \n\n\n\n tidy for class prcomp\n\n\nCodepc &lt;- prcomp(USArrests, scale = TRUE)\n\n# information about rotation (V matrix)\nbroom::tidy(pc, \"rotation\") %&gt;%\n  head(5)\n\n# A tibble: 5 × 3\n  column     PC  value\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 Murder      1 -0.536\n2 Murder      2 -0.418\n3 Murder      3  0.341\n4 Murder      4  0.649\n5 Assault     1 -0.583\n\n\nWith matrix=\"rotation\", the result is again a long format tibble version of the orthogonal matrix \\(V\\) from the SVD factorization\nThis tibble is convenient when plotting the correlation circles associated with different sets of components\n\nCodeas_tibble(pc$rotation) %&gt;%\n  add_column(column=rownames(pc$rotation)) %&gt;%\n  pivot_longer(starts_with(\"PC\"),\n               names_to=\"PC\",\n               names_prefix=\"PC\") %&gt;%\n  head()\n\n# A tibble: 6 × 3\n  column  PC     value\n  &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;\n1 Murder  1     -0.536\n2 Murder  2     -0.418\n3 Murder  3      0.341\n4 Murder  4      0.649\n5 Assault 1     -0.583\n6 Assault 2     -0.188\n\n\n\n tidy for class prcomp\n\n\nCode# information about singular values\nbroom::tidy(pc, \"pcs\") %&gt;%\n  knitr::kable()\n\n\n\nPC\nstd.dev\npercent\ncumulative\n\n\n\n1\n1.5748783\n0.62006\n0.62006\n\n\n2\n0.9948694\n0.24744\n0.86750\n\n\n3\n0.5971291\n0.08914\n0.95664\n\n\n4\n0.4164494\n0.04336\n1.00000\n\n\n\n\nCodebroom::tidy(pc, \"pcs\") %&gt;%\n  ggplot() +\n  aes(x=as.integer(PC), y=percent) +\n  geom_col() +\n  xlab(\"PC\") +ylab(\"Percentage of inertia\") +\n  ggtitle(\"USArrests: screeplot\")\n\n\n\n\n\n\n\nWith matrix=\"pcs\" or matrix=\"eigen\", we obtain a tibble that is convenient for generating a screeplot\nIt contains the information returned by summary(pc)\nIndeed, column percent is obtained by squaring column std.dev and normalizing the column\nSee sloop::s3_get_method(tidy.prcomp)\nor\n\nCodeas_tibble(pc$sdev) %&gt;%\n  rename(std.dev=value) %&gt;%\n  rownames_to_column(\"PC\") %&gt;%\n  mutate(PC=as.integer(PC),\n         percent=(std.dev^2/sum(std.dev^2)),\n         cumulative=cumsum(percent))\n\n# A tibble: 4 × 4\n     PC std.dev percent cumulative\n  &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1     1   1.57   0.620       0.620\n2     2   0.995  0.247       0.868\n3     3   0.597  0.0891      0.957\n4     4   0.416  0.0434      1    \n\nCode# or\nt(summary(pc)$importance) %&gt;%\n  data.frame() %&gt;%\n  rownames_to_column()\n\n  rowname Standard.deviation Proportion.of.Variance Cumulative.Proportion\n1     PC1          1.5748783                0.62006               0.62006\n2     PC2          0.9948694                0.24744               0.86750\n3     PC3          0.5971291                0.08914               0.95664\n4     PC4          0.4164494                0.04336               1.00000\n\n\nPackage \n\nVignette\nbroom’s ambitions go far beyond tidying the output of prcomp(...)\nThe vignette describes the goals of the package and the way they fit in the tidverse\n\n broom::tidy(): a generic S3 function\nDispatching to ad hoc methods\nbroom::tidy() is an S3 generic function.\nWhen called, it invokes a dispatcher that calls a method tailored to the class of the object passed as argument\n&gt; broom::tidy\nfunction (x, ...)\n{\n    UseMethod(\"tidy\")\n}\n\nWhen a function calling UseMethod(\"fun\") is applied to an object with class attribute c(\"first\", \"second\"), the system searches for a function called fun.first and, if it finds it, applies it to the object…\n\nThe hard work is performed by the tidy.prcomp() registered for class prcomp\n\nCodeclass(pc)\n\n[1] \"prcomp\"\n\nCode# sloop::s3_get_method(tidy.prcomp)\n\n\nSee S3 section in Advanced R Programmming\n\n broom::augment()\n\nAn excerpt of the body of augment.prcomp\nfunction (x, data = NULL, newdata, ...)\n{\n    ret &lt;- if (!missing(newdata)) {\n        # ...\n    }\n    else {\n        pred &lt;- as.data.frame(predict(x))\n        names(pred) &lt;- paste0(\".fitted\", names(pred))\n        if (!missing(data) && !is.null(data)) {\n            cbind(.rownames = rownames(as.data.frame(data)),\n                data, pred)\n        }\n        else {\n          # ...\n        }\n    as_tibble(ret)\n}\nFunction augment is also an S3 generic function.\nThe method relies on another generic function predict\nFor prcomp predict returns the x component of the list\nThe output combines the columns of the original dataframe and (data) with their names and the predictions\n\n Mixing PCA and geographical information\n\nCoderequire(\"maps\")\n\nLoading required package: maps\n\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nCodepc %&gt;%\n  broom::tidy(matrix = \"samples\") %&gt;%\n  mutate(region = tolower(row)) %&gt;%\n  inner_join(map_data(\"state\"), by = \"region\") %&gt;%\n  ggplot() +\n  aes(long, lat) +\n  aes(group = group, fill = value) +\n  geom_polygon() +\n  facet_wrap(~PC) +\n  scale_fill_viridis_c() +\n  ggtitle(\"Principal components of arrest data\")\n\nWarning in inner_join(., map_data(\"state\"), by = \"region\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\n\n\n augment for prcomp\n\n\nCodeau &lt;- augment(pc, data = USArrests)\n\nau %&gt;% head(3)\n\n# A tibble: 3 × 9\n  .rownames Murder Assault UrbanPop  Rape .fittedPC1 .fittedPC2 .fittedPC3\n  &lt;chr&gt;      &lt;dbl&gt;   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Alabama     13.2     236       58  21.2     -0.976     -1.12      0.440 \n2 Alaska      10       263       48  44.5     -1.93      -1.06     -2.02  \n3 Arizona      8.1     294       80  31       -1.75       0.738    -0.0542\n# ℹ 1 more variable: .fittedPC4 &lt;dbl&gt;\n\nCodeggplot(au) +\n  aes(.fittedPC1, .fittedPC2) +\n  geom_point() +\n  geom_text(aes(label = .rownames), vjust = 1, hjust = 1)"
  },
  {
    "objectID": "labs/lab-dplyr-SQL.html",
    "href": "labs/lab-dplyr-SQL.html",
    "title": "Table manipulations I: dplyr and SQL",
    "section": "",
    "text": "Codestopifnot(\n  require(tidyverse), \n  require(glue),\n  require(cowplot),\n  require(patchwork),\n  require(nycflights13),\n  require(DBI),\n  require(RSQLite),\n  require(RPostgreSQL),\n  require(dtplyr),\n  require(dbplyr)\n)\n\nold_theme &lt;- theme_set(theme_minimal())\nFrom the Documentation\ndplyr provides an elegant implementation of table calculus, as embodied by SQL.\nWe will play with the nycflights13 dataset"
  },
  {
    "objectID": "labs/lab-dplyr-SQL.html#in-memory",
    "href": "labs/lab-dplyr-SQL.html#in-memory",
    "title": "Table manipulations I: dplyr and SQL",
    "section": "In memory",
    "text": "In memory\n\nCodeflights &lt;- nycflights13::flights\nweather &lt;- nycflights13::weather\nairports &lt;- nycflights13::airports\nairlines &lt;- nycflights13::airlines\nplanes &lt;- nycflights13::planes\n\n\n\nCodecon &lt;- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\nflights_lite &lt;- copy_to(con, nycflights13::flights)\nairports_lite &lt;- copy_to(con, nycflights13::airports)\nplanes_lite &lt;- copy_to(con, nycflights13::planes)\nweather_lite &lt;- copy_to(con, nycflights13::weather)\nairlines_lite &lt;- copy_to(con, nycflights13::airlines)"
  },
  {
    "objectID": "labs/lab-dplyr-SQL.html#projection-pi-select...",
    "href": "labs/lab-dplyr-SQL.html#projection-pi-select...",
    "title": "Table manipulations I: dplyr and SQL",
    "section": "Projection, \\(\\pi\\), select(...)\n",
    "text": "Projection, \\(\\pi\\), select(...)\n\nProjection of a table \\(R\\) on columns \\(A_{i_1},\\ldots, A_{i_k}\\) results in a table with schema \\(A_{i_1},\\ldots, A_{i_k}\\) and one row for each row of \\(R\\). Projection is denoted by \\(\\pi(R, A_{i_1},\\ldots, A_{i_k})\\).\nIn SQL this reads as\nSELECT Ai1, , Aik\nFROM R \n\nCodeR_lite &lt;- copy_to(con, R)\nS_lite &lt;- copy_to(con, S)\n\n\nIn the sequel, we illustrate operations on the next two toy tables\n\n\n\n\n\n\nTable R\n\n\n\n\nA\nB\nC\n\n\n\n2024-02-06\n10\nu\n\n\n2024-02-08\n1\no\n\n\n2024-02-03\n9\ni\n\n\n2024-02-10\n6\nq\n\n\n2024-02-08\n4\np\n\n\n2024-02-02\n7\nk\n\n\n2024-02-05\n4\nd\n\n\n2024-02-03\n9\nh\n\n\n2024-02-01\n5\nh\n\n\n2024-02-09\n3\nd\n\n\n\n\n\n\n\nTable S\n\n\n\n\nA\nD\nF\n\n\n\n2024-02-03\n21\nb\n\n\n2024-02-03\n21\ne\n\n\n2024-02-06\n28\nm\n\n\n2024-02-03\n27\nq\n\n\n2024-02-03\n26\nd\n\n\n2024-02-08\n27\nr\n\n\n2024-02-06\n30\nx\n\n\n2024-02-05\n28\nu\n\n\n\n\n\n\n\n\nIn Relational Algebra, tables are sets rather than multisets, there are no duplicates. In SQL we are handling multisets of rows, duplicates need to be removed explicitly\nSELECT DISTINCT Ai1, ..., Aik\nFROM R \ndplyr has one verb select(...) for \\(\\pi\\) or SELECT, and verb distinct() for SELECT DISTINCT ....\nIf we have no intention to remove duplicates:\nselect(R, Ai1, ..., Aik)\n# or\nR |&gt; \n  select(Ai1, ..., Aik)\nIf we want to remove deplicates\ndistinct(R, Ai1, ..., Aik)\n# or\nR |&gt; \n  distinct(Ai1, ..., Aik)\n\n\n\n\n\n\n\\(\\pi(R, B, C)\\) (SELECT B, C FROM R) leads to\n\n\n\n\nB\nC\n\n\n\n10\nu\n\n\n1\no\n\n\n9\ni\n\n\n6\nq\n\n\n4\np\n\n\n7\nk\n\n\n4\nd\n\n\n9\nh\n\n\n5\nh\n\n\n3\nd\n\n\n\n\n\n\n\n\\(\\pi(R, B)\\) and SELECT DISTINCT B FROM R lead to\n\n\n\n\nB\n\n\n\n10\n\n\n1\n\n\n9\n\n\n6\n\n\n4\n\n\n7\n\n\n5\n\n\n3\n\n\n\n\n\n\n\n\n\nFor each departure airport (denoted by origin'),  each day of the year, list the codes (denoted bycarrier`) of the airlines that have one or more planes taking off from that airport on that day."
  },
  {
    "objectID": "labs/lab-dplyr-SQL.html#selection-sigma-filter...",
    "href": "labs/lab-dplyr-SQL.html#selection-sigma-filter...",
    "title": "Table manipulations I: dplyr and SQL",
    "section": "Selection, \\(\\sigma\\), filter(...)\n",
    "text": "Selection, \\(\\sigma\\), filter(...)\n\nSelection of a table \\(R\\) according to condition \\(\\texttt{expr}\\) is an expression that can be evaluated on each row of \\(R\\) results in a table with the same schema as \\(R\\) and all rows of \\(R\\) where \\(\\texttt{expr}\\) evaluates to TRUE. Selection is denoted by \\(\\sigma(R, \\texttt{expr})\\).\nIn SQL this reads as\nSELECT R.*\nFROM R \nWHERE expr\ndplyr has one verb filter(...) for \\(\\sigma\\).\n\n\n\n\n\n\n\\(\\sigma(R, A &lt; \\text{2024-02-06} \\wedge \\text{2024-02-02} \\leq A)\\) (SELECT * FROM R WHERE A &lt; CAST('2024-02-06' AS DATE) AND A &gt;= CAST('2024-02-02' AS DATE)) leads to\n\n\n\n\nA\nB\nC\n\n\n\n2024-02-03\n9\ni\n\n\n2024-02-02\n7\nk\n\n\n2024-02-05\n4\nd\n\n\n2024-02-03\n9\nh\n\n\n\n\n\n\n\nSELECT DISTINCT B FROM R leads to\n\n\n\n\nB\n\n\n\n10\n\n\n1\n\n\n9\n\n\n6\n\n\n4\n\n\n7\n\n\n5\n\n\n3\n\n\n\n\n\n\n\n\n\nList all the planes built by a manufacturer named like AIRBUS between 2005 and 2010"
  },
  {
    "objectID": "labs/lab-dplyr-SQL.html#joins-bowtie-xxx_join...",
    "href": "labs/lab-dplyr-SQL.html#joins-bowtie-xxx_join...",
    "title": "Table manipulations I: dplyr and SQL",
    "section": "Joins, \\(\\bowtie\\), xxx_join(...)\n",
    "text": "Joins, \\(\\bowtie\\), xxx_join(...)\n\nIn relational algebra, a \\(\\theta\\)-join boils down to a selection according to expression \\(\\theta\\) over a cross product (possibly after renaming some columns)\n\\[\\bowtie(R, S, \\theta) \\approx \\sigma(R \\times S, \\theta)\\] dplyr does not (yet?) offer such a general join (which anyway can be very expensive on a cutting edge RDBMS) several variants of equijoin.\nChatGPT asserts:\n\nAn equijoin is a type of join operation in relational databases where the join condition involves an equality comparison between two attributes from different tables. In other words, it’s a join operation that combines rows from two tables based on matching values in specified columns. These specified columns are usually called the “join columns” or “join keys.”\n\nJoins in dplyr documentation\n\ninner_join()\nleft_join()\nright_join()\nfull_join()\n\nbut also\n\nsemi_join()\nanti_join()\n\nthe matching columns are stated using optional argument by=...\nIf argument by is omitted, NATURAL JOIN is assumed.\n\\(\\bowtie(R, S)\\) (SELECT * FROM R NATURAL JOIN S) leads to\n\n\nJoining with `by = join_by(A)`\n\n\nWarning in inner_join(R, S): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 6 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\n\\(\\bowtie^{\\text{Left outer}}(R, S)\\) and SELECT * FROM R LEFT JOIN S ON (R.A=S.A) lead to\n\n\nJoining with `by = join_by(A)`\n\n\nWarning in left_join(R, S): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 6 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\nList weather conditions at departure for all flights operated by airline named Delta ...."
  },
  {
    "objectID": "labs/lab-lifeexp-1948-2016.html#loading",
    "href": "labs/lab-lifeexp-1948-2016.html#loading",
    "title": "Life Tables: 1948-2006",
    "section": "Loading",
    "text": "Loading\n\nCodedatafile &lt;- 'tamed_life_table.Rds'\nfpath &lt;- str_c(\"../DATA/\", datafile) # here::here('DATA', datafile)   # check getwd() if problem \n\nif (! file.exists(fpath)) {\n  download.file(\"https://stephane-v-boucheron.fr/data/tamed_life_table.Rds\", \n                fpath,\n                mode=\"wb\")\n}\n\nlife_table &lt;- readr::read_rds(fpath)"
  },
  {
    "objectID": "labs/lab-lifeexp-1948-2016.html#period-tables-versus-cohort-tables",
    "href": "labs/lab-lifeexp-1948-2016.html#period-tables-versus-cohort-tables",
    "title": "Life Tables: 1948-2006",
    "section": "Period tables versus cohort tables",
    "text": "Period tables versus cohort tables\nTwo kinds of Life Tables can be distinguished: Period tables (Table du moment) which contain for each period (here a period is a calendar year), the mortality risks at different age ranges (here, we have one year ranges) for that very period; and Tables de génération which contain for a given birthyear, the mortality risks at which an individual born during that year has been exposed.\nThe life tables investigated in this lab are Table du moment. According to the document by Vallin and Meslé, building the life tables required ,decisions and doctoring."
  },
  {
    "objectID": "labs/lab-lifeexp-1948-2016.html#lexis-diagrams",
    "href": "labs/lab-lifeexp-1948-2016.html#lexis-diagrams",
    "title": "Life Tables: 1948-2006",
    "section": "Lexis diagrams",
    "text": "Lexis diagrams\nLexis diagrams provide a graphical device that summarizes the construction of mortality quotients (and other rates in demography).\n\nCodebirth_dates &lt;- as_date(\"1999-01-01\") + duration(sample(2*365, size=20, replace=T),units=\"day\")\ndeath_dates &lt;- as_date(\"2009-07-01\") + duration(sample(3*365, size=20, replace=T),units=\"day\")\n\nb_period &lt;- as_date(\"2010-01-01\")\nb_frame &lt;-  as_date(b_period - duration(1, units = \"year\"))\nb_age &lt;- 10L\n\ntb_ld &lt;- tibble(birth=birth_dates, death=death_dates)\n\ntb_ld |&gt; \n  ggplot() +\n  geom_segment(aes(x=b_frame,\n                   xend=death,\n                   y=interval(birth, b_frame)/years(1),\n                   yend=interval(birth, death)/years(1))\n               ) +\n  annotate(geom=\"rect\",\n           xmin=b_period,\n           xmax=as_date(b_period + duration(1, units = \"year\")),\n           ymin=b_age,\n           ymax=b_age + 1L,\n           fill=\"grey\",\n          alpha=.5) +\n  ylab(\"Age\") +\n  xlab(\"Time\") +\n  coord_cartesian(xlim=c(as_date(b_period - duration(6, units = \"months\")), \n                         as_date(b_period + duration(18, units = \"months\"))),\n                  ylim=c(b_age - .5, b_age+1.5)) +\n  labs(\n    title=\"A Lexis diagram\",\n    subtitle = \"for mortality quotient at Age 10 during Year 2010-11\"\n  )\n\n\n\n\n\n\n\nEach line represents the life line of an individual born during years 1999 and 2000 and deceased beetween mid 2009 and mid 2012. In order to compute the mortality quotient at age 10 for year 2010, we have to compute the relevant number of occurrences, that is the number of segments ending in the grey rectangle, and the sum of exposure times, which is proportional to the sum of the lengths of the segments crossing the grey rectangle.\nHave a look at Lexis diagram or at Preston et al.\nDefinitions can be obtained from www.lifeexpectancy.org. We translate it into mathematical (rather than demographic) language.\nThe mortality quotients define a probability distribution over \\(\\mathbb{N}\\). This probability distribution is a construction that reflects the health situation in a population at a given time. This probability distribution does not describe the sequence of sanitary situations experienced by a cohort (people born during a specific year).\n\nOne works with a period, or current, life table (table du moment). This summarizes the mortality experience of persons across all ages in a short period, typically one year or three years. More precisely, the death probabilities \\(q_x\\) for every age \\(x\\) are computed for that short period, often using census information gathered at regular intervals. These \\(q_x\\)’s are then applied to a hypothetical cohort of \\(100 000\\) people over their life span to produce a life table.\n\n\nCodesmall_tb &lt;- life_table |&gt; \n  filter(Country=='France', Year== 2010, Gender=='Female', Age &lt; 10 | between(Age, 80, 89)) |&gt; \n  select(Age, qx, mx, lx, dx, Lx, Tx, ex)\n\nsmall_tb\n\n# A tibble: 20 × 8\n     Age      qx      mx     lx    dx    Lx      Tx    ex\n   &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;\n 1     0 0.00324 0.00325 100000   324 99722 8465207 84.6 \n 2     1 0.00032 0.00032  99676    32 99660 8365484 83.9 \n 3     2 0.00015 0.00015  99645    15 99637 8265824 83.0 \n 4     3 0.00011 0.00011  99630    11 99624 8166187 82.0 \n 5     4 0.00008 0.00008  99619     8 99615 8066563 81.0 \n 6     5 0.00005 0.00005  99611     5 99608 7966948 80.0 \n 7     6 0.00008 0.00008  99606     8 99602 7867339 79.0 \n 8     7 0.00008 0.00008  99598     8 99594 7767737 78.0 \n 9     8 0.00008 0.00008  99590     8 99586 7668143 77   \n10     9 0.00007 0.00007  99582     7 99578 7568557 76   \n11    80 0.0298  0.0302   75619  2252 74493  802295 10.6 \n12    81 0.0346  0.0352   73367  2535 72099  727802  9.92\n13    82 0.0398  0.0406   70832  2818 69423  655702  9.26\n14    83 0.0464  0.0475   68014  3158 66435  586280  8.62\n15    84 0.0539  0.0554   64856  3493 63109  519845  8.02\n16    85 0.0610  0.0630   61362  3745 59490  456736  7.44\n17    86 0.0699  0.0725   57617  4029 55603  397246  6.89\n18    87 0.0793  0.0826   53588  4249 51464  341643  6.38\n19    88 0.0922  0.0966   49339  4547 47066  290180  5.88\n20    89 0.105   0.111    44792  4706 42440  243114  5.43\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nThe table above is not as readable as it should. Use package gt to get a more tunable outpout.\nReorder and filter the columns so that Age comes first (they identify rows), then qx, mx up to ex. You can use select or relocate, or both to do this. Note that Gender, Country, Year are constant in this tibble and need to be reported in the table header, but nowhere else.\nColumns qx and mx (for mortality quotient and central death rate) should be dsplayed in scientific notation so that the fact that their range extends over several orders of magnitude shows up.\nColumns lx, dx, Lx, Tx contain integer values.\nColumn ex (residual life expectancy) is a (fictional) decimal number of years"
  },
  {
    "objectID": "labs/lab-lifeexp-1948-2016.html#understanding-the-columns-of-the-life-table",
    "href": "labs/lab-lifeexp-1948-2016.html#understanding-the-columns-of-the-life-table",
    "title": "Life Tables: 1948-2006",
    "section": "Understanding the columns of the life table",
    "text": "Understanding the columns of the life table\nIn the sequel, we denote by \\(F_{t}\\) the cumulative distribution function for year \\(t\\). We agree on \\(\\overline{F}_t = 1 - F_t\\) and \\(F_t(-1)=0\\). Henceforth, \\(\\overline{F}\\) is called the survival function.\n\nqx\n\n(age-specific) risk of death at age \\(x\\), or mortality quotient at given age \\(x\\) for given year \\(t\\).\n\n\n\n\n\n\n\n\nAbout the definition of q_{t,x}\n\n\n\nDefining and computing q_{t,x} does not boil down to knowing the number of people at age \\(x\\) at the beginning of ear \\(t\\) and knowing how many of them died during year \\(t\\). If we want to be rigorous, we need to know all life lines in the Lexis diagram, or equivalently, how many people at Age \\(x\\) were alive on each day of Year \\(t\\).\n\n\n\n\n\n\n\n\nMortality quotients define a probability distribution\n\n\n\nFor a given year \\(t\\), the sequence of mortality quotients define a survival function \\(\\overline{F}_t\\) using the following recursion:\n\\[q_{t,x} = \\frac{\\overline{F}_t(x) - \\overline{F}_t(x+1)}{\\overline{F}_t(x)}\\] with boundary condition \\(\\overline{F}_t(-1) =1\\).\nThis recursion can also be read as:\n\\[\\overline{F}_{t}(x+1) = \\overline{F}_{t}(x) \\times (1-q_{t,x+1})\\, .\\]\nThis artificial probability distribution is used to define and compute life expectancies.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(q_{t,x}\\) is the hazard rate of \\(\\overline{F}_t\\) at age \\(x\\).\n\n\n\nmx\n\ncentral death rate at age \\(x\\) during year \\(t\\). This is connected with \\(q_{t,x}\\) by \\[m_{t,x} = -\\log(1- q_{t,x}) \\,,\\]\n\n\nor equivalently \\[q_{t,x} = 1 - \\exp(-m_{t,x})\\]\n\n\n\n\n\n\nAbout central death rate\n\n\n\nIf we want to define a continuous probability distribution \\(G\\) over \\([0,\\infty)\\) so that \\(G\\) and \\(F\\) coincide over integers and \\(G\\) has piecewise constant hazard rate, we can pick \\(m_{t,x}\\) as the piecewise constant hazard rate.\n\n\n\nlx\n\nthe so-called survival function: the scaled proportion of persons alive at age \\(x\\). These values are computed recursively from the \\(q_{t,x}\\) values using the formula\n\n\n\\[l_t(x+1) = l_t(x) \\times (1-q_{t,x}) \\, ,\\] with \\(l_{t,0}\\), the radix of the table, (arbitrarily) set to \\(100000\\). In the table lx is rounded to the next integer\nFunction \\(l_{t,\\cdot}\\) and \\(\\overline{F}_t\\) are connected by\n\\[l_{t,x + 1} = l_{t,0} \\times \\overline{F}_t(x)\\,.\\]\n\ndx\n\n\\(d_{t,x} = q_{t,x} \\times l_{t,x}\\). The fictitious number of deaths occurring at age \\(x\\) during year \\(t\\). Again this is a rounded quantity.\n\nTx\n\nTotal number of person-years lived by the cohort from age \\(x\\) to \\(x+1\\). This is the sum of the years lived by the \\(l_{t, x+1}\\) persons who survive the interval, and the \\(d_{t,x}\\) persons who die during the interval. The former contribute exactly \\(1\\) year each, while the latter contribute, on average, approximately half a year, so that \\(L_{t,x} = l_{t,x+1} + 0.5 \\times d_{t,x}\\). This approximation assumes that deaths occur, on average, half way in the age interval x to x+1. Such is satisfactory except at age 0 and the oldest age, where other approximations are often used.\n\n\nCompare with the denominator in the definition of qx and its description using the Lexis diagram.\nWe will stick to a simplified vision \\(L_{t,x}= l_{t,x+1}\\)\n\n\nex:\n\nResidual Life Expectancy at age \\(x\\) and year \\(t\\)\n\n\nThis is the expectation of \\(X -x\\) for a random variable \\(X\\) distributed according to \\(\\overline{F}_t\\) conditionnally on the event \\(\\{ X \\geq x \\}\\). That is \\(e_{t,x}\\) is the expectation of the probability distribution defined by \\(\\overline{F}_t(\\cdot + x-1)/\\overline{F}_t(x-1)\\).\n\n\n\n\n\n\nQuestion\n\n\n\nCheck dependencies between columns"
  },
  {
    "objectID": "labs/lab-gss.html#download-the-data",
    "href": "labs/lab-gss.html#download-the-data",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Download the data",
    "text": "Download the data\n\nCodedownload_data &lt;-  function(fname,\n                           baseurl = 'https://stephane-v-boucheron.fr/data',\n                           datapath = \"../DATA\") {\n  fpath &lt;- paste(datapath, fname, sep = \"/\")\n  \n  if (!file.exists(fpath)) {\n    url &lt;- paste(baseurl, fname, sep = \"/\")\n    \n    rep &lt;- httr::GET(url)\n    stopifnot(rep$status_code == 200)\n    \n    con &lt;- file(fpath, open = \"wb\")\n    writeBin(rep$content, con)\n    close(con)\n    \n    print(glue('File \"{fname}\" downloaded!'))\n  } else {\n    print(glue('File \"{fname}\" already on hard drive!'))\n  }\n}\n\n\ndownload_data(fname=\"sub-data.txt\")\ndownload_data(fname=\"sub-cdbk.txt\")\n\n\n\n\n\n\nBase R (package utils) offers a function download.file(). There is\nfname &lt;- 'sub-data.txt'\nbaseurl &lt;- 'https://stephane-v-boucheron.fr/data'\ndownload.file(url=paste(baseurl, fname, sep=\"/\"),\n              destfile=paste('./DATA', fname, sep=\"/\"))\nThere is no need to (always) reinvent the wheel!"
  },
  {
    "objectID": "labs/lab-gss.html#load-the-data-in-your-session",
    "href": "labs/lab-gss.html#load-the-data-in-your-session",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Load the data in your session",
    "text": "Load the data in your session\nFile inspection shows that the data file sub-data.txt is indeed a csv file\n09:01 $ file DATA/sub-data.txt\nDATA/sub-data.txt: CSV text\nWe do not know the peculiarities of this file formatting. We load it as if fields were separated by coma (,, this is an American file). and prevent any type inference by asserting that all columns should be treated as character (c).\nAnswer the following questions:\n\nWhat are the observations/individuals/sample points?\nWhat do the columns stand for?\nIs the dataset tidy/messy?\n\nInspect the schema of dataframe (there are 540 columns!)"
  },
  {
    "objectID": "labs/lab-gss.html#howm-many-missing-values-per-column",
    "href": "labs/lab-gss.html#howm-many-missing-values-per-column",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Howm many missing values per column ?",
    "text": "Howm many missing values per column ?"
  },
  {
    "objectID": "labs/lab-gss.html#drop-null-columns",
    "href": "labs/lab-gss.html#drop-null-columns",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Drop NULL columns",
    "text": "Drop NULL columns"
  },
  {
    "objectID": "labs/lab-gss.html#what-are-the-unique-values-in-columns-whose-name-contains-income",
    "href": "labs/lab-gss.html#what-are-the-unique-values-in-columns-whose-name-contains-income",
    "title": "General Social Survey, Univariate Analysis",
    "section": "What are the unique values in columns whose name contains income ?",
    "text": "What are the unique values in columns whose name contains income ?"
  },
  {
    "objectID": "labs/lab-gss.html#make-income-and-rincome-a-factor",
    "href": "labs/lab-gss.html#make-income-and-rincome-a-factor",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Make income and rincome a factor",
    "text": "Make income and rincome a factor"
  },
  {
    "objectID": "labs/lab-gss.html#summarize-and-visualize-the-distributions-of-income-and-rincome",
    "href": "labs/lab-gss.html#summarize-and-visualize-the-distributions-of-income-and-rincome",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Summarize and Visualize the distributions of income and rincome\n",
    "text": "Summarize and Visualize the distributions of income and rincome"
  },
  {
    "objectID": "labs/lab-gss.html#the-factors-need-reordering",
    "href": "labs/lab-gss.html#the-factors-need-reordering",
    "title": "General Social Survey, Univariate Analysis",
    "section": "The factors need reordering",
    "text": "The factors need reordering"
  },
  {
    "objectID": "labs/lab-gss.html#recode-factors",
    "href": "labs/lab-gss.html#recode-factors",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Recode factors",
    "text": "Recode factors"
  },
  {
    "objectID": "labs/lab-gss.html#make-year-an-integer-column",
    "href": "labs/lab-gss.html#make-year-an-integer-column",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Make year an integer column",
    "text": "Make year an integer column"
  },
  {
    "objectID": "labs/lab-gss.html#plot-rincome-and-income-distributions-with-respect-to-year",
    "href": "labs/lab-gss.html#plot-rincome-and-income-distributions-with-respect-to-year",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Plot rincome and income distributions with respect to year\n",
    "text": "Plot rincome and income distributions with respect to year"
  },
  {
    "objectID": "labs/lab-gss.html#scatterplot-of-conrinc-y-with-respect-to-coninc-facet-by-sex",
    "href": "labs/lab-gss.html#scatterplot-of-conrinc-y-with-respect-to-coninc-facet-by-sex",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Scatterplot of conrinc (y) with respect to coninc, facet by sex\n",
    "text": "Scatterplot of conrinc (y) with respect to coninc, facet by sex"
  },
  {
    "objectID": "labs/lab-gss.html#facet-histogram-for-conrinc-according-to-income",
    "href": "labs/lab-gss.html#facet-histogram-for-conrinc-according-to-income",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Facet histogram for conrinc according to income\n",
    "text": "Facet histogram for conrinc according to income"
  },
  {
    "objectID": "labs/lab-gss.html#retype-age",
    "href": "labs/lab-gss.html#retype-age",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Retype age\n",
    "text": "Retype age\n\nColumn age should be numeric."
  },
  {
    "objectID": "labs/lab-gss.html#compute-the-numerical-summary-of-column-age",
    "href": "labs/lab-gss.html#compute-the-numerical-summary-of-column-age",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Compute the numerical summary of column age\n",
    "text": "Compute the numerical summary of column age"
  },
  {
    "objectID": "labs/lab-gss.html#boxplot-of-age-with-respect-to-sex",
    "href": "labs/lab-gss.html#boxplot-of-age-with-respect-to-sex",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Boxplot of age with respect to sex",
    "text": "Boxplot of age with respect to sex\nThe boxplot delivers a graphical output starting from the robust estimators (the quartiles) of location and scale."
  },
  {
    "objectID": "labs/lab-gss.html#histogram-of-age-distribution-facetted-by-sex",
    "href": "labs/lab-gss.html#histogram-of-age-distribution-facetted-by-sex",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Histogram of age distribution facetted by sex\n",
    "text": "Histogram of age distribution facetted by sex\n\n\nCodedf_redux |&gt; \n  ggplot() +\n  aes(x=age) +\n  geom_histogram(aes(y=after_stat(density)), fill=\"white\", color=\"black\", bins = 20) +\n  facet_wrap(~ sex, ncol = 1) +\n  ggtitle(\"Age histogram per sex\")\n\nWarning: Removed 585 rows containing non-finite outside the scale range\n(`stat_bin()`)."
  },
  {
    "objectID": "labs/lab-gss.html#ecdf-of-age-distribution-by-sex",
    "href": "labs/lab-gss.html#ecdf-of-age-distribution-by-sex",
    "title": "General Social Survey, Univariate Analysis",
    "section": "ECDF of age distribution by sex",
    "text": "ECDF of age distribution by sex\n\nCodedf_redux |&gt; \n  ggplot() +\n  aes(x=age, linetype=sex, color=sex) +\n  stat_ecdf() +\n  ggtitle(\"Age ECDF per sex\")\n\nWarning: Removed 585 rows containing non-finite outside the scale range\n(`stat_ecdf()`)."
  },
  {
    "objectID": "labs/lab-gss.html#is-not-responding-to-the-question-about-age-associated-with-sex",
    "href": "labs/lab-gss.html#is-not-responding-to-the-question-about-age-associated-with-sex",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Is “not responding to the question about age” associated with sex?",
    "text": "Is “not responding to the question about age” associated with sex?\n\nCodedf_redux |&gt; \n  select(age, sex) |&gt; \n  mutate(age=is.na(age)) |&gt; \n  table() |&gt; \n  chisq.test()\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(mutate(select(df_redux, age, sex), age = is.na(age)))\nX-squared = 1.5752, df = 1, p-value = 0.2095"
  },
  {
    "objectID": "labs/lab-gss.html#scatterplot-of-conrinc-with-respect-to-age",
    "href": "labs/lab-gss.html#scatterplot-of-conrinc-with-respect-to-age",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Scatterplot of conrinc with respect to age\n",
    "text": "Scatterplot of conrinc with respect to age\n\nPlay with geom_jitter, transparency (alpha), point size, and logarithmic scale for income.\n\nCodep &lt;- df_redux |&gt; \n  ggplot() +\n  aes(x=age, y=conrinc)  \n\np1 &lt;- p + geom_point(size=.1, alpha=.5)\np2 &lt;- p + geom_jitter(size=.1, alpha=.5)\np3 &lt;- p + geom_point(size=.1, alpha=.5) + scale_y_log10()\np4 &lt;- p + geom_jitter(size=.1, alpha=.5) + scale_y_log10()\n\n((p1 + p2) / (p3 + p4) ) + plot_annotation(\n  title= 'Income versus Age'\n)\n\nWarning: Removed 9065 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 9065 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 9065 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 9065 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nCodep4 + ggtitle('Income versus Age')\n\nWarning: Removed 9065 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "labs/lab-gss.html#boxplot-of-conrinc-with-respect-to-sex",
    "href": "labs/lab-gss.html#boxplot-of-conrinc-with-respect-to-sex",
    "title": "General Social Survey, Univariate Analysis",
    "section": "Boxplot of conrinc with respect to sex\n",
    "text": "Boxplot of conrinc with respect to sex\n\n\nCodedf_redux |&gt; \n  ggplot() +\n  aes(x=sex, y=conrinc) +\n  geom_boxplot(varwidth=T) +\n  scale_y_log10() +\n  ggtitle(\"Income with respect to sex\")\n\nWarning: Removed 8869 rows containing non-finite outside the scale range\n(`stat_boxplot()`)."
  },
  {
    "objectID": "labs/lab-gss-r.html",
    "href": "labs/lab-gss-r.html",
    "title": "GSS R: installation and first exploration",
    "section": "",
    "text": "Codeif (!require(gssr)) {\n  if (!require(remotes)){\n    install.packages(\"remotes\")\n  }\n  remotes::install_github(\"kjhealy/gssr\")\n}\n\n\n\n\n\n\n\n\n\nL3 MIASHS\nUniversité Paris Cité\nAnnée 2024-2025\n\nCourse Homepage\n\nMoodle\n\n\n\n\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\n\n\nInstall and use package gssr\n\n\n\n\n\n\n\nWe work again with General Social Survey (GSS) data.\nWe take advantage of R package gssr\nif (!require(gssr)) {\n  if (!require(remotes)){\n    install.packages(\"remotes\")\n  }\n  remotes::install_github(\"kjhealy/gssr\")\n}\n\n\n\n\n\n\n\n\n\nGet data for year 2018\nThe GSS is carried out every two years. It offers both cross-sectional data and panel data.\nPackage gssr offers a simple way to retrieve yearly data.\n\ndf_2018 &lt;- gssr::gss_get_yr(2018)\n\nFetching: https://gss.norc.org/documents/stata/2018_stata.zip\n\n\nInspect the data\n\nHow many observations?\nHow many variables?\nAre the data tidy/messy?\nNumerical summaries for age and agekdbrn\n\nThe 2018 data provide (among too many other things) columns named age abd agekdbrn. Get numerical summaries about these two columns.\nThanks to gssr, you can get meta-information about the columns\n?aged\n?agekdbrn\n?sex\nHow is sex encoded? Is it worth recoding it?\nHistogram and density plots for age distribution/facet by sex\n\nCompare sample age distribution with population age distribution\nCodeknitr::include_url(\"https://perspective.usherbrooke.ca/bilan/servlet/BMPagePyramide/USA/2018/?\", height=600)\n\n\n\nSherbrooke University offers visual information about the age structure of population of a wide range of countries.\nFollowing demographic usage, the age structure is presented through an age pyramid.\nNote that an age pyramid is a special kind of histogram\n\n\n\n\n\n\n\n\n\nParallel boxplots of age with respect to sex\n\nQQplot comparing sample male and female age distributions\nMake your own qqplot\nScatterplot for age and agekdbrn, facet by sex `\nWorking with gss_sub\n\n\nCodedata(\"gss_sub\")\n\ngss_sub |&gt; \n  glimpse()\n\nRows: 72,390\nColumns: 20\n$ year     &lt;dbl+lbl&gt; 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 197…\n$ id       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ ballot   &lt;dbl+lbl&gt; NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), N…\n$ age      &lt;dbl+lbl&gt; 23, 70, 48, 27, 61, 26, 28, 27, 21, 30, 30, 56, 54, 49, 4…\n$ race     &lt;dbl+lbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, …\n$ sex      &lt;dbl+lbl&gt; 2, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, …\n$ degree   &lt;dbl+lbl&gt; 3, 0, 1, 3, 1, 1, 1, 3, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 3, …\n$ padeg    &lt;dbl+lbl&gt;     0,     0,     0,     3,     0,     3,     3,     3,  …\n$ madeg    &lt;dbl+lbl&gt; NA(i),     0,     0,     1,     0,     4,     1,     1,  …\n$ relig    &lt;dbl+lbl&gt; 3, 2, 1, 5, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ polviews &lt;dbl+lbl&gt; NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), N…\n$ fefam    &lt;dbl+lbl&gt; NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), N…\n$ vpsu     &lt;dbl+lbl&gt; NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), N…\n$ vstrat   &lt;dbl+lbl&gt; NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), N…\n$ oversamp &lt;dbl+lbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ formwt   &lt;dbl+lbl&gt; NA(y), NA(y), NA(y), NA(y), NA(y), NA(y), NA(y), NA(y), N…\n$ wtssall  &lt;dbl+lbl&gt; 0.4446, 0.8893, 0.8893, 0.8893, 0.8893, 0.4446, 0.4446, 0…\n$ wtssps   &lt;dbl+lbl&gt; 0.6631963, 0.9173700, 0.8974125, 1.0663408, 0.9443237, 0.…\n$ sampcode &lt;dbl+lbl&gt; NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), NA(i), N…\n$ sample   &lt;dbl+lbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\n\nEducation through generations\nWhat kind of information do we get through variables degree and padeg?\n?degree\n?padeg\nCompute contingency table for degree and padeg\n\nVisualize contingency table for degree and padeg\n\nRearrange the levels of degree and padeg"
  },
  {
    "objectID": "labs/lab-slr.html#numerical-summaries",
    "href": "labs/lab-slr.html#numerical-summaries",
    "title": "Bivariate analysis: simple linear regression",
    "section": "Numerical summaries",
    "text": "Numerical summaries\nThe numerical summary of a numerical bivariate sample consists of an empirical mean\n\\[\\begin{pmatrix}\\overline{X}_n \\\\ \\overline{Y}_n \\end{pmatrix} = \\frac{1}{n} \\sum_{i=1}^n \\begin{pmatrix} x_i \\\\ y_i \\end{pmatrix}\\]\nand an empirical covariance matrix\n\\[\\begin{pmatrix}\\operatorname{var}_n(X) & \\operatorname{cov}_n(X, Y) \\\\ \\operatorname{cov}_n(X, Y) & \\operatorname{var}_n(Y)\\end{pmatrix}\\]\nwith\n\\[\\operatorname{var}_n(X, Y) = \\frac{1}{n}\\sum_{k=1}^n \\Big(x_i-\\overline{X}_n\\Big)^2\\]\nand\n\\[\\operatorname{cov}_n(X, Y) = \\frac{1}{n}\\sum_{k=1}^n \\Big(x_i-\\overline{X}_n\\Big)\\times \\Big(y_i-\\overline{Y}_n\\Big)\\]"
  },
  {
    "objectID": "labs/lab-slr.html#covariance-matrices-have-properties",
    "href": "labs/lab-slr.html#covariance-matrices-have-properties",
    "title": "Bivariate analysis: simple linear regression",
    "section": "Covariance matrices have properties",
    "text": "Covariance matrices have properties\nThe empirical covariance matrix is the covariance matrix of the joint empirical distribution.\nAs a covariance matrix, the empirical covariance matrix is symmetric, semi-definite positive (SDP)\n\n\n\n\n\n\nSemi-definiteness\n\n\n\n\nA square \\(n \\times n\\) matrix \\(A\\) is semi-definite positive (SDP) iff\n\n\\[\\forall u \\in \\mathbb{R}^n, \\qquad  u^T \\times A u = \\langle u, Au \\rangle \\geq 0\\]\n\nA square \\(n \\times n\\) matrix \\(A\\) is definite positive (DP) iff\n\n\\[\\forall u \\in \\mathbb{R}^n \\setminus \\{0\\}, \\qquad  u^T \\times A u = \\langle u, Au \\rangle &gt; 0\\]\n\n\n\n\n\n\n\n\nLinear correlation coefficient\n\n\n\nThe linear correlation coefficient is defined from the covariance matrix as\n\\[\\rho = \\frac{\\operatorname{cov}_n(X, Y)}{\\sqrt{\\operatorname{var}_n(X)  \\operatorname{var}_n(Y)}}\\]\n\n\n By the Cauchy-Schwarz inequality, we always have\n\\[-1 \\leq \\rho \\leq 1\\]\n Translating and/or rescaling the columns does not modify the linear correlation coefficient!\nFunctions cov and cor from base  perform the computations\nVisualizing quantitative bivariate samples\nSuppose now, we want to visualize a quantitative bivariate sample of length \\(n\\).\nThis bivariate sample (a dataframe) may be handled as a real matrix with \\(n\\) rows and \\(2\\) columns\nGeometric concepts come into play\nExploring column space\nWe may attempt to visualize the two columns, that is the two \\(n\\)-dimensional vectors or the rows, that is \\(n\\) points on the real plane.\n If we try to visualize the two columns, we simplify the problem by projecting on the plane generated by the two columns\nThen what matters is the angle between the two vectors.\nIts cosine is precisely the linear correlation coefficient defined above\nExploring row space\nIf we try visualize the rows, the most basic visualization of a quantitative bivariate sample is the scatterplot.\nIn the grammar of graphics parlance, it consists in mapping the two variables on the two axes, and mapping rows to points using geom_point and stat_identity\nA Gaussian cloud\nWe build an artificial bivariate sample, by first building a covariance matrix COV (it is randomly generated). Then we build a bivariate normal sample s of length n and turn it into a dataframe u. The dataframe is then fed to ggplot.\n\nCodeset.seed(1515) # for the sake of reproducibility\n\nn &lt;- 100\nV &lt;- matrix(rnorm(4, 1, 1), nrow = 2)\nCOV &lt;- V %*% t(V)         # a random covariance matrix, COV is symmetric and SDP\n\ns &lt;- t(V %*% matrix(rnorm(2 * 10 * n), ncol=10*n))\nu &lt;- as_tibble(list(X=s[,1], Y=s[, 2]))                      # a bivariate normal sample\n\nemp_mean &lt;- as_tibble(t(colMeans(u)))\n\n\nNumerical summary\n\nMean vector (Empirical mean)\n\n\nCodet(colMeans(u)) %&gt;%\n  knitr::kable(digits = 3, col.names = c(\"$\\\\overline{X_n}$\", \"$\\\\overline{Y_n}$\"))\n\n\n\n\\(\\overline{X_n}\\)\n\\(\\overline{Y_n}\\)\n\n\n0.004\n-0.004\n\n\n\n\n\nCovariance matrix (Empirical covariance matrix)\n\n\nCodecov(u) %&gt;% as.data.frame() %&gt;% knitr::kable(digits = 3)\n\n\n\n\nX\nY\n\n\n\nX\n4.370\n-0.706\n\n\nY\n-0.706\n1.212\n\n\n\n\n\nCode\n\nCodep_scatter_gaussian &lt;- u %&gt;%\n  ggplot() +\n  aes(x = X, y = Y) +\n  geom_point(alpha = .25, size = 1) +\n  geom_point(data = emp_mean, color = 2, size = 5) +\n  stat_ellipse(type = \"norm\", level = .9) +\n  stat_ellipse(type = \"norm\", level = .5) +\n  stat_ellipse(type = \"norm\", level = .95) +\n  annotate(geom=\"text\", x=emp_mean$X+1.5, y= emp_mean$Y+1, label=\"Empirical mean\")+\n  geom_segment(aes(x=emp_mean$X, y=emp_mean$Y, xend=emp_mean$X+1.5, yend=emp_mean$Y+1)) +\n  coord_fixed() +\n  ggtitle(stringr::str_c(\"Gaussian cloud, cor = \",\n    round(cor(u$X, u$Y), 2),\n    sep = \"\"\n  ))\n\np_scatter_gaussian\n\nWarning in geom_segment(aes(x = emp_mean$X, y = emp_mean$Y, xend = emp_mean$X + : All aesthetics have length 1, but the data has 1000 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row."
  },
  {
    "objectID": "labs/lab-slr.html#qualitative-and-quantitative-variables",
    "href": "labs/lab-slr.html#qualitative-and-quantitative-variables",
    "title": "Bivariate analysis: simple linear regression",
    "section": "Qualitative and quantitative variables",
    "text": "Qualitative and quantitative variables\nConditional summaries\nFor each modality \\(i \\in \\mathcal{X}\\), we define:\n\nConditional Mean of \\(X\\) given \\(\\{ X = i \\}\\)\n\n\n\\[\\overline{Y}_{n\\mid i}  = \\frac{1}{n_i} \\sum_{k\\leq n} \\mathbb{I}_{x_k =i} \\times  y_k\\]\n\nConditional Variance \\(Y\\) given \\(\\{ X= i\\}\\)\n\n\n\\[\\sigma^2_{Y\\mid i}  = \\frac{1}{n_i} \\sum_{k \\leq n}  \\mathbb{I}_{x_k =i} \\times \\bigg( y_k  - \\overline{Y}_{n \\mid i}\\bigg)^2\\]\nHuygens-Pythagoras formula\n\\[\\sigma^2_{Y} =  \\underbrace{\\sum_{i\\in \\mathcal{X}} \\frac{n_i}{n} \\sigma^2_{Y \\mid i}}_{\\text{mean of conditional variances}}  + \\underbrace{\\sum_{i\\in \\mathcal{X}} \\frac{n_i}{n} \\big(\\overline{Y}_{n \\mid i} - \\overline{Y}_{n}\\big)^2}_{\\text{variance of conditional means}}\\]\n Check this\nRobust bivariate summaries\nIt is also possible and fruitful to compute\n\nconditional quantiles (median, quartiles) and\nconditional interquartile ranges (IQR)\n\nConditional mean, variance, median, IQR ()\n\nCodetit &lt;-  readr::read_csv(\"../DATA/titanic/train.csv\")\n\nRows: 891 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Name, Sex, Ticket, Cabin, Embarked\ndbl (7): PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodetit &lt;-  tit |&gt;\n    mutate(across(c(Survived, Pclass, Name, Sex, Embarked), as.factor)) \n\ntit |&gt;  \n  glimpse()\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;fct&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;fct&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;fct&gt; male, female, female, female, male, male, male, male, fema…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;fct&gt; S, C, S, S, S, Q, S, S, S, C, S, S, S, S, S, S, Q, S, S, C…\n\n\n\nCodetit %&gt;%\n  dplyr::select(Survived, Fare) %&gt;%\n  dplyr::group_by(Survived) %&gt;%\n  dplyr::summarise(cmean=mean(Fare, na.rm=TRUE), #&lt;&lt;\n                   csd=sd(Fare,na.rm = TRUE),\n                   cmedian=median(Fare, na.rm = TRUE),\n                   cIQR=IQR(Fare,na.rm = TRUE))\n\n# A tibble: 2 × 5\n  Survived cmean   csd cmedian  cIQR\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 0         22.1  31.4    10.5  18.1\n2 1         48.4  66.6    26    44.5\n\n\nVisualization of mixed bivariate samples\nVisualization of qualitative/quantitative bivariate samples\nconsists in displaying visual summaries of conditional distribution of \\(Y\\) given \\(X=i, i \\in \\mathcal{X}\\)\nBoxplots and violinplots are relevant here\nMixed bivariate samples from Titanic (violine plots)\n\nCodefiltered_tit &lt;- tit %&gt;%\n  dplyr::select(Pclass, Survived, Fare) %&gt;%\n  dplyr::filter(Fare &gt; 0 )\n\nv &lt;- filtered_tit %&gt;%\n  ggplot() +\n  aes(y=Fare) +\n  scale_y_log10()\n\n# vv &lt;- v + geom_violin()\n\nfiltered_tit |&gt; \n  glimpse()\n\nRows: 876\nColumns: 3\n$ Pclass   &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2…\n$ Survived &lt;fct&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0…\n$ Fare     &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21…\n\n\n\nCodep &lt;- v +\n  aes(x=Pclass) + \n  geom_violin() +\n  ggtitle(\"Titanic: Fare versus Passenger Class\")\n\nq &lt;- v +\n  aes(x=Survived) +\n  geom_violin() +\n  ggtitle(\"Titanic: Fare versus Survival\")\n\np + q\n\n\n\n\n\n\n\nMixed bivariate samples from Titanic (boxplots)\n\nCode(\nv + aes(x=Pclass) +\n  geom_boxplot() +\n  ggtitle(\"Titanic: Fare versus Passenger Class\")\n) + (\nv +\n  aes(x=Survived) +\n  geom_boxplot() +\n  ggtitle(\"Titanic: Fare versus Survival\")\n)\n\n\n\n\n\n\n\nDataset whiteside (from package MASS of )\n\nMr Derek Whiteside of the UK Building Research Station recorded the weekly gas consumption and average external temperature at his own house in south-east England for two heating seasons, one of 26 weeks before, and one of 30 weeks after cavity-wall insulation was installed. The object of the exercise was to assess the effect of the insulation on gas consumption.\n\nDataset whiteside\n\nGas and Temp are both quantitative variables while Insul is qualitative with two modalities (Before, After).\n\nInsul\n\nA factor, before or after insulation.\n\nTemp\n\nPurportedly the average outside temperature in degrees Celsius. (These values is far too low for any 56-week period in the 1960s in South-East England. It might be the weekly average of daily minima.)\n\nGas\n\nThe weekly gas consumption in 1000s of cubic feet.\n\n\n\nCodeMASS::whiteside %&gt;%\n  ggplot(aes(x=Insul, y=Temp)) +\n  geom_violin() +\n  ggtitle(\"Whiteside data: violinplots\")"
  },
  {
    "objectID": "labs/lab-slr.html#simple-linear-regression",
    "href": "labs/lab-slr.html#simple-linear-regression",
    "title": "Bivariate analysis: simple linear regression",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nWe now explore association between two quantitative variables\nWe investigate the association between two quantitative variables as a prediction problem\nWe aim at predicting the value of \\(Y\\) as a function of \\(X\\).\nWe restrict our attention to linear/affine prediction.\n\nWe look for \\(a, b \\in \\mathbb{R}\\) such that \\(y_i \\approx a x_i +b\\)\nMaking \\(\\approx\\) meaningful compels us to choose a goodness of fit criterion.\nSeveral criteria are possible, for example:\n\\[\n\\begin{array}{rl}\n\\text{Mean absolute deviation} & = \\frac{1}{n}\\sum_{i=1}^n \\big|y_i - a x_i -b \\big| \\\\ \\text{Mean quadratic deviation} & = \\frac{1}{n}\\sum_{i=1}^n \\big|y_i - a x_i -b \\big|^2\n\\end{array}\n\\]\nIn their days, Laplace championed the mean absolute deviation, while Gauss advocated the mean quadratic deviation. For computational reasons, we focus on minimizing the mean quadratic deviation.\n\n\n\n\n\n\n\nThe fourth chapter of Laplace treatise includes an exposition of the method of least squares, a remarkable testimony to Laplace’s command over the processes of analysis.\n\n\nIn 1805 Legendre had published the method of least squares, making no attempt to tie it to the theory of probability.\n\n\n\n\nIn 1809 Gauss had derived the normal distribution from the principle that the arithmetic mean of observations gives the most probable value for the quantity measured; then, turning this argument back upon itself, he showed that, if the errors of observation are normally distributed, the least squares estimates give the most probable values for the coefficients in regression situations"
  },
  {
    "objectID": "labs/lab-slr.html#least-square-regression",
    "href": "labs/lab-slr.html#least-square-regression",
    "title": "Bivariate analysis: simple linear regression",
    "section": "Least Square Regression",
    "text": "Least Square Regression\nMinimizing a cost function\nThe Least Square Regression problem consists of minimizing with respect to \\((a,b)\\) :\n\\[\n\\begin{array}{rl} \\ell_n(a,b)  & = \\sum_{i=1}^n \\big(y_i - a x_i -b \\big)^2  \\\\ & = \\sum_{i=1}^n \\big((y_i - \\overline{Y}_n) - a (x_i - \\overline{X}_n) + \\overline{Y}_n - a \\overline{X}_n-b \\big)^2 \\\\ & = \\sum_{i=1}^n \\big((y_i - \\overline{Y}_n) - a (x_i - \\overline{X}_n) \\big)^2 + n \\big(\\overline{Y}_n - a \\overline{X}_n-b\\big)^2\n\\end{array}\n\\]\nDeriving the solution\nThe function to be minimized is smooth and strictly convex over \\(\\mathbb{R}^2\\) : a unique minimum is attained where the gradient vanishes\nIt is enough to compute the partial derivatives.\n\\[\\begin{array}{rl}\\frac{\\partial \\ell_n}{\\partial a} & = - 2  \\operatorname{cov}(X,Y) + 2 a \\operatorname{var}(X) -2 n \\big(\\overline{Y}_n - a \\overline{X}_n-b\\big) \\overline{X}_n \\\\  \\frac{\\partial \\ell_n}{\\partial b} & = -2 n \\big(\\overline{Y}_n - a \\overline{X}_n-b\\big)\\end{array}\\]\nA closed-form solution\nZeroing partial derivatives leads to\n\\[\n\\begin{array}{rl}\n  \\widehat{a} & = \\frac{\\operatorname{cov}(X,Y)}{\\operatorname{var}(X)} \\\\\n  \\widehat{b} & = \\overline{Y}_n - \\frac{\\operatorname{cov}(X,Y)}{\\operatorname{var}(X)} \\overline{X}_n\n\\end{array}\n\\]\nor\n\\[\n\\begin{array}{rl}\n  \\widehat{a} & = \\rho \\frac{\\sigma_y}{\\sigma_x} \\\\\n  \\widehat{b} & = \\overline{Y}_n - \\rho\\frac{\\sigma_y}{\\sigma_x} \\overline{X}_n\n\\end{array}\n\\]\n If the sample were standardized, that is, if \\(X\\) (resp. \\(Y\\)) were divided by \\(\\sigma_X\\) (resp. \\(\\sigma_Y\\)), the slope of the regression line would be the correlation coefficient\nOverplotting the Gaussian cloud\n\nThe slope and intercept can be computed from the sample summary (empirical mean and covariance matrix)\nIn higher dimension, coefficients are from lm(...)\n\n\nCodep_scatter_gaussian +\n  geom_smooth(method=\"lm\", se=FALSE)\n\nWarning in geom_segment(aes(x = emp_mean$X, y = emp_mean$Y, xend = emp_mean$X + : All aesthetics have length 1, but the data has 1000 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nlm(formula, data)\n\nCodemod &lt;- lm(formula=Y ~ X, data=u)\n\nmod %&gt;% summary()\n\n\nCall:\nlm(formula = Y ~ X, data = u)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0168 -0.7106 -0.0079  0.7294  3.5773 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.003685   0.033145  -0.111    0.911    \nX           -0.161562   0.015864 -10.184   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.048 on 998 degrees of freedom\nMultiple R-squared:  0.09415,   Adjusted R-squared:  0.09324 \nF-statistic: 103.7 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n\nCodesqrt(sum((mod$residuals)^2)/(mod$df.residual))\n\n[1] 1.048131\n\nCodecor(u)^2\n\n           X          Y\nX 1.00000000 0.09414501\nY 0.09414501 1.00000000\n\n\nResiduals\nThe residuals are the prediction errors \\(\\left(y_i - \\widehat{a}x_i - \\widehat{b}\\right)_{i\\leq n}\\)\nResiduals play a central role in regression diagnostic\nThe Residual Standard Error, is the square root of the normalized sum of squared residuals:\n\\[\\frac{1}{n-2}\\sum_{i=1}^n \\left(y_i - \\widehat{a}x_i - \\widehat{b}\\right)^2\\]\nThe normalization coefficient is the number of rows \\(n\\) diminished by the number of adjusted parameters (the so-called degrees of freedom)\n\nCodesqrt(sum((mod$residuals)^2)/(mod$df.residual))\n\n[1] 1.048131\n\n\n This makes sense if we adopt a modeling perspective, if we accept the Gaussian Linear Models assumptions from the Statistical Inference course\n\nCodep_scatter_gaussian %+%\n  broom::augment(lm(Y ~ X, u)) +  #&lt;&lt;\n  geom_line(aes(x=X, y=.fitted)) +\n  geom_segment(aes(x=X, xend=X, y=.fitted, yend=Y,\n                   color=forcats::as_factor(sign(.resid))),\n               alpha=.2) +\n  theme(legend.position = \"None\") +\n  ggtitle(\"Gaussian cloud\",subtitle = \"with residuals\")\n\nWarning in geom_segment(aes(x = emp_mean$X, y = emp_mean$Y, xend = emp_mean$X + : All aesthetics have length 1, but the data has 1000 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\nThe residuals are the lengths of the segments connecting sample points to their projections on the regression line\n\nTechnically, the Multiple R-squared or\n\ncoefficient of determination is the squared empirical correlation coefficient \\(\\rho^2\\) between the explanatory and the response variables (in simple linear regression)\n\n\n\\[1 - \\frac{\\sum_{i=1}^n \\left(y_i - \\widehat{a}x_i - \\widehat{b}\\right)^2}{\\sum_{i=1}^n \\left(y_i - \\overline{Y}_n\\right)^2}= 1 - \\frac{\\sum_{i=1}^n \\left(y_i - \\widehat{y}_i \\right)^2}{\\sum_{i=1}^n \\left(y_i - \\overline{Y}_n\\right)^2}\\]\n\nCodecor(u$X, u$Y)^2\n\n[1] 0.09414501\n\nCode(sum(u$X*u$Y)/nrow(u) - mean(u$X)* mean(u$Y))*(nrow(u)/(nrow(u)-1))\n\n[1] -0.7059866\n\nCode((988/999)*cov(u$X, u$Y)/sqrt(var(u$X)*var(u$Y)))^2\n\n[1] 0.09208316\n\n\nIt is also understood as the share of the variance of the response variable that is explained by the explanatory variable\nThe Adjusted R-squared is a deflated version of Multiple R-squared\n\\[1 - \\frac{\\sum_{i=1}^n \\left(y_i - \\widehat{a}x_i - \\widehat{b}\\right)^2/(n-p-1)}{\\sum_{i=1}^n \\left(y_i - \\overline{Y}_n\\right)^2/(n-1)}\\]\nIt is useful when comparing the merits of several competing models (this takes us beyond the scope of this lesson)\n\n\nCodep_scatter_gaussian %+%\n  broom::augment(lm(Y ~ X, u)) +  #&lt;&lt;\n  geom_line(aes(x=X, y=.fitted)) +\n  geom_segment(aes(x=X,\n                   xend=X,\n                   y=.fitted,\n                   yend=Y,\n                   color=as_factor(sign(.resid))),\n               alpha=.2) +\n  theme(legend.position = \"None\") +\n  ggtitle(\"Gaussian cloud\",\n          subtitle = \"with residuals!\")\n\nWarning in geom_segment(aes(x = emp_mean$X, y = emp_mean$Y, xend = emp_mean$X + : All aesthetics have length 1, but the data has 1000 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row."
  },
  {
    "objectID": "labs/lab-slr.html#y-xt-beta-sigma-epsilon-the-biggest-lie",
    "href": "labs/lab-slr.html#y-xt-beta-sigma-epsilon-the-biggest-lie",
    "title": "Bivariate analysis: simple linear regression",
    "section": "\n\\(y = x^T \\beta + \\sigma \\epsilon\\) : The biggest lie?",
    "text": "\\(y = x^T \\beta + \\sigma \\epsilon\\) : The biggest lie?\n\n\n\n\n\n\n\nAny numeric bivariate sample can be fed to lm\nWhatever the bivariate dataset, you will obtain a linear prediction model\nIt is not wise to rely exclusively on the Multiple R-squared to assess a linear model\n Different datasets can lead to the same regression line and the same Multiple R-squared and the same Adjusted R-squared\n\n\n\n\nAnscombe quartet\n4 simple linear regression problems packaged in dataframe datasets::anscombe\n\ny1 ~ x1\ny2 ~ x2\ny3 ~ x3\ny4 ~ x4\n\n\nCodeanscombe &lt;- datasets::anscombe\n\nanscombe %&gt;% \n    gt::gt()\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\nAnscombe quartet: 4 datasets, 1 linear fit with almost identical goodness of fits\n\nCodelm(y1 ~ x1, anscombe) %&gt;% summary\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nCodelm(y2 ~ x2, anscombe) %&gt;% summary\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nCodelm(y3 ~ x3, anscombe) %&gt;% summary\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nCodelm(y4 ~ x4, anscombe) %&gt;% summary\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\n\nAll four numerical summaries look similar:\n\n\nIntercept \\(\\approx 3.0017\\)\n\n\nslope \\(\\approx 0.5\\)\n\nResidual standard error \\(\\approx 1.236\\)\n\nMultiple R-squared \\(\\approx .67\\)\n\nF-statistic \\(\\approx 18\\)\n\n\n\\(n\\) is equal to 11\nThe number of adjusted parameters \\(p\\) is 2 The number of degrees of freedom is \\(n-p=9\\)\nHow is RSE computed ?\n\\[\\frac{1}{n-p}\\sum_{i=1}^n \\left(y_j[i] - \\widehat{y}_j[i] \\right)^2\\]\n\n\n\n\n\n\nVisual inspection of the data reveals that some linear models are more relevant than others\n\n\n\nThis is the message of the Anscombe quartet.\nIt is made of four bivariate samples with \\(n=11\\) individuals.\n\nCodedatasets::anscombe %&gt;%\n  pivot_longer(everything(),  #&lt;&lt;\n    names_to = c(\".value\", \"group\"), #&lt;&lt;\n    names_pattern = \"(.)(.)\" #&lt;&lt;\n  )  %&gt;%\n  rename(X=x, Y=y) %&gt;%\n  arrange(group)-&gt; anscombe_long\n\n\nFrom https://tidyr.tidyverse.org/articles/pivot.html\nPerforming regression per group\nFor each value of group we perform a linear regression of Y versus X\n\nCodelist_lm &lt;- purrr::map(anscombe_long$group ,\n                      .f = \\(x) lm(Y ~ X,\n                                   anscombe_long,\n                                   subset = anscombe_long$group==x))\n\n\n Don’t Repeat Yourself (DRY)\nWe use functional programming: purrr::map(.l, .f) where\n\n.l is a list\n.f is a function to be applied to each item of list .l or a formula to be evaluated on each list item\n\npurrr package\nInspecting summaries\nAll four regressions lead to the same intercept and the same slope\nAll four regressions have the same Sum of Squared Residuals\nAll four regressions have the same Adjusted R-square\nWe are tempted to conclude that\n\nall four linear regressions are equally relevant\n\nPlotting points and lines helps dispel this illusion\nUnveiling points\n\nCodep &lt;- anscombe_long %&gt;%\n  ggplot(aes(x=X, y=Y)) +\n  geom_smooth(method=\"lm\", se=FALSE) +  #&lt;&lt;\n  facet_wrap(~ group) +                 #&lt;&lt;\n  ggtitle(\"Anscombe quartet: linear regression Y ~ X\")\n\n(p + (p + geom_point()))\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nAmong the four datasets, only the two left ones are righteously handled using simple linear regression\nThe bottom left dataset outlines the impact of outliers on Least Squares Minimization"
  },
  {
    "objectID": "labs/lab-slr.html#regression-on-the-whiteside-data",
    "href": "labs/lab-slr.html#regression-on-the-whiteside-data",
    "title": "Bivariate analysis: simple linear regression",
    "section": "Regression on the Whiteside data",
    "text": "Regression on the Whiteside data\n\nCodewhiteside &lt;- MASS::whiteside\nlm0 &lt;- whiteside %&gt;% \n  lm(Gas ~ Temp, .)\n\n\n\nCodelm0 |&gt; \n  broom::tidy() |&gt;\n  gt::gt() |&gt;\n  gt::fmt_number(\n    columns = -term,\n    decimals=2\n  )\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n5.49\n0.24\n23.28\n0.00\n\n\nTemp\n−0.29\n0.04\n−6.88\n0.00\n\n\n\n\n\n\n\nCodep &lt;- lm0 |&gt; \n  broom::augment(data=whiteside) |&gt;\n  ggplot() +\n  aes(x=Temp, y=Gas) +\n  geom_point(aes(shape=Insul)) \n  \np +\n  geom_smooth(\n    formula = y ~ x,\n    method=\"lm\",\n    se=FALSE\n    ) +\n  ggtitle(\"Gas ~ Temp, whiteside data\")\n\n\n\n\n\n\n\n\nCodelm_before &lt;- whiteside %&gt;% \n  filter(Insul==\"Before\") %&gt;% \n  lm(Gas ~ Temp, .)\n\nlm_after &lt;- whiteside %&gt;% \n  filter(Insul==\"After\") %&gt;% \n  lm(Gas ~ Temp, .)\n\n\n\nCodep +\n  geom_smooth(\n    formula = y ~ x,\n    method=\"lm\",\n    se=FALSE,\n    color=\"red\",\n    ) +\n  geom_abline(\n    intercept=coefficients(lm_before)[1],\n    slope=coefficients(lm_before)[2],\n    color='blue'\n  ) +\n  geom_abline(\n    intercept=coefficients(lm_after)[1],\n    slope=coefficients(lm_after)[2],\n    color='blue'\n  ) +\n  labs(\n    title=\"Gas ~ Temp, whiteside data\",\n    subtitle=\"Regressions per group in blue\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhich regression should we trust?\nCan we build confidence interval for estimated coefficients?\nCan we estimate noise intensity?\nCan we trust the homoschedasticity assumption?\nCan we trust\n\n\n\nUsing diagnostic plots\n\nCoderequire(ggfortify)\n\nLoading required package: ggfortify\n\nCodeautoplot(lm0, data=whiteside, shape='Insul')\n\n\n\n\n\n\n\n\nCodeautoplot(lm_before)\n\n\n\n\n\n\n\n\nCodeautoplot(lm_after)"
  },
  {
    "objectID": "labs/lab-lifeexp.html",
    "href": "labs/lab-lifeexp.html",
    "title": "Life expectancy: a global health index",
    "section": "",
    "text": "M1 MIDS MA7BY020\nUniversité Paris Cité\nAnnée 2024-2025\n\nCourse Homepage\n\nMoodle\n\n\n\n\n\n\n\nObjectives\nLoading\n\nCodedatafile &lt;- 'tamed_life_table.Rds'\nfpath &lt;- str_c(\"./DATA/\", datafile) # here::here('DATA', datafile)   # check getwd() if problem \n\nif (! file.exists(fpath)) {\n  download.file(\"https://stephane-v-boucheron.fr/data/tamed_life_table.Rds\", \n                fpath,\n                mode=\"wb\")\n}\n\nlife_table &lt;- readr::read_rds(fpath)\n\n\n\n\n\n\n\n\nReferences\n\n\n\nFor definitions of column, check on http://www.mortality.org the meaning of the different columns.\nSee also Demography: Measuring and Modeling Population Processes by SH Preston, P Heuveline, and M Guillot. Blackwell. Oxford. 2001.\nDocument Tables de mortalité françaises pour les XIXe et XXe siècles et projections pour le XXIe siècle contains detailed information on the construction of Life Tables for France.\n\n\nIn the sequel, we denote by \\(F_{t}\\) the cumulative distribution function for year \\(t\\). We agree on \\(\\overline{F}_t = 1 - F_t\\) and \\(F_t(-1)=0\\). Henceforth, \\(\\overline{F}\\) is called the survival function.\n\nqx\n\n(age-specific) risk of death at age \\(x\\), or mortality quotient at given age \\(x\\) for given year \\(t\\).\n\n\n\n\n\n\n\n\nAbout the definition of \\(q_{t,x}\\)\n\n\n\nDefining and computing \\(q_{t,x}\\) does not boil down to knowing the number of people at age \\(x\\) at the beginning of ear \\(t\\) and knowing how many of them died during year \\(t\\). If we want to be rigorous, we need to know all life lines in the Lexis diagram, or equivalently, how many people at Age \\(x\\) were alive on each day of Year \\(t\\).\n\n\n\n\n\n\n\n\nMortality quotients define a probability distribution\n\n\n\nFor a given year \\(t\\), the sequence of mortality quotients define a survival function \\(\\overline{F}_t\\) using the following recursion:\n\\[q_{t,x} = \\frac{\\overline{F}_t(x) - \\overline{F}_t(x+1)}{\\overline{F}_t(x)}\\] with boundary condition \\(\\overline{F}_t(-1) =1\\).\nThis recursion can also be read as:\n\\[\\overline{F}_{t}(x+1) = \\overline{F}_{t}(x) \\times (1-q_{t,x+1})\\, .\\]\nThis artificial probability distribution is used to define and compute life expectancies.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(q_{t,x}\\) is the hazard rate of \\(\\overline{F}_t\\) at age \\(x\\).\n\n\n\n\nex:\n\nResidual Life Expectancy at age \\(x\\) and year \\(t\\)\n\n\nThis is the expectation of \\(X -x\\) for a random variable \\(X\\) distributed according to \\(\\overline{F}_t\\) conditionnally on the event \\(\\{ X \\geq x \\}\\). That is \\(e_{t,x}\\) is the expectation of the probability distribution defined by \\(\\overline{F}_t(\\cdot + x-1)/\\overline{F}_t(x-1)\\).\nRearrangement\n\n\n\n\n\n\nQuestion\n\n\n\nFrom dataframe life_table, compute another dataframe called life_table_pivot with primary key Country, Gender and Year, with a column for each Age from 0 up to 110. For each age column, the entry should be the central death rate at the age defined by column, for Country, Gender and Year identifying the row.\nYou may use functions pivot_wider, pivot_longer from tidyr:: package.\nThe resulting schema should look like:\n\n\nColumn Name\nType\n\n\n\nCountry\nfactor\n\n\nGender\nfactor\n\n\nYear\ninteger\n\n\n0\ndouble\n\n\n1\ndouble\n\n\n2\ndouble\n\n\n3\ndouble\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nUsing life_table_pivot compute life expectancy at birth for each Country, Gender and Year using formula\n\\[e_{t,0} =  \\sum_{x=0}^\\infty \\overline{F}_t(x)\\]\n\n\nLife expectancy and window functions\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a function that takes as input a vector of mortality quotients, as well as an age, and returns the residual life expectancy corresponding to the vector and the given age.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a function that takes as input a dataframe with the same schema as life_table and returns a data frame with columns Country, Gender, Year, Age defining a primary key and a column res_lex containing residual life expectancy corresponding to the pimary key.\n\n\nIn order to compute residual life expectancies, you may consider using window functions over apropriately defined windows. The next window function suffices to compute life expectancy at birth. It computes the logarithm of survival probabilities for each Country, Year, Gender (partition) at each Age. Note that the expression mentions an aggregation function sum and that the correction of the result is ensured by a correct design of the frame argument.\n\n\n\n\n\n\nQuestion\n\n\n\nCompute residal life expectancies at all ages using window functions\nYou can use slider::slide().\n\n\nComputing residual life expectancies using window functions and accumulate\n\n\n\n\n\n\n\nThe official calculation of residual life expectancies assumes that except at age \\(0\\) and great age, people die uniformly at random between age \\(x\\) and \\(x+1\\): \\[\ne_{t,x} = (1- q_{t,x}) \\times (1 + e_{t,x+1}) + \\frac{1}{2} \\times q_{t,x}\n\\]\nThis recursion suggests a more efficient to compute residual life expectancies at all ages.\nIndeed, purrr::accumulate() allows to compute all values for \\(e_{t,x}\\) using exactly one pass over the table.\nSee https://purrr.tidyverse.org/reference/accumulate.html\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCompute and display residual life expectancies for ages \\(0\\) to \\(9\\) for year \\(1972\\)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPlot residual life expectancy as a function of Year at ages \\(60\\) and \\(65\\), facet by Gender and Country.\n\n\n\n\n\n\n\n\nQuestion"
  },
  {
    "objectID": "labs/lab-kmeans.html#setup",
    "href": "labs/lab-kmeans.html#setup",
    "title": "Clustering: k-means",
    "section": "Setup",
    "text": "Setup\n\nCodestopifnot(\n  require(DT),\n  require(skimr),\n  require(GGally),\n  require(patchwork),\n  require(ggforce),\n  require(glue),\n  require(ggfortify),\n  require(ggvoronoi),\n  require(magrittr),\n  require(broom),\n  require(tidyverse)\n)\n\ntidymodels::tidymodels_prefer(quiet = TRUE)\n\nold_theme &lt;-theme_set(\n  theme_minimal(base_size=9, \n                base_family = \"Helvetica\")\n)\n\n\n\nCodeknitr::opts_chunk$set(\n  message = FALSE,\n  warning = FALSE,\n  comment=NA,\n  prompt=FALSE,\n  cache=FALSE,\n  echo=TRUE,\n  results='asis'\n)\n\n\n\nCodegc &lt;- options(ggplot2.discrete.colour=\"viridis\")\ngc &lt;- options(ggplot2.discrete.fill=\"viridis\")\ngc &lt;- options(ggplot2.continuous.fill=\"viridis\")\ngc &lt;- options(ggplot2.continuous.colour=\"viridis\")"
  },
  {
    "objectID": "labs/lab-corr-babynames.html",
    "href": "labs/lab-corr-babynames.html",
    "title": "Babynames II: patterns of popularity",
    "section": "",
    "text": "Code\nstopifnot(\n  require(patchwork),\n  require(httr),\n  require(glue),\n  require(ineq),\n  require(here),\n  require(slider),\n  require(tidyverse),\n  require(gtools)\n)\n\n# old_theme &lt;- theme_set(theme_minimal())"
  },
  {
    "objectID": "labs/lab-corr-babynames.html#setup",
    "href": "labs/lab-corr-babynames.html#setup",
    "title": "Babynames II: patterns of popularity",
    "section": "Setup",
    "text": "Setup\n\n\nCode\npath_data &lt;- 'DATA'\nfname &lt;- 'nat2021_csv.zip'\nfpath &lt;- here(path_data, fname)\n\nif (!file.exists(fpath)){\n  url &lt;- \"https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip\"\n  download.file(url, fpath, mode=\"wb\")\n}   \n\ndf_fr &lt;- readr::read_csv2(fpath)\n\n\n\n\nCode\n  if (!require(\"babynames\")){\n  install.packages(\"babynames\")\n    stopifnot(require(\"b,abynames\"), \"Couldn't install and load package 'babynames'\")\n}\n\n\n\n\nCode\nlkp &lt;- list(year=\"annais\",\n  sex=\"sexe\",\n  name=\"preusuel\",\n  n=\"nombre\")\n\n\n\n\nCode\nbirths_fr_path &lt;- here(path_data, 't35.fr.xls')\nbirths_fr_url &lt;- 'https://www.ined.fr/fichier/s_rubrique/168/t35.fr.xls'\n\nif (!file.exists(births_fr_path)) {\n  download.file(births_fr_url, births_fr_path)\n}\n\n\n\n\nCode\nbirths_fr &lt;-  readxl::read_excel(births_fr_path, skip = 3)\n\nbirths_fr &lt;- births_fr[-1, ]  \nnames(births_fr)[1] &lt;- \"year\"\n\nbirths_fr &lt;- births_fr |&gt; \n  mutate(year=as.integer(year)) |&gt;\n  drop_na() \n\n\n\n\nCode\nbabynames &lt;- babynames |&gt;\n  mutate(country='us') |&gt;\n  mutate(sex=as_factor(sex))\n  \nbirths_us &lt;- births\n\n\n\n\nCode\ndf &lt;- bind_rows(babynames, df_fr)\n\n\n\n\nCode\ndf &lt;- df |&gt;\n  filter(year &gt; 1947) |&gt;\n  drop_na() |&gt;\n  filter(name!='_PRENOMS_RARES')\n\n\n\n\nCode\ndf &lt;- df |&gt; \n  group_by(year, sex, country) |&gt;\n  arrange(desc(n), .by_group=T) |&gt;\n  mutate(rnk=row_number(), \n         rrnk=rnk/n(), \n         cprop=cumsum(prop)) |&gt; \n  ungroup() \n\n\n\n\nCode\nmin_maj &lt;- function(cprop, rrnk){\n  1- rrnk[findInterval(.5, cprop)]  \n}\n\n\n\n\nCode\nlast_dec &lt;- function(cprop, rrnk) {\n  cprop[findInterval(.1, rrnk)]\n}\n\n\n\n\nCode\nineq_idx_fns &lt;- list(\n  gini=Gini, \n  atkinson=Atkinson, \n  ent=entropy,\n  theil=Theil)\n\nineq_idxes &lt;- df |&gt; \n  summarize(\n    across(n, .fns=ineq_idx_fns),\n    n_alker=min_maj(cprop, rrnk),\n    n_last_dec=last_dec(cprop, rrnk),\n    .by= c(year, sex, country),\n    ) |&gt;\n  pivot_longer(\n    cols=starts_with(\"n\"), \n    names_to=\"index_name\", \n    values_to=\"index\")\n\n\n\n\nCode\nineq_idxes |&gt; \n  ggplot() +\n  aes(x=year, y=index, color=sex) +\n  geom_line() +\n  facet_grid(rows=vars(index_name), cols=vars(country), scales=\"free_y\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf &lt;- df |&gt; \n  group_by(country, sex, name) |&gt;\n  mutate(best_rnk=min(rnk)) |&gt;\n  ungroup()\n\n\n\n\nCode\n(\n  df |&gt;\n  filter(rrnk&lt;.9, round(10000*rrnk)%%10==1) |&gt;\n  ggplot() +\n    aes(x=1-rrnk, y=1-cprop, color=sex, frame=year) +\n    geom_point(size=.2)  +\n    coord_fixed() +\n    facet_wrap(~ country) \n) |&gt; \n  plotly::ggplotly()\n\n\n\n\n\n\n\n\nCode\n(\n  df |&gt; \n    filter(rnk &lt;=10, year %% 30 ==0) |&gt;\n    ggplot() +\n    aes(x=rnk, y=prop, frame=year, fill=sex) +\n    geom_col(position=\"dodge\") +\n#    coord_flip() +\n    facet_grid(cols=vars(country), \n               rows=vars(year), \n      scales=\"free\")) \n\n\n\n\n\n\n\n\n\nCode\n# |&gt;  plotly::ggplotly()\n\n\n\n\nCode\ndf_fr &lt;- df_fr |&gt;\n  rename(!!!lkp) |&gt;\n  mutate(country='fr') |&gt;\n  mutate(sex=as_factor(sex)) |&gt;\n  mutate(sex=fct_recode(sex, \"M\"=\"1\", \"F\"=\"2\")) |&gt;\n  mutate(sex=fct_relevel(sex, \"F\", \"M\")) |&gt; \n  mutate(year=ifelse(year==\"XXXX\", NA, year)) |&gt;\n  mutate(year=as.integer(year)) |&gt;\n  group_by(year,sex) |&gt;\n  mutate(prop=n/sum(n)) |&gt; \n  ungroup() |&gt;\n  select(year, sex, name, n, prop, country)\n\n\nU+0128\n\n\nCode\nextract_pattern &lt;- \\(x) \n  str_c((as.character(lkp[as.character(x[x!=0 & !is.na(x)])])), collapse=\"\")\n\n\n\n\nCode\ndf &lt;- df |&gt; \n  group_by(country,sex, name) |&gt; \n  arrange(year) |&gt; \n  mutate(sprop=slide_dbl(pmax(prop, 1e-4), mean, .before=2, .after =2)) |&gt;\n  ungroup()\n\n\n\n\nCode\ndf_patterns &lt;- df |&gt; \n  group_by(country,sex, name) |&gt; \n  arrange(year) |&gt;\n  mutate(change=log(sprop)) |&gt; \n  mutate(change=sign(change-lag(change, default = change[1]))) |&gt; \n  summarise(change_pattern=extract_pattern(change), .groups = \"drop\") |&gt;\n  arrange(country,sex, change_pattern) \n\n\n\n\nCode\ndf_patterns |&gt; \n  filter(name %in% c('JULES', 'KEVIN', 'STÉPHANE', 'ARTHUR', 'MICHEL', 'EMILE'), sex=='M')\n\n\n# A tibble: 6 × 4\n  country sex   name     change_pattern                                         \n  &lt;chr&gt;   &lt;fct&gt; &lt;chr&gt;    &lt;chr&gt;                                                  \n1 fr      M     KEVIN    NULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNU…\n2 fr      M     ARTHUR   NULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNU…\n3 fr      M     JULES    NULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNU…\n4 fr      M     EMILE    NULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNU…\n5 fr      M     MICHEL   NULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNU…\n6 fr      M     STÉPHANE NULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNULLNU…\n\n\n\n\nCode\ndf  |&gt; \n  filter(name %in% c('STÉPHANE', 'KEVIN', 'ENZO'), sex=='M') |&gt; \n  ggplot() +\n  aes(x=year) +\n  geom_point(aes(y=prop, shape=name), color=\"blue\", alpha=.5, size=.2) +\n  geom_line(aes(y=sprop, linetype=name), color=\"red\", linewidth=.2) +\n  scale_y_log10()"
  },
  {
    "objectID": "projects/hmw-miashs-23-24-b.html#tables-de-mortalité-1900-1925",
    "href": "projects/hmw-miashs-23-24-b.html#tables-de-mortalité-1900-1925",
    "title": "Devoir II : Etude de tables de mortalite",
    "section": "Tables de mortalité (1900-1925)",
    "text": "Tables de mortalité (1900-1925)\nCe devoir porte sur les tables de mortalité (life tables) américaines et européennes entre 1900 et 1925.\nLes tables ont été obtenues de https://www.mortality.org.\nNous étudions les tables de mortalité de quelques pays d’Europe occidentale (France, Grande-Bretagne –en fait, Angleterre et Pays de Galles–, Italie, Pays-Bas, Espagne, et Suède) et des États-Unis d’Amérique.\nLes tables peuvent être téléchargées à l’aide des instructions suivantes :\n\ndatafile &lt;- 'full_life_table.Rds'\nfpath &lt;- stringr::str_c(\"./DATA/\", datafile) \n\n# here::here('DATA', datafile)   \n# check getwd() if problem \n\nif (! file.exists(fpath)) {\n  download.file(\"https://stephane-v-boucheron.fr/data/full_life_table.Rds\", \n                fpath,\n                mode=\"wb\")\n}\n\nlife_table &lt;- readr::read_rds(fpath)\n\n\nlife_table &lt;- life_table %&gt;%\n  mutate(Country = as_factor(Country)) %&gt;%\n  mutate(Country = fct_relevel(Country, \"Spain\", \"Italy\", \"France\", \n  \"England & Wales\", \"Netherlands\", \"Sweden\", \"USA\")) %&gt;%\n  mutate(Gender = as_factor(Gender)) \n\nlife_table &lt;- life_table %&gt;%\n  mutate(Area = fct_collapse(Country, \n                        SE = c(\"Spain\", \"Italy\", \"France\"), \n                        NE = c(\"England & Wales\", \"Netherlands\", \"Sweden\"), \n                        USA=\"USA\")) \n\nSur les données françaises, le document Tables de mortalité françaises pour les XIXe et XXe siècles et projections pour le XXIe siècle contient des informations utiles sur la construction des tables de mortalité.\n\nNotation (Rappel)\nDans la suite \\(F\\) désigne une fonction de répartition sur \\(\\mathbb{N} = \\mathbb{Z}_+\\), et \\(\\overline{F}=1 - F\\) la fonction de survie associée. Cette fonction \\(F\\) est définie à partir des quotients de mortalité (voir plus bas). Elle ne décrit pas la pyramide des âges. On définit une fonction pour chaque année \\(t\\). Pour chaque année, pays, sexe, \\(F_t(x)\\) est la proportion des membres d’une cohorte (fictive) qui vivent au moins jusqu’à l’âge \\(x\\) dans l’année \\(t\\).\n\nqx\n\n(age-specific) risque de décès à l’age (révolu) \\(x\\), ou encore quotient de mortalité à l’age \\(x\\) pour l’année \\(t\\): \\(q_{t,x} = \\frac{\\overline{F}_t(x) - \\overline{F}_t(x+1)}{\\overline{F}_t(x)}\\).\nPour chaque année, chaque âge, \\(q_{t,x}\\) est déterminé par les données de l’année.\n\n\nNous avons aussi \\[\\overline{F}_{t}(x+1) = \\overline{F}_{t}(x) \\times (1-q_{t,x+1})\\, .\\]\n\nmx\n\ntaux central de décès à l’âge (révolu) \\(x\\) durant l’année \\(t\\). C’est relié à \\(q_{t,x}\\) par \\[m_{t,x} = -\\log(1- q_{t,x}) \\,,\\] ou de manière équivalente \\(q_{t,x} = 1 - \\exp(-m_{t,x})\\).\n\nlx\n\nla fonction de survie: un multiple de proportion de personnes encore vivantes à l’âge \\(x\\). Ces valeurs sont calculées à partir de \\(q_{t,x}\\) via la formule \\[l_t(x+1) = l_t(x) \\times (1-q_{t,x}) \\, ,\\] avec \\(l_{t,0}\\), la racine (radix) de la table, en fait, choisi égal à \\(100000\\). Les fonctions \\(l_{t,\\cdot}\\) et \\(\\overline{F}_t\\) sont liées par \\[l_{t,x + 1} = l_{t,0} \\times \\overline{F}_t(x)\\,.\\]\n\ndx\n\n\\(d_{t,x} = q_{t,x} \\times l_{t,x}\\)\n\nTx\n\nNombre total de personnes-années vécues par la cohorte des gens d’âge compris entre \\(x\\) et \\(x+1\\) (pour une année donnée dans une société donnée). C’est nombre d’années vécues par les \\(l_{t, x+1}\\) personnes qui survivent à l’intervalle de temps, et les \\(d_{t,x}\\) personnes qui décèdent durant cette intervalle (ici l’intervalle est une année). Les premiers contribuent chacun exactement \\(1\\) année, alors que ces derniers contribuent, en moyenne, approximativement pour une demi-année. Ainsi \\(L_{t,x} = l_{t,x+1} + 0.5 \\times d_{t,x}\\). Cette approximation équivaut à supposer qu’un décès à l’âge révolu \\(x\\), intervient en moyenne au milieu de l’année. C’est acceptable excepté durant la première année (âge 0) et aux grands âges. Nous en restons à l’approximation simpliste \\(L_{t,x}= l_{t,x+1}\\).\n\nex:\n\nEspérance de vie résiduelle à l’âge \\(x\\) pour l’année \\(t\\). C’est (presque) l’espérance de la loi sur \\([0, \\infty)\\) définie par \\(F_t\\) (et donc par les qx), de la façon suivante: si \\(X \\sim F\\), c’est\n\n\n\\[\\mathbb{E}_{\\{X \\geq x\\}}\\left[X -x \\right]= \\frac{\\mathbb{E}\\left[(X-x) \\mathbb{I}_{X\\geq x}\\right]}{\\mathbb{E}\\left[\\mathbb{I}_{X\\geq x}\\right]}\\]\n\n\n\n\n\n\nTip\n\n\n\nLe package R nommé demography met à disposition un certain nombre d’outils et de concepts élaborés par les démographes.\n\n\nSources: Demography: measuring and modeling population processes. Preston, Heuveline et Guillot. Blackwell Publishing. 2001.\n\n\n\n\n\n\nQuestion\n\n\n\nPour chaque pays et chaque sexe, illustrer et commenter (brièvement) l’évolution des quotients de mortalité entre 1900 et 1913.\nRemarquer qu’on peut étudier qx comme une fonction de l’année \\(t\\), mais aussi pour une année donnée, étudier qx comme une fonction de l’âge x.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPour chaque pays, chaque sexe, chaque année entre 1900 et 1913, puis entre 1921 et 1925, effectuer une régression linéaire du logarithme du quotient de mortalité en fonction de l’âge, pour les âges compris entre 30 et 70 ans.\nIllustrer et commenter.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPour chaque pays et chaque sexe, considérer la cohorte des individus nés en 1890. Déterminer les quotients de mortalité effectivement subis par cette cohorte entre 1890 et 1980. Illustrer la différence entre les quotients de mortalité tirés des tables du moment de l’année 1890 et les quotients de mortalités effectivement subis."
  },
  {
    "objectID": "projects/hmw-miashs-23-24-b.html#barème",
    "href": "projects/hmw-miashs-23-24-b.html#barème",
    "title": "Devoir II : Etude de tables de mortalite",
    "section": "Barème",
    "text": "Barème\n\n\n\nCritère\nPoints\nDétails\n\n\n\n\nOrthographe et grammaire\n20%\nEnglish/Français \n\n\nGraphiques\n25%\nChoix des aesthetics, geom, scale … \n\n\nStyle des Graphiques\n15%\nTitres, légendes, étiquettes … \n\n\nManipulations de tables\n25%\n\n\n\nRespect DRY\n15%\nPrincipe DRY  Wikipedia\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nCeci n’est pas un devoir d’Histoire. Ne cherchez pas à montrer votre culture. Demandez-vous ce qu’il y a de remarquable dans les données, et énoncez les questions que ces données peuvent poser aux historiens."
  },
  {
    "objectID": "labs/lab-life-tables.html#setup",
    "href": "labs/lab-life-tables.html#setup",
    "title": "Introduction to Life Tables",
    "section": "Setup",
    "text": "Setup\ntidyverse conflicts"
  },
  {
    "objectID": "labs/lab-life-tables.html#different-kinds-of-data",
    "href": "labs/lab-life-tables.html#different-kinds-of-data",
    "title": "Introduction to Life Tables",
    "section": "Different kinds of data",
    "text": "Different kinds of data"
  },
  {
    "objectID": "labs/lab-life-tables.html#life-tables",
    "href": "labs/lab-life-tables.html#life-tables",
    "title": "Introduction to Life Tables",
    "section": "Life tables",
    "text": "Life tables"
  },
  {
    "objectID": "labs/lab-life-tables.html#table-wrestling",
    "href": "labs/lab-life-tables.html#table-wrestling",
    "title": "Introduction to Life Tables",
    "section": "Table wrestling",
    "text": "Table wrestling"
  },
  {
    "objectID": "projects/hmw-dev-2024.html",
    "href": "projects/hmw-dev-2024.html",
    "title": "MA7BY020 - Spring 2025",
    "section": "",
    "text": "Homework 3 (2023-24): R Programming\nDue date : 2024-05-29 23:55\n\nM1 MIDS & MFA\nUniversité Paris Cité\nAnnée 2023-2024\nCourse Homepage\n\n\n\n\n\n\n\n\n\n Objectives\nThis homework is concerned with developping methods for objects produced by factorial methods like Correspondence Analysis, Multiple Correspondence Analysis, Canonical Correlation Analysis, … (R).\n\n\nSupplementing the broom package\nThe broom package offers S3 generic functions for building dataframes from the output of a variety of statistical techniques (for example lm, prcomp, or kmeans): augment, tidy, and glance.\nThe first goal of this homework is to design and code methods for generic functions augment, tidy, and glance for classes CA, MCA, CCA.\nYou may use classes CA, MCA, and CCA from FactoMineR or design your own classes.\n\n\nProgramming with dplyr and ggplot2\nThe second goal of this homework is to design and code functions that take as input the output of augment, tidy, and glance (possibly simultaneously) to build ggplot objects corresponding to the plots associated with CA, MCA, and CCA (screeplot, row plot, column plot, and symmetric plot).\n\n\n\n\n\n\nNote\n\n\n\nggplot2 offers a generic function autoplot(). See Tidyverse documentation on autoplot, More generally have a look at automatic plotting\n\n\nThe third goal of this homework is to design and code methods for generic function autoplot() for classes CA, MCA, CCA.\n\n\n\n\n\n\nTip\n\n\n\nHave a look at autolayer() generic. Could be useful for implementing symmetric plots (biplots).\n\n\n\n\nPackage development\nThe function and methods coded in this homework should be delivered as a package.\nFollow the package devlopment guidelines in R Packages (2e).\n\nDefine a dedicated rstudio project for this homework\nDon’t forget documentation and testing\n\n\n\nReferences\n\nAdvanced R Programming\nS3\nProgramming with/for ggplot2\nCheatsheets\nPackages\n\n\n\n Grading criteria\n\n\n\n\n\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nDocumentation\n25%\nEnglish/French \n\n\nTesting\n25%\n\n\n\nCoding\n50%"
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "MA7BY020 Syllabus",
    "section": "",
    "text": "Schedule\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nTime\nLocation\nStart\n\n\n\n\nLab session 1\nTBA\nTBA\nSophie Germain TBA\n2025-01-13\n\n\nLab session 2\nTBA\nTBA - 18:15\nSophie Germain TBA\n2024-01-15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\n\n\n\n\n\n\n\n\nUniversité Paris Cité\n\n\n\nUseful links:\n\nCharte Université Paris Cité\nDémarches et accessibilité\nCentre de contact\nRelais handicap\n\n\n\n\n\n\n\n\n\nCommunication \n\n\n\nAll material is available from s-v-b.github.io/MA7BY020\nMoodle\nSubscribe to Moodle portal\n\n\n\n\n\n\n\n\nSoftware \n\n\n\n …\n\n\nR\nPosit\nrstudio\nquarto\n[jupyter]\n[vs code]\n\n\n\n\n\n\n\n\n\nReferences \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCourse material \n\n\n\nSlides\nThe labs are available (html and pdf)\nLabs corrections are available\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 projects:\n\n\\(\\textsf{P}_1\\)\n\\(\\textsf{P}_2\\)\n\\(\\textsf{P}_3\\)\n\n2 Examinations\n\n\\(\\textsf{EST}_1\\)\n\\(\\textsf{EST}_2\\)\n\nGrading\n\n\\[.2 (\\textsf{P}_1 + \\textsf{P}_2) + . 3 \\textsf{P}_3 + .1 (\\textsf{EST}_1+\\textsf{EST}_2)\\]\n\n\n\n\n\n\n\n\n Tips\n\n\n\n\n\n\n\n\n\n\n\n\nCode of conduct\n\n\n\nTL;DR: No cheating!\n\n\n\n\n\n\n\n\nSave the dates ! \n\n\n\nClick here for U Paris Cite Calendar.\nClick here for M1 MIDS Calendar",
    "crumbs": [
      "Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "projects-listings.html",
    "href": "projects-listings.html",
    "title": "Projects",
    "section": "",
    "text": "Note\n\n\n\nProjects …\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Titre\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitre\n\n\nDescription\n\n\n\n\n\n\nInvalid Date\n\n\nHomework 3 (2023-24): R Programming\n\n\n \n\n\n\n\nInvalid Date\n\n\n Objectives\n\n\n \n\n\n\n\nInvalid Date\n\n\nConcentration des distributions de prénoms\n\n\n \n\n\n\n\nInvalid Date\n\n\nDevoir II : Etude de tables de mortalite\n\n\n \n\n\n\n\nFeb 24, 2023\n\n\nHomework 1 (2023): Data Wrangling and Visualization\n\n\nDue date : 2023-02-24 23:55 (this is a hard deadline)\n\n\n\n\nFeb 24, 2023\n\n\nHomework 1 (2023): Data Wrangling and Visualization\n\n\nDue date : 2023-02-24 23:55 (this is a hard deadline)\n\n\n\n\nJan 13, 2024\n\n\nHmw I : Tables and visualization\n\n\n \n\n\n\n\n \n\n\nLAB: Fake report\n\n\n \n\n\n\n\n \n\n\nHmw template\n\n\n \n\n\n\n\n \n\n\nPropaganda, start the spark session\n\n\n \n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nTips\n\n\n\n\n\n\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nSpelling and syntax\n20%\nEnglish/French \n\n\nPlots correction\n20%\nchoice of aesthetics, geom, scale … \n\n\nPlot style\n15%\nTitles, legends, labels, breaks … \n\n\nTable wrangling\n15%\nETL, SQL like manipulations \n\n\nComputing Statistics\n15%\nAggregations, LR, PCA, CA, … \n\n\nDRY compliance\n15%\nDRY principle at  Wikipedia"
  },
  {
    "objectID": "slides-listings.html",
    "href": "slides-listings.html",
    "title": "Slides",
    "section": "",
    "text": "Slides summarize the lectures. Feel free to watch them before and after the lectures.\nIcon  point to material to be developped on blackboard.\n\n\n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitre\n\n\nDescription\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nMode d’emploi\n\n\n\nSlides use library revealjs from . They are displayed in your browser.\nTo get help, press",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "quarto-format.html",
    "href": "quarto-format.html",
    "title": "Quarto",
    "section": "",
    "text": "Un format\n\n\nUn traducteur",
    "crumbs": [
      "Support",
      "Quarto"
    ]
  },
  {
    "objectID": "labs-solutions-listings.html",
    "href": "labs-solutions-listings.html",
    "title": "Labs Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Tags\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\nJan 15, 2025\n\n\nIntroduction and Visualization\n\n\nVisualization, Public Statistics\n\n\n\n\nJan 15, 2025\n\n\nBrush up your R\n\n\nR language, Tidyverse, IDE\n\n\n\n\nJan 22, 2025\n\n\nGSS data\n\n\nUnivariate data, GSS\n\n\n\n\nJan 22, 2025\n\n\nTable wranglig\n\n\nR language, dplyr, tabula data\n\n\n\n\nJan 29, 2025\n\n\nBivariate data. GSS\n\n\nGSS, bivariate data, mosaicplots, scatterplots, simple linear regression\n\n\n\n\nJan 29, 2025\n\n\nMultiple Linear regression: OLS\n\n\nLinear regression, OLS, lm\n\n\n\n\nFeb 5, 2025\n\n\nMultiple Linear Regression: diagnostics, broom\n\n\nLinear regression, diagnostics, broom\n\n\n\n\nFeb 5, 2025\n\n\nLinear regression, Variable selection, Transformations\n\n\nLinear regression, Variable selection, Transformations, Penalized LS\n\n\n\n\nFeb 12, 2025\n\n\nBi and Multivariate categorical data\n\n\nCategorical data, Tests, Area plots\n\n\n\n\nFeb 12, 2025\n\n\nTemporal data\n\n\nTmeporal data, Time series, Babynames\n\n\n\n\nFeb 19, 2025\n\n\nSVD\n\n\nSVD, PCA, CA, MCA, CCA\n\n\n\n\nFeb 19, 2025\n\n\nSVD and PCA\n\n\nSVD, PCA, CA, MCA, CCA\n\n\n\n\nFeb 19, 2025\n\n\nSVD and PCA\n\n\nSVD, PCA, CA, MCA, CCA\n\n\n\n\nMar 5, 2025\n\n\nPCA, CA, MCA\n\n\nSVD, PCA, CA, MCA, CCA\n\n\n\n\nMar 5, 2025\n\n\nPCA, CA, MCA\n\n\nSVD, PCA, CA, MCA, CCA\n\n\n\n\nMar 12, 2025\n\n\nSVD plots and R programming\n\n\nSVD, tidy evaluation, R packages\n\n\n\n\nMar 12, 2025\n\n\nSVD plots and R programming (continued)\n\n\nSVD, tidy evaluation, R packages\n\n\n\n\nMar 12, 2025\n\n\nSVD, Life tables and Lee-Cater modeling\n\n\nSVD, Life tables, Lee-Carter\n\n\n\n\nMar 19, 2025\n\n\nR programming Genericity\n\n\nR programming, S3\n\n\n\n\nMar 19, 2025\n\n\nR programming Vectorization\n\n\nR programming, Vectorization\n\n\n\n\nMar 26, 2025\n\n\nClustering: kmeans\n\n\nClustering, k-means\n\n\n\n\nMar 26, 2025\n\n\nClustering: Hierarchical\n\n\nClustering, Hierarchical\n\n\n\n\nApr 2, 2025\n\n\nClustering: kmeans\n\n\nClustering, k-means\n\n\n\n\nApr 2, 2025\n\n\nClustering: Hierarchical\n\n\nClustering, Hierarchical\n\n\n\n\nApr 9, 2025\n\n\nSpatial data visualization\n\n\nClustering, k-means\n\n\n\n\nApr 9, 2025\n\n\nSpatio-Temporal data visualization\n\n\nClustering, Hierarchical\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Solutions"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA7BY020: Exporatory Data Analysis",
    "section": "",
    "text": "twitter\n  \n  \n    \n     Github\n  \n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nSyllabus\nTeaching team\nComputing environment\nSlides\nLabs\nLabs solutions\nProjects\nPast examninations",
    "crumbs": [
      "Information",
      "Glimpse"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing environment",
    "section": "",
    "text": "Version &gt; 4.0\nCheatsheets\n\n\nIDEs\n\nrstudio\nvs code\nemacs\njupyter\n\n\n\nPositcloud\nPosit Cloud lets you access Posit’s powerful set of data science tools right in your browser – no installation or complex configuration required.\n\n\nQuarto\nDownload from Quarto Website\n\n\n\n\n\n\nFrom the Quarto website\n\n\n\n\nAn open-source scientific and technical publishing system\nAuthor using Jupyter notebooks or with plain text markdown in your favorite editor.\nCreate dynamic content with Python, R, Julia, and Observable.\nPublish reproducible, production quality articles, presentations, dashboards, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more.\nShare knowledge and insights organization-wide by publishing to Posit Connect, Confluence, or other publishing systems.\nWrite using Pandoc markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\n\n\n\n\n\n Docker",
    "crumbs": [
      "Support",
      "Computing resources"
    ]
  },
  {
    "objectID": "labs-listings.html",
    "href": "labs-listings.html",
    "title": "Labs",
    "section": "",
    "text": "Note\n\n\n\nSessions are orgnized around labs. Feel free to look at the lab before sessions. Do not rush to solutions proposed here\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nBabynames I\n\n\nBabynames, Inequalities, Lorenz curve, Gini index\n\n\n\n\nBabynames II: patterns of popularity\n\n\nBabynames, Window functions\n\n\n\n\nBivariate analysis\n\n\nBivariate analysis, Boxplots, Pairplots\n\n\n\n\nBivariate analysis: simple linear regression\n\n\nBivariate analysis, Simple Linear regression, Correlation\n\n\n\n\nClustering: hierarchical\n\n\nClustering, Hierarchical clustering, Dendrograms\n\n\n\n\n\nClustering: k-means\n\n\n\nClustering, k-means, Lloyd's Algorithm\n\n\n\n\n\nCorrespondence Analysis of Mortality Data\n\n\nCA, Correspondence Analysis, SVD, Mortality Data\n\n\n\n\nCorrespondence Analysis of Survey Data\n\n\nCA, Correspondence Analysis, SVD, Likert, Vaccine hesitancy, EpiCov\n\n\n\n\nData visualization\n\n\nVisualization, Rosling, gapminder, ggplot2, Grammar of Graphics\n\n\n\n\nData visualization for OECD data\n\n\nVisualization, OECD, Grammar of Graphics, SDXM\n\n\n\n\nData visualization with Plotly\n\n\nVisualization, Rossling, gapminder, plotly, Grammar of Graphics\n\n\n\n\nGSS R: installation and first exploration\n\n\nGSS, Package Installatoin, ECDF, QQ plots\n\n\n\n\nGeneral Social Survey, Univariate Analysis\n\n\nGSS, Univariate Analysis, Null values, Lexikon\n\n\n\n\nIntroduction to Life Tables\n\n\nLife tables, Mortality quotients, Life Expectancy\n\n\n\n\nLife Tables: 1948-2006\n\n\nLife tables, Mortality quotients, Life Expectancy, 1948-2006\n\n\n\n\nLife expectancy: a global health index\n\n\n \n\n\n\n\nLife tables, EDA, Mortality quotients\n\n\nLife tables, Mortality quotients, Life Expectancy\n\n\n\n\nLife tables, Lee-Carter Modeling\n\n\nLife tables, Lee Carter, Motalit quotients, SVD\n\n\n\n\nLinear Regression: Exercises on Gaussian Linear Modeling\n\n\nGaussian Linear Modeling, Testing, Diagnosis, Linear regression\n\n\n\n\nLinear regression I\n\n\n \n\n\n\n\nLinear regression II\n\n\nLinear regression, OLS, Diagnostics\n\n\n\n\nLinear regression, diagnostics, variable selection\n\n\nLinear regression, Diagnostics, Formula\n\n\n\n\nLinear regression: ANOVA\n\n\nLinear Regression, ANOVA, Variable selection\n\n\n\n\nPCA I: Up and running\n\n\nPVA, svd, broom, SVD plots\n\n\n\n\nPCA II: Swiss fertility data\n\n\nPCA, SVD, Fertility data, SVD plots\n\n\n\n\nR language: a tour\n\n\nR, Vectors, Lists, Arrays, Data.frames, Functions\n\n\n\n\nR programming: generic programming, tidy evaluation\n\n\nR, S3 classes, Tidy evaluation\n\n\n\n\nR programming: vectorization\n\n\nR programming, vectorization, functional programming\n\n\n\n\nR programming: vectors\n\n\nR programming, S3 vectors, vctrs\n\n\n\n\nTable manipulations I: dplyr and SQL\n\n\nSQL, dplyr, nycflights13\n\n\n\n\nTables manipulation II\n\n\nTables, tibbles, dplyr, SQL, Relational Algebra, nycflights13\n\n\n\n\nTesting Bernoulli and Binomial parameters\n\n\nTest, Goodness-of-fit, Bernoulli parameter, Sex ratio, Chi-square\n\n\n\n\nTesting independence\n\n\nTest, Independence, Conditional Independence, Simpson, Chi-square\n\n\n\n\nUnivariate analysis I\n\n\nUnivariate analysis, Numerical data, Boxplots, ECDF, Histograms, Quantiles, GSS\n\n\n\n\nUnivariate analysis II\n\n\nUnivariate analysis, Categorical data, Factors, Column plots, Bar plots, GSS\n\n\n\n\nUniveariate analysis: Historgrams and Density plots\n\n\nHistograms, Density plots, Bandwidth selection\n\n\n\n\nVisualization: GSS data\n\n\nVisualization, GSS\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n\nTip\n\n\n\nBefore working out a lab, make sure the relevant packages are installed in your environment.",
    "crumbs": [
      "Labs"
    ]
  },
  {
    "objectID": "posit-cloud.html",
    "href": "posit-cloud.html",
    "title": "Positcloud",
    "section": "",
    "text": "Website",
    "crumbs": [
      "Support",
      "Posit cloud"
    ]
  },
  {
    "objectID": "rstudio-client.html#project-options",
    "href": "rstudio-client.html#project-options",
    "title": "Rstudio Desktop",
    "section": "Project options",
    "text": "Project options",
    "crumbs": [
      "Support",
      "rstudio"
    ]
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": " Teacher",
    "section": "",
    "text": "Teacher",
    "crumbs": [
      "Information",
      "Team"
    ]
  },
  {
    "objectID": "projects/hmw_template.html#grading-criteria",
    "href": "projects/hmw_template.html#grading-criteria",
    "title": "Hmw template",
    "section": " Grading criteria",
    "text": "Grading criteria\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nSpelling and syntax\n20%\nEnglish/French \n\n\nPlots correction\n20%\nchoice of aesthetics, geom, scale … \n\n\nPlot style\n15%\nTitles, legends, labels, breaks … \n\n\nTable wrangling\n15%\nETL, SQL like manipulations \n\n\nComputing Statistics\n15%\nAggregations, LR, PCA, CA, … \n\n\nDRY compliance\n15%\nDRY principle at  Wikipedia"
  },
  {
    "objectID": "labs/lab-vectorization.html",
    "href": "labs/lab-vectorization.html",
    "title": "R programming: vectorization",
    "section": "",
    "text": "Vectors in R\nvctrs package"
  },
  {
    "objectID": "labs/lab-vectorization.html#most-mathematical-functions",
    "href": "labs/lab-vectorization.html#most-mathematical-functions",
    "title": "R programming: vectorization",
    "section": "Most mathematical functions",
    "text": "Most mathematical functions\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the type of the input? of the output?\nx &lt;- c(6:-4)\nsqrt(x)  \nlog(x)"
  },
  {
    "objectID": "projects/hmw_gapminder_oecd.html",
    "href": "projects/hmw_gapminder_oecd.html",
    "title": "Hmw I : Tables and visualization",
    "section": "",
    "text": "Due : January 26, 2024"
  },
  {
    "objectID": "projects/hmw_gapminder_oecd.html#grading-criteria",
    "href": "projects/hmw_gapminder_oecd.html#grading-criteria",
    "title": "Hmw I : Tables and visualization",
    "section": " Grading criteria",
    "text": "Grading criteria\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nSpelling and syntax\n20%\nEnglish/French \n\n\nPlots correction\n20%\nchoice of aesthetics, geom, scale … \n\n\nPlot style\n15%\nTitles, legends, labels, breaks … \n\n\nTable wrangling\n15%\nETL, SQL like manipulations \n\n\nComputing Statistics\n15%\nAggregations, LR, PCA, CA, … \n\n\nDRY compliance\n15%\nDRY principle at  Wikipedia"
  },
  {
    "objectID": "projects/hmw-miashs-23-24-a.html#objectifs",
    "href": "projects/hmw-miashs-23-24-a.html#objectifs",
    "title": "Concentration des distributions de prénoms",
    "section": "Objectifs",
    "text": "Objectifs\nAttendu : une fichier au format Rmarkdown (.Rmd) ou Quarto (.qmd), compilable en format html.\nDans ce fichier, on trouvera le code nécessaire à la génération des graphiques et des tables correspondants aux questions ci-dessous."
  },
  {
    "objectID": "projects/hmw-miashs-23-24-a.html#mesures-de-linégalité-et-de-la-diversité",
    "href": "projects/hmw-miashs-23-24-a.html#mesures-de-linégalité-et-de-la-diversité",
    "title": "Concentration des distributions de prénoms",
    "section": "Mesures de l’inégalité et de la diversité",
    "text": "Mesures de l’inégalité et de la diversité\nLes mesures de l’inégalité d’une distribution ont été élaborées pour étudier des questions posées en:\n\nÉconomie (distribution des revenus, des patrimoines, politiques de redistribution, …)\nBiodiversité (distribution des espèces vivant sur un territoire, …)\nPolitique,\n…\n\n\n\n\n\n\n\nTip\n\n\n\nLe package R nommé ineq met à disposition un certain nombre de concepts élaborés depuis plus d’un siècle.\n\n\nOn peut aussi utiliser ces mesures de l’inégalité/de la concentration pour étudier la distribution des prénoms donnés chaque année dans une société (un pays, une région, …)\nSchématiquement, une distribution sur une population de \\(n\\) individus, est un vecteur de longueur \\(n\\) à coefficients positifs, que l’on notera \\(x\\). Dans le cas qui nous intéresse, si \\(n\\) prénoms distincts ont été utilisés durant une année, \\(x_i\\) sera le nombre de fois où le prénom \\(i\\) aura été attribué dans l’année, \\(p_i \\stackrel{\\text{def}}{=}x_i / \\Big(\\sum_{j=1}^n x_j\\Big)\\), représentera la part des nouveaux-nés ayant reçu le prénom \\(i\\) (la popularité du prénom \\(i\\)).\nQuand on s’intéresse au caractère plus ou moins inégalitaire d’une distribution, l’important n’est pas qui reçoit quoi, mais quelle quantité reçoivent les mieux lotis ou les moins bien lotis. Si une distribution \\(y\\) sur les \\(n\\) prénoms peut être obtenue à partir de \\(x\\) en échangeant les rôles de quelques prénoms (par exemple en renommant les Pierre en Paul et vice versa), \\(y\\) est ni plus ni moins inégalitaire que \\(x\\). Une bonne mesure de l’inégalité des distributions doit être invariante par permutation.\nDans la suite, on suppose les vecteurs représentants les permutations triés par ordre croissant. Si \\(x\\) et \\(y\\) représentent deux distributions sur \\(n\\) individus (prénoms pour nous). On dira que \\(x ≼ y\\) (\\(y\\) majore \\(x\\)) si pour tout \\(i\\leq n\\) \\[\\sum_{j\\leq i}p_j = \\frac{\\sum_{j\\leq i} x_j}{\\sum_{j\\leq n}   x_j} \\geq  \\frac{\\sum_{j\\leq i} y_j}{\\sum_{j\\leq n} y_j}= \\sum_{j\\leq i} q_j\\] autrement dit, si pour tout \\(i\\leq n\\), la part de richesse allouée au \\(i\\) moins bien lotis dans \\(x\\) est plus grande que dans \\(y\\).\nDeux distributions de richesse sur un même ensemble ne sont pas toujours comparables au sens de l’ordre \\(≼\\): il se peut que l’on n’ait ni \\(x ≼ y\\), ni \\(y ≼ x\\). Il s’agit d’un ordre partiel.\nLa courbe de Lorenz \\(L_x\\) d’un vecteur \\(x\\) est définie par \\[L_x(i/n) = \\frac{\\sum_{j\\leq i} x_j}{\\sum_{j\\leq n}   x_j}\\qquad \\text{pour }1\\leq i\\leq n\\] et \\[L_x(t) = L_x(i/n) \\qquad \\text{pour } \\frac{i-1}{n}&lt; t\\leq \\frac{i}{n}\\]\nDessiner les courbes de Lorenz des vecteurs \\(x\\) et \\(y\\) permet de visualiser la relation \\(≼\\) : si \\(x ≼ y\\) alors \\(L_x(t) \\geq L_y(t)\\) pour tout \\(t \\in ]0,1[\\).\nLes courbes de Lorenz étant des fonctions de \\(]0,1[\\) dans \\([0,1]\\), on peut superposer sur un même graphique des courbes de Lorenz définies par des vecteurs de longueur différentes. Pour nous, cela permet de comparer les distributions de prénoms des deux sexes dans un même pays durant une même année, de comparer des années différentes, etc.\n\nSi \\(\\phi\\) doit être utilisée comme une mesure, un indice d’inégalité alors \\[x ≼ y \\Longrightarrow  \\phi(x) \\leq \\phi(y)\\] soit \\(\\phi\\) est Schur-convexe.\n\\(\\phi(ax) = \\phi(x)\\) (pour \\(a&gt;0\\)) (invariance par changement d’échelle)\nL’extremum est atteint lorsque toutes les composantes de \\(x\\) sont égales.\n\nSources: Section F. de Measuring Inequality and Diversity dans Inequalities: Theory of Majorization and its Applications Marshall et Olkin. Springer-Verlag.\n\n\n\n\n\n\nQuestion\n\n\n\nCalculer pour chaque année, sexe et pays les indicateurs suivants de la dispersion/concentration de la distribution des prénoms\n\nIndice de Gini (vu en cours) \\(1 -2\\int_{0}^1 L_p(z)\\mathrm{d}z\\)\nEntropie de Shannon \\((p_i)_{i\\leq N} \\mapsto \\sum_{i=1}^N p_i \\log_2 p_i\\)\nEntropie de Rényi (ordre 2) \\((p_i)_{i\\leq N} \\mapsto - \\log_2\\left(\\sum_{i=1}^N p^2_i\\right)\\) (voir aussi mesure de diversité de Simpson)\nMajorité minimale d’Alker: \\(\\inf \\{ z : L_x(z) \\geq 1/2\\}\\)\nPart du dernier décile: pour \\(\\alpha=10\\%\\), \\(p \\mapsto 1- L_p(1-\\alpha)\\)\nIndice(s) d’Atkinson \\(1 - N \\left(\\frac{1}{N} \\sum_{i=1}^N  p_i^{1-a} \\right)^{1/(1-a)}\\) pour \\(a \\in (0,1)\\), choisir \\(\\alpha\\).\n\nLe résultat sera conservé dans une table où chaque ligne correspondra à un pays, une année, un sexe (la clé) avec en plus une colonne par indicateur.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nTracer les graphes de l’évolution de ces indicateurs de la dispersion/concentration de la distribution. Utiliser le mécanisme des facettes pour juxtaposer les graphes correspondants aux quatre couples (Pays, Sexe). Pour chaque (Pays, Sexe), superposez les graphes des indicateurs en fonction du temps."
  },
  {
    "objectID": "projects/hmw-miashs-23-24-a.html#ajustement-à-une-loi-de-zipf",
    "href": "projects/hmw-miashs-23-24-a.html#ajustement-à-une-loi-de-zipf",
    "title": "Concentration des distributions de prénoms",
    "section": "Ajustement à une loi de Zipf",
    "text": "Ajustement à une loi de Zipf\nUne distribution de Zipf est classiquement une loi définie à partir le nombre d’occurrences des mots dans un texte. une loi sur \\(\\mathbb{N}\\). On utiliser cette approche pour étudier la distribution des prénoms donnés une année aux bébés d’un sexe donné: on range les prénoms par popularité décroissante, et on trace le graphe de la popularité en fonction du rang. Pour visualiser, on choisit des échelles logarithmiques pour les deux axes. On appelle ces graphes des diagrammes de Zipf.\n\n\n\n\n\n\nQuestion\n\n\n\nTracer les digrammes de Zipf, pour les deux sexes, les USA et la France, pour les années \\(1950\\), \\(1990\\), et \\(2015\\)."
  },
  {
    "objectID": "projects/hmw-miashs-23-24-a.html#profils-de-popularité",
    "href": "projects/hmw-miashs-23-24-a.html#profils-de-popularité",
    "title": "Concentration des distributions de prénoms",
    "section": "Profils de popularité",
    "text": "Profils de popularité\nNous croyons qu’il existe quatre sortes de prénoms. La première catégorie consiste en des prénoms qui ont connu une baisse continuelle de popularité depuis la seconde guerre mondiale. La deuxième catégorie comprend les noms qui ont connu une hausse continuelle de leur popularité durant cette période. La troisième catégorie est constituée de prénoms qui sont devenus progressivement à la mode et qui sont ensuite retournés dans l’ombre. La quatrième catégorie est constituée de prénoms qui ont décliné puis connu un regain de popularité.\n\n\n\n\n\n\nCaution\n\n\n\n“Baisse continuelle”, “hausse continuelle” ne sont pas des notions formalisées. Pour donner un sens effectif aux quatre catégories, on peut lisser les popularités en calculant des moyennes mobiles, et analyser les variations des series lissées.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nConsidérez les prénoms qui ont figuré au moins une fois depuis 1948 parmi les \\(300\\) prénoms les plus populaires dans leur pays (pour un genre donné). Proposez une classification de ces prénoms en fonction de l’évolution de leur popularité au cours de \\(70\\) dernières années."
  },
  {
    "objectID": "projects/hmw-miashs-23-24-a.html#barème",
    "href": "projects/hmw-miashs-23-24-a.html#barème",
    "title": "Concentration des distributions de prénoms",
    "section": "Barème",
    "text": "Barème\n\n\n\nCritère\nPoints\nDétails\n\n\n\n\nOrthographe et grammaire\n20%\nEnglish/Français \n\n\nGraphiques\n25%\nChoix des aesthetics, geom, scale … \n\n\nStyle des Graphiques\n15%\nTitres, légendes, étiquettes … \n\n\nManipulations de tables\n25%\n\n\n\nRespect DRY\n15%\nPrincipe DRY  Wikipedia"
  },
  {
    "objectID": "labs/lab-progr.html",
    "href": "labs/lab-progr.html",
    "title": "R programming: generic programming, tidy evaluation",
    "section": "",
    "text": "Codestopifnot(\n  require(broom),\n  require(devtools),\n  require(ggforce),\n  require(ggfortify),\n  require(glue),\n  require(Hmisc),\n  require(lobstr),\n  require(patchwork),\n  require(rlang),\n  require(skimr),\n  require(testthat),\n  require(tidyverse),\n  require(usethis)\n)\n\ntidymodels::tidymodels_prefer(quiet = TRUE)\n\nold_theme &lt;-theme_set(\n  theme_minimal(base_size=9, \n                base_family = \"Helvetica\")\n)\nCodegc &lt;- options(ggplot2.discrete.colour=\"viridis\")\ngc &lt;- options(ggplot2.discrete.fill=\"viridis\")\ngc &lt;- options(ggplot2.continuous.fill=\"viridis\")\ngc &lt;- options(ggplot2.continuous.colour=\"viridis\")"
  },
  {
    "objectID": "labs/lab-progr.html#generics-and-s3-classes",
    "href": "labs/lab-progr.html#generics-and-s3-classes",
    "title": "R programming: generic programming, tidy evaluation",
    "section": "Generics and S3 classes",
    "text": "Generics and S3 classes\nOO in Advanced R Programming"
  },
  {
    "objectID": "labs/lab-progr.html#programming-with-dplyr-and-ggplot2",
    "href": "labs/lab-progr.html#programming-with-dplyr-and-ggplot2",
    "title": "R programming: generic programming, tidy evaluation",
    "section": "Programming with dplyr and ggplot2\n",
    "text": "Programming with dplyr and ggplot2\n\nWe aim at programming a function that takes as input a dataframe df, a column name col, and that, depending on the type of the column denoted by col, plots a histogram (for numerical column), a barplot (for factors), or raise an error of the column is neither categorical, nor numerical.\nThe function should return a ggplot object.\nHere is a first attempt.\n\nCodetb &lt;- tibble( \n  col_num = rnorm(100), \n  col_fac = as_factor(sample(letters, 100, replace = T)), \n  col_ts = Sys.time() + duration(sample(1:20, 100, replace=T),units=\"days\")\n) \n\ntb |&gt; \n  head()\n\n# A tibble: 6 × 3\n  col_num col_fac col_ts             \n    &lt;dbl&gt; &lt;fct&gt;   &lt;dttm&gt;             \n1 -0.0432 x       2024-09-05 09:01:05\n2 -0.871  i       2024-09-15 09:01:05\n3  0.587  h       2024-09-03 09:01:05\n4 -0.634  v       2024-09-10 09:01:05\n5 -0.628  x       2024-09-19 09:01:05\n6  0.881  m       2024-09-11 09:01:05\n\n\n\nCodegg_obj &lt;-  function(df, col){\n  \n  vct &lt;- df[[col]]\n  tp &lt;- class(vct)\n\n  if (tp != \"numeric\" & tp !=\"factor\") {\n    stop(paste0(col, \" is of wrong type!\"))\n  }\n\n  p &lt;- ggplot(df) + \n    aes(x=.data[[col]]) \n\n  if (tp==\"numeric\") {\n    p &lt;- p + geom_histogram()\n  } else {\n    p &lt;- p + geom_bar()\n  }\n\n  p  \n}\n\n\n\npass more optional arguments\navoid quoting the column name\n\n\nCodegg_obj_2 &lt;-  function(df, col, ...){\n\n  vct &lt;- pull(df, {{col}})\n  tp &lt;- class(vct)[1]\n\n  if (tp != \"numeric\" & tp !=\"factor\") {\n    stop(\"column is of wrong type!\")\n    return\n  }\n\n  p &lt;- ggplot(df) + \n    aes(x={{col}}) \n\n  if (tp==\"numeric\") {\n    p &lt;- p + geom_histogram(...)\n  } else {\n    p &lt;- p + geom_bar(...)\n  }\n\n  p  \n}"
  },
  {
    "objectID": "labs/lab-progr.html#inside-lm",
    "href": "labs/lab-progr.html#inside-lm",
    "title": "R programming: generic programming, tidy evaluation",
    "section": "Inside lm()\n",
    "text": "Inside lm()\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn classes like lm, prcomp, … we have a member called call. What does it represent? How is it constructed?\nRead the code of lm.\n\n\n&gt; lm \nfunction (formula, data, subset, weights, na.action, method = \"qr\", \n    model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, \n    contrasts = NULL, offset, ...) \n{\n    ret.x &lt;- x\n    ret.y &lt;- y\n    cl &lt;- match.call()\n    mf &lt;- match.call(expand.dots = FALSE)\n    m &lt;- match(c(\"formula\", \"data\", \"subset\", \"weights\", \"na.action\", \n        \"offset\"), names(mf), 0L)\n    mf &lt;- mf[c(1L, m)]\n    mf$drop.unused.levels &lt;- TRUE\n    mf[[1L]] &lt;- quote(stats::model.frame)\n    mf &lt;- eval(mf, parent.frame())\n    if (method == \"model.frame\") \n        return(mf)\n    else if (method != \"qr\") \n        warning(gettextf(\"method = '%s' is not supported. Using 'qr'\", \n            method), domain = NA)\n    mt &lt;- attr(mf, \"terms\")\n    y &lt;- model.response(mf, \"numeric\")\n    w &lt;- as.vector(model.weights(mf))\n    if (!is.null(w) && !is.numeric(w)) \n        stop(\"'weights' must be a numeric vector\")\n    offset &lt;- model.offset(mf)\n    mlm &lt;- is.matrix(y)\n    ny &lt;- if (mlm) \n        nrow(y)\n    else length(y)\n    if (!is.null(offset)) {\n        if (!mlm) \n            offset &lt;- as.vector(offset)\n        if (NROW(offset) != ny) \n            stop(gettextf(\"number of offsets is %d, should equal %d (number of observations)\", \n                NROW(offset), ny), domain = NA)\n    }\n    if (is.empty.model(mt)) {\n        x &lt;- NULL\n        z &lt;- list(coefficients = if (mlm) matrix(NA_real_, 0, \n            ncol(y)) else numeric(), \n                  residuals = y, \n                  fitted.values = 0 * y, \n                  weights = w, \n                  rank = 0L, \n                  df.residual = if (!is.null(w)) sum(w != 0) else ny\n              )\n        if (!is.null(offset)) {\n            z$fitted.values &lt;- offset\n            z$residuals &lt;- y - offset\n        }\n    }\n    else {\n        x &lt;- model.matrix(mt, mf, contrasts)\n        z &lt;- if (is.null(w)) \n            lm.fit(x, y, offset = offset, singular.ok = singular.ok, \n                ...)\n        else lm.wfit(x, y, w, offset = offset, singular.ok = singular.ok, \n            ...)\n    }\n    class(z) &lt;- c(if (mlm) \"mlm\", \"lm\")\n    z$na.action &lt;- attr(mf, \"na.action\")\n    z$offset &lt;- offset\n    z$contrasts &lt;- attr(x, \"contrasts\")\n    z$xlevels &lt;- .getXlevels(mt, mf)\n    z$call &lt;- cl\n    z$terms &lt;- mt\n    if (model) \n        z$model &lt;- mf\n    if (ret.x) \n        z$x &lt;- x\n    if (ret.y) \n        z$y &lt;- y\n    if (!qr) \n        z$qr &lt;- NULL\n    z\n}\n&lt;bytecode: 0x55564224e930&gt;\n&lt;environment: namespace:stats&gt;"
  },
  {
    "objectID": "labs/lab-progr.html#data-masking-and-environments",
    "href": "labs/lab-progr.html#data-masking-and-environments",
    "title": "R programming: generic programming, tidy evaluation",
    "section": "Data masking and environments",
    "text": "Data masking and environments\n\n\n\n\n\n\nQuestion"
  },
  {
    "objectID": "labs/lab-progr.html#tidy-evaluation",
    "href": "labs/lab-progr.html#tidy-evaluation",
    "title": "R programming: generic programming, tidy evaluation",
    "section": "Tidy evaluation",
    "text": "Tidy evaluation\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is quasi-quotation?\nKeep the rlang cheatsheet around.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the difference between an expression and a quosure\n\n\n\n\n\n\n\n\nQuestion"
  },
  {
    "objectID": "labs/lab-progr.html#references",
    "href": "labs/lab-progr.html#references",
    "title": "R programming: generic programming, tidy evaluation",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "labs/lab-test-miashs.html",
    "href": "labs/lab-test-miashs.html",
    "title": "Testing Bernoulli and Binomial parameters",
    "section": "",
    "text": "Codestopifnot(\n  require(patchwork),\n  require(glue),\n  require(here),\n  require(tidyverse),\n  require(plotly),\n  require(DT),\n  require(GGally),\n  require(ggforce),\n  require(ggfortify)\n)\n\ntidymodels::tidymodels_prefer(quiet = TRUE)\n\nold_theme &lt;-theme_set(theme_minimal(base_size=9, base_family = \"Helvetica\"))"
  },
  {
    "objectID": "labs/lab-test-miashs.html#hypothesis-testing",
    "href": "labs/lab-test-miashs.html#hypothesis-testing",
    "title": "Testing Bernoulli and Binomial parameters",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nDownload ‘Naissances totales par sexe’ from URL https://www.ined.fr/fichier/s_rubrique/168/t35.fr.xls from INED.\n\nCodepath_data &lt;- 'DATA'\nbirths_fr_path &lt;- here(path_data, 't35.fr.xls')\nbirths_fr_url &lt;- 'https://www.ined.fr/fichier/s_rubrique/168/t35.fr.xls'\n\nif (!file.exists(births_fr_path)) {\n  download.file(births_fr_url, births_fr_path, mode = \"wb\")\n}\n\n\n\nCodebirths_fr &lt;-  readxl::read_excel(births_fr_path, skip = 3)\n\nbirths_fr &lt;- births_fr[2:122, ] \n\n\nbirths_fr &lt;- births_fr |&gt; \n  rename(year= `Répartition par sexe et vie`,\n         n_livebirths = `Ensemble des nés vivants`,\n         n_live_boys = `Nés vivants - Garçons`,\n         n_stillbirths = `Ensemble des enfants sans vie`,\n         n_still_boys =`Enfants sans vie - Garçons`) |&gt; \n  select(year, starts_with('n_')) \n\n#births_fr &lt;- births_fr[1:122,]\n\nbirths_fr |&gt; \n  glimpse()\n\nRows: 121\nColumns: 5\n$ year          &lt;chr&gt; \"1901\", \"1902\", \"1903\", \"1904\", \"1905\", \"1906\", \"1907\", …\n$ n_livebirths  &lt;dbl&gt; 917075, 904434, 884498, 877091, 865604, 864745, 829632, …\n$ n_live_boys   &lt;dbl&gt; 468125, 462097, 451510, 447651, 442397, 441358, 424692, …\n$ n_stillbirths &lt;dbl&gt; 32410, 32000, 31076, 30673, 30108, 29671, 29208, 29834, …\n$ n_still_boys  &lt;chr&gt; \"18522\", \"18172\", \"17875\", \"17299\", \"17289\", \"16977\", \"1…\n\n\n\nNull Hypothesis\n\nThe probability of a live newborn baby being a boy is \\(p_0 =.5121244\\)\n\nAlternative Hypothesis\n\nThe probability of a live newborn baby being a boy is \\(p &gt; p_0=.5121244\\)\n\n\nData and modeling\nProbability of observed data if live newborn sex is distributed according to Bernoulli(\\(p\\))\nIf amongst \\(n\\) livebirths we observe \\(n_g\\) boys: \\[\\binom{n}{n_g} p^{n_g} (1-p)^{n-n_g}\\] Compute the Likelihood Ratio for alternative \\(p&lt;p_0\\)\n\\[\\left(\\frac{p(1-p_0)}{(1-p)p_0}\\right)^{n_g} \\times \\left(\\frac{1-p}{1-p_0}\\right)^n\\]\n\n\n\n\n\n\nThe Likelihood Ratio increases with respect to \\(n_g\\) for all values of \\(p &gt; p_0\\).\nComparing the likelihood ratio to a threshold amounts to compare \\(n_g\\) to a(nother) threshold.\nHere Likelihood Ratio testing is motivated by common sense and can be justified by Theory (Neyman-Pearson’s Lemma).\n\n\n\n\n\n\n\n\n\nDefinition: Error of the first kind (Type I error)\n\n\n\nThe error of the first kind occurs when the null hypothesis is true, but the test would reject it.\n\n\n\n\n\n\n\n\nDefinition: Error of the second kind (Type II error)\n\n\n\nThe error of the second kind occurs if the alternative hypothesis is true but the test is deciding in favor of the null-hypothesis.\n\n\nThe next lemma justifies our interest in Likelihood Ratio testing\n\n\n\n\n\n\nLemma (Neyman-Pearson, simplified)\n\n\n\nWhen testing a simple null hypothesis \\(H_0 : X \\sim P_0\\) against a simple alternative \\(H_1 : X \\sim P_1\\), if there exists a threshold \\(\\tau\\) such that the test \\(T\\) with critical region \\(\\{x : p_1(x) \\geq \\tau \\times p_0(x) \\}\\) has type I error probability equal to \\(\\alpha \\in (0,1)\\), then for any test \\(T'\\) \\[P_0\\{ T'(x) =1\\} \\leq \\alpha \\quad \\Rightarrow \\quad P_1\\{ T'(x) =1\\}\\leq P_1\\{ T(x) =1\\}\\]\n\n\n\n\n\n\n\n\nLevel (of significance)\n\n\n\nThe level of significance is defined as the fixed probability of wrong elimination of null hypothesis when in fact, it is true. The level of significance is stated to be the probability of type I error and is preset by the researcher."
  },
  {
    "objectID": "labs/lab-test-miashs.html#testing-a-bernoulli-parameter",
    "href": "labs/lab-test-miashs.html#testing-a-bernoulli-parameter",
    "title": "Testing Bernoulli and Binomial parameters",
    "section": "Testing a Bernoulli parameter",
    "text": "Testing a Bernoulli parameter\nWe think of the sex of newborns as a sequence of independent Bernoulli trials. Under the simplest model, all Bernoulli trials have the same “success” probability.In principle, we have no a priori knowledge of the “success” probability. We take \\(p_0\\) as the empirical frequency of livebirth of boys throughout the century.\n\nCodebirths_fr |&gt; \n  summarise(tot=sum(n_livebirths, na.rm = T), \n            tot_boys=sum(n_live_boys, na.rm = T), \n            msr=tot_boys/tot)\n\n# A tibble: 1 × 3\n       tot tot_boys   msr\n     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 92078262 47155839 0.512\n\n\nWe compute now for each year, the probability that a binomial random variable with size number of livebirths during the year and success probability \\(p_0\\), exceeds the number of livebirths of boys during that year, the result is denoted by pval.\n\nCodep_0 &lt;- 0.5121244\n\nbirths_fr &lt;- births_fr |&gt; \n  mutate(pval = pbinom(n_live_boys, size = n_livebirths, prob = p_0, lower.tail = F)) |&gt;\n  relocate(pval, .after = year) \n\n\nUnder our null hypothesis, pval is a random variable, and it is (almost) uniformly distributed over \\([0,1]\\). If we want a testing procedure with type I error \\(\\alpha\\), we can decide to reject the null hypothesis when pval (usually called the \\(p\\)-value) is less than \\(\\alpha\\).\nAgree on Type I error probability (\\(\\alpha\\)) equal to \\(.05\\).\n\nCodebirths_fr |&gt; \n  DT::datatable()  |&gt; \n  DT::formatSignif('pval', digits=3) |&gt; \n  DT::formatStyle(\n  'pval',\n  backgroundColor = DT::styleInterval(c(.05, 1), values = c('red', 'lightgreen', 'white'))\n)\n\n\n\n\n\nThroughout the \\(123\\) years in the sample we observe \\(25\\) p-values smaller than \\(5\\%\\). This is far more than what we expect."
  },
  {
    "objectID": "labs/lab-test-miashs.html#geissler-data-goodness-of-fit-testing.",
    "href": "labs/lab-test-miashs.html#geissler-data-goodness-of-fit-testing.",
    "title": "Testing Bernoulli and Binomial parameters",
    "section": "Geissler data, goodness of fit testing.",
    "text": "Geissler data, goodness of fit testing.\nFrom package vcdExtra\n\nGeissler (1889) published data on the distributions of boys and girls in families in Saxony, collected for the period 1876-1885. The Geissler data tabulates the family composition of 991,958 families by the number of boys and girls listed in the table supplied by Edwards\n\n\nCodeGeissler &lt;- vcdExtra::Geissler \nGeissler |&gt;  glimpse()\n\nRows: 90\nColumns: 4\n$ boys  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ girls &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 0, 1, 2, 3, 4, 5, 6, 7, 8…\n$ size  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9…\n$ Freq  &lt;int&gt; 108719, 42860, 17395, 7004, 2839, 1096, 436, 161, 66, 30, 8, 3, …\n\n\nWe isolate families of size \\(12\\).\n\nCodebig_families &lt;- Geissler |&gt; \n  filter(size==12) |&gt; \n  select(-size, -girls) \n\n\nThere are 6115 of them.\n\nCodebig_families |&gt; \n  pivot_wider(names_from = boys, values_from = Freq) |&gt; \n  knitr::kable()\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n3\n24\n104\n286\n670\n1033\n1343\n1112\n829\n478\n181\n45\n7\n\n\n\n\nAccording to our simple null hypothesis, the large families compositions should be distributed according to a binomial distribution with size \\(12\\) and success probability \\(p_0\\).\nWe can perform a goodness of fit test for this distribution. The Chi-square goodness of fit test comes to mind\n\nCodeexpected &lt;- dbinom(0:12, 12, p_0)\nobserved &lt;- big_families$Freq\n\nchisq.test(observed, p=expected) |&gt; \n  broom::tidy() |&gt; \n  knitr::kable()\n\nWarning in chisq.test(observed, p = expected): Chi-squared approximation may be\nincorrect\n\n\n\n\n\n\n\n\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n130.3315\n0\n12\nChi-squared test for given probabilities\n\n\n\n\nWe merge rare events as an attempt to avoid the warning.\n\nCodeexpected_collapsed &lt;- expected[2:12]\nexpected_collapsed[1] &lt;- expected_collapsed[1]+ expected[1]\nexpected_collapsed[11] &lt;- expected_collapsed[11]+ expected[13]\n\nobserved_collapsed &lt;- observed[2:12]\nobserved_collapsed[1] &lt;- observed_collapsed[1]+ observed[1]\nobserved_collapsed[11] &lt;- observed_collapsed[11]+ observed[13]\n\n\n\nCodechisq.test(observed_collapsed, p=expected_collapsed) |&gt; \n  broom::tidy() |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n124.9965\n0\n10\nChi-squared test for given probabilities\n\n\n\n\n\n\n\n\n\n\nWe are led to reject the null hypothesis for all reasonable type I error probabilities.\n\n\n\nLet us compare the empirical distribution and the theoretical distribution of large families compisitions\n\nCodep &lt;- big_families |&gt; \n  ggplot() + \n  aes(x= boys, y=Freq/sum(Freq)) +\n  geom_col(fill=\"white\", color=\"black\", alpha=.5) +\n  geom_col(aes(y=expected), fill=\"white\", color=\"blue\", alpha=.5) +\n  labs(\n    title=\"Geissler data, composition of 6115 families of size 12\",\n    subtitle=\"Black= empirical distribution. Blue=Theoretical distribution\"\n  )\n\np\n\n\n\n\n\n\n\nUsing logarithmic scale on the y axis emphasizes the overdispersion of the empirical distribution.\n\nCodep + scale_y_log10()"
  },
  {
    "objectID": "labs/lab-histo-density.html",
    "href": "labs/lab-histo-density.html",
    "title": "Univeariate analysis: Historgrams and Density plots",
    "section": "",
    "text": "Codeparams = list(\n  truc= \"Science des Données\",\n  year= 2023 ,\n  curriculum= \"L3 MIASHS\",\n  university= \"Université Paris Cité\",\n  homepage= \"https://stephane-v-boucheron.fr/courses/scidon\",\n  moodle= \"https://moodle.u-paris.fr/course/view.php?id=13227\",\n  path_data = './DATA',\n  country_code= '...',\n  country= '...',\n  datafile= '...'\n  )\n\nattach(params)\nCodestopifnot(\n  require(patchwork),\n  require(glue),\n  require(here),\n  require(tidyverse),\n  require(ggmosaic),\n  require(skimr),\n  require(plotly),\n  require(DT),\n  require(GGally),\n  require(ggforce),\n  require(ggfortify),\n  require(vcd)\n)\n\ntidymodels::tidymodels_prefer(quiet = TRUE)\n\nold_theme &lt;-theme_set(theme_minimal(base_size=9, base_family = \"Helvetica\"))"
  },
  {
    "objectID": "labs/lab-histo-density.html#density-estimation",
    "href": "labs/lab-histo-density.html#density-estimation",
    "title": "Univeariate analysis: Historgrams and Density plots",
    "section": "Density estimation",
    "text": "Density estimation\n\n\n\n\n\n\nHistogram\n\n\n\n\n\n\nA histogram is a piecewise constant density estimator.\n\n\n\n\n\n\nSliding window estimator\n\n\n\nLet \\(h&gt;0\\) be a bandwidth, let \\(x_1, \\ldots, x_n\\) be a sample, the sliding window density is defined by \\[\\widehat{f}_n(x) = \\sum_{i=1}^n \\frac{1}{2h}\\mathbb{I}_{[-1/2,1/2]}\\left(\\frac{x-x_i}{h}\\right)\\] ou \\[\n\\widehat{f}_n(x) = \\frac{1}{2h} \\left(F_n(x+h) -F_n(x-h) \\right)\n\\]\n\n\n\n\n\n\n\n\nKernel density estimator"
  },
  {
    "objectID": "labs/lab-histo-density.html#simulations",
    "href": "labs/lab-histo-density.html#simulations",
    "title": "Univeariate analysis: Historgrams and Density plots",
    "section": "Simulations",
    "text": "Simulations\n\n\n\n\n\n\nQuestion\n\n\n\nSimulate \\(N=10\\) samples of size \\(n=500\\) from a mixture of two Gaussian distributions \\(\\lambda \\mathcal{N}(0,1) + (1- \\lambda) \\mathcal{N}(\\mu, \\sigma^2)\\).\nHenceforth, \\(\\lambda\\) is the mixing parameter. \\(\\mathcal{N}(0,1)\\) is the standard Gaussian and \\(\\mathcal{N}(\\mu, \\sigma^2)\\) is the non-standard Gaussian component of our mixture distribution,\n\n\n\n\n\n\n\n\nMixture distributions\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPlot regular histograms for different sample replicates.\nTry different number of bins or binwidths.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nRepeat the above operations, but sample according the uniform distribution on \\([0,1]\\)\nbut choose the breaks so that the intervals all have the same probability under the sampling distribution.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAssume that you have chosen \\(B\\) bins.\n\nWhat is the distribution of the the number of sample points in a bin?\nWhat is the average number of points in a bin, what is its variance?\nProvide an upper bound on the expected maximum number of points in a bin.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAssume that you have chosen \\(B\\) bins.\nCompare the empirical distribution of the number of points in a bin with the theoretical distribution of the number of points in a bin."
  },
  {
    "objectID": "projects/fake-report.html",
    "href": "projects/fake-report.html",
    "title": "LAB: Fake report",
    "section": "",
    "text": "Motivation\n\n\nAdipiscing aenean massa facilisi varius gravida, lacinia nisi accumsan congue cubilia. Euismod ridiculus luctus nam hendrerit, elementum venenatis lacinia quis libero lobortis. Eros sem vitae mauris a vivamus eros imperdiet? Sagittis ligula nostra vulputate sed, egestas etiam habitant. Vestibulum placerat tempus pulvinar pretium nostra. Cursus placerat nec.\n\n\n\n\n200 countries, 50 years, 20 lines of code\n\n\nElit est et pulvinar ante venenatis dictumst; risus ultrices commodo. Arcu ullamcorper ad diam semper dis fermentum rutrum. Platea molestie habitasse quisque vestibulum inceptos fusce. Viverra sodales nisi sollicitudin, tempus pulvinar fusce phasellus magna sociosqu purus. Tempus fringilla nisl placerat aenean, diam tellus euismod. Nec scelerisque nisi accumsan primis mi; aenean malesuada nostra. Lacinia netus feugiat torquent molestie, purus elementum dictum duis ante natoque mollis. Morbi conubia facilisis sapien etiam, odio etiam nascetur fringilla, ultricies imperdiet quam sagittis rutrum blandit turpis auctor?\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453\n\n\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.8530\n\n\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.1007\n\n\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.1971\n\n\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.9811\n\n\nAfghanistan\nAsia\n1977\n38.438\n14880372\n786.1134\n\n\n\n\n\n\n\nAdipiscing vulputate consequat varius varius senectus consequat pretium habitant feugiat nascetur nam! Phasellus mattis proin litora.\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatic with plotly\n\n\nIpsum tempor nunc erat dapibus, facilisis donec etiam leo ut! Aliquet nunc auctor semper nisl purus nam tincidunt viverra praesent, donec risus maecenas? Sagittis vulputate ligula enim torquent.\n\n\n\n\n\n\n\n\n\n\nWizardry\n\n\nLorem conubia placerat blandit imperdiet eros? Et condimentum tristique curae arcu pellentesque nostra platea cum; morbi ultricies pulvinar. Eleifend risus varius conubia justo platea fermentum!\n\n\n\n\n\n\n\n\n\n\nAppendix\n\n\nSit platea dui blandit vehicula: turpis molestie mauris. Dictum pellentesque dictum tortor fermentum convallis at, hac – urna sociis rutrum. Penatibus luctus feugiat tempus fames fames aliquet integer facilisi euismod ad himenaeos. Massa semper semper pellentesque parturient eu class pulvinar, ad rutrum erat vulputate odio augue. Mi cras interdum.\n\n\n\nneat_color_scale &lt;-\n      c(\"Africa\" = \"#01d4e5\",\n        \"Americas\" = \"#7dea01\" ,\n        \"Asia\" = \"#fc5173\",\n        \"Europe\" = \"#fde803\",\n        \"Oceania\" = \"#536227\")\na_year &lt;- sample(gapminder$year, 1)\n\np &lt;- gapminder |&gt;\n    filter(year==a_year) |&gt;\n    ggplot() +\n    aes(x = gdpPercap) +\n    aes(y = lifeExp) +\n    aes(size = pop) +\n    aes(text = country) +                   #\n    aes(fill = continent) +\n    aes(color= continent) +\n    aes(frame = year) +                     #\n    geom_point(alpha=.5) +\n    scale_x_log10() +\n    scale_size_area(max_size = 15,\n                    labels= scales::label_number(scale=1/1e6,\n                                                suffix=\" M\")) +\n    scale_fill_manual(values = neat_color_scale) +\n    labs(title= glue(\"Gapminder  {a_year}\"),\n        x = \"Yearly Income per Capita\",\n        y = \"Life Expectancy\",\n        caption=\"From sick  and poor (bottom left) to healthy and rich (top right)\")\n\np \n\n(p + theme(legend.position = \"none\")) |&gt; \n    plotly::ggplotly(height = 500, width=750)\n\n(p %+% \n    gapminder + \n    theme(legend.position = \"none\") +\n    ggtitle(\"Gapminder\")) |&gt; \n    plotly::ggplotly(height = 500, width=750)"
  },
  {
    "objectID": "labs/lab-bivariate.html#setup",
    "href": "labs/lab-bivariate.html#setup",
    "title": "Bivariate analysis",
    "section": "Setup",
    "text": "Setup\n\nCodestopifnot(\n  require(tidyverse), \n  require(glue),\n  require(magrittr),\n  require(lobstr),\n  require(arrow),\n  require(ggforce),\n  require(vcd),\n  require(ggmosaic),\n  require(httr),\n  require(cowplot),\n  require(patchwork)\n)\n\n\nBivariate techniques depend on the types of columns we are facing.\nFor numerical/numerical samples\n\nScatter plots\nSmoothed lineplots (for example linear regression)\n2-dimensional density plots\n\nFor categorical/categorical samples : mosaicplots and variants\nFor numerical/categorical samples\n\nBoxplots per group\nHistograms per group\nDensity plots per group\nQuantile-Quantile plots"
  },
  {
    "objectID": "labs/lab-bivariate.html#chi-square-independenceassociation-test",
    "href": "labs/lab-bivariate.html#chi-square-independenceassociation-test",
    "title": "Bivariate analysis",
    "section": "Chi-square independence/association test",
    "text": "Chi-square independence/association test"
  },
  {
    "objectID": "labs/lab-bivariate.html#grouped-boxplots",
    "href": "labs/lab-bivariate.html#grouped-boxplots",
    "title": "Bivariate analysis",
    "section": "Grouped boxplots",
    "text": "Grouped boxplots\nPlot boxplots of AGE according to NIV_ETUDES\nDraw density plots of AGE, facet by NIV_ETUDES and SEXE\nCollapse rare levels of NIV_ETUDES and replay."
  },
  {
    "objectID": "labs/lab-hclust.html#setup",
    "href": "labs/lab-hclust.html#setup",
    "title": "Clustering: hierarchical",
    "section": "Setup",
    "text": "Setup\n\nCodestopifnot(\n  require(DT),\n  require(skimr),\n  require(GGally),\n  require(patchwork),\n  require(ggforce),\n  require(glue),\n  require(ggfortify),\n  require(ggvoronoi),\n  require(magrittr),\n  require(broom),\n  require(ggdendro),\n  require(dendextend),\n  require(plotly),\n  require(tidyverse)\n)\n\ntidymodels::tidymodels_prefer(quiet = TRUE)\n\nold_theme &lt;-theme_set(\n  theme_minimal(base_size=9, \n                base_family = \"Helvetica\")\n)\n\n\n\nCodeknitr::opts_chunk$set(\n  message = FALSE,\n  warning = FALSE,\n  comment=NA,\n  prompt=FALSE,\n  cache=FALSE,\n  echo=TRUE,\n  results='asis'\n)\n\n\n\nCodegc &lt;- options(ggplot2.discrete.colour=\"viridis\")\ngc &lt;- options(ggplot2.discrete.fill=\"viridis\")\ngc &lt;- options(ggplot2.continuous.fill=\"viridis\")\ngc &lt;- options(ggplot2.continuous.colour=\"viridis\")\n\n\n\n\n\n\n\n\nObjectives"
  },
  {
    "objectID": "labs/lab-univariate-numeric.html#setup",
    "href": "labs/lab-univariate-numeric.html#setup",
    "title": "Univariate analysis I",
    "section": "Setup",
    "text": "Setup\nIf the required packages have not (yet) been installed, install them.\n\nCodestopifnot(\n  require(skimr),   # Univariate summaries from the shelf\n  require(lobstr),  # R introspection\n  require(rlang),   # R introspection\n  require(glue),    # Like formatted strings\n  require(gssr),\n  require(gssrdoc),\n  require(fs),      # File manipulation\n  require(patchwork), # piecing ggplots together\n  require(tidyverse) # What else?\n)"
  },
  {
    "objectID": "labs/lab-univariate-numeric.html#boxplots",
    "href": "labs/lab-univariate-numeric.html#boxplots",
    "title": "Univariate analysis I",
    "section": "Boxplots",
    "text": "Boxplots\n\n\n\n\n\n\nQuestion\n\n\n\n\nBuild a boxplot for age.\nEquip the plot with a title, a subtitle, a caption\nAnnotate the boxplot with summary statistics.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nBuild a boxplot of age distribution according to sex.\nWhat is the impact of argument varwidth=T?\nWhat is the impact of argument notch=T?\nWhat is the difference between stat_boxplot() and geom_boxplot()?\nHow would you get rid of the useless ticks on the x-axis?"
  },
  {
    "objectID": "labs/lab-univariate-numeric.html#histograms",
    "href": "labs/lab-univariate-numeric.html#histograms",
    "title": "Univariate analysis I",
    "section": "Histograms",
    "text": "Histograms\n\n\n\n\n\n\nQuestion\n\n\n\n\nPlot a histogram of the age distribution\nFacet by sex\n\nDraw the age distribution histograms for each sex on the same plot\nFacet by sex and year\n\nBuild an animated histogram plot where frame is determined by year\n\n\n\n\nHistograms are used to sketch possibly (absolutely) continuous distributions by using piecewise constant approximations of density functions. Histograms can also be viewed as column plots for binned data (that is discretizations of “continuous” data).\n\n\n\n\n\n\nQuestion\n\n\n\n\nDefine breaks for age data\n\nregular breaks with age ranges of length 5\nirregular breaks [18-25[, [25, 35[, [35,50[, [50, 65[, [65,+∞[\n\n\n\nBin age according to defined breaks using cut()\n\nPlot the binned data using geom_bar() or geom_col()\n\n\n\n\nDemographers use population pyramids to sketch the age distribution in a population. Population pyramids are special facetted histograms or barplots.\n\n\n\n\n\n\nQuestion\n\n\n\n\nPlot an age-sex pyramid for the gss sample.\nAnimate with respect to year"
  },
  {
    "objectID": "labs/lab-univariate-numeric.html#density-plots",
    "href": "labs/lab-univariate-numeric.html#density-plots",
    "title": "Univariate analysis I",
    "section": "Density plots",
    "text": "Density plots\nHistograms deliver piecewise constant estimations/approximations of a population density. If we suspect the population density to be smooth, it is sensible to try to build smooth estimates/aproximations of the population density. This is the purpose of density estimates.\n\n\n\n\n\n\nQuestion\n\n\n\n\nDraw density plots for age distribution\nUse different bandwidths\nUse different kernels\nFacet by sex\n\nFacet by sex and year\n\nOverlay histograms and density plots (in geom_histogram() use aes(y=after_stat(density)))\n\n\n\n\n\n\n\n\n\nUse stat_density()\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nBuild violine plots for age distribution (use geom_violine())."
  },
  {
    "objectID": "labs/lab-univariate-numeric.html#cumulative-distribution-functions",
    "href": "labs/lab-univariate-numeric.html#cumulative-distribution-functions",
    "title": "Univariate analysis I",
    "section": "Cumulative Distribution Functions",
    "text": "Cumulative Distribution Functions\nNot all probability distributions have densities, but all are characterized by their Cumulative Distribution Functions (CDFs). Each sample defines an Empirical Cumulative Distribution Function (ECDF).\n\n\n\n\n\n\nQuestion\n\n\n\n\nPlot the age ECDF using stat_ecdf()\n\nFacet by sex, then by year and sex\n\nUse base R ecdf() and stat_function() to draw the same plot.\n\n\n\n\n\n\n\n\n\nQuestion \n\n\n\n\nCompare the age distributions for women and men using the Kolmogorov-Smirnov statistic (ks.test())\nHow is the Kolmogorov-Smirnov statistic computed?"
  },
  {
    "objectID": "labs/lab-univariate-numeric.html#quantile-plots",
    "href": "labs/lab-univariate-numeric.html#quantile-plots",
    "title": "Univariate analysis I",
    "section": "Quantile plots",
    "text": "Quantile plots\nThe quantile function of a probability distribution is the (generalized, left-continuous) inverse of its CDF. Quantile functions are useful devices in EDA and random generation.\n\n\n\n\n\n\nQuestion\n\n\n\n\nPlot the quantile function of the age empirical distribution\nPlot the quantile functions of the age empirical distributions for men and women\nDesign a function that takes as input a univariate numerical sample and returns the quantile function (in the same way as ecdf() does)\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nDraw a quantile-quantile plot to compare age distribution for women and men with base R qqplot()\n\n\n Draw a quantile-quantile plot to compare age distribution for women and men using ggplot2."
  },
  {
    "objectID": "labs/lab-lt.html",
    "href": "labs/lab-lt.html",
    "title": "Life tables, EDA, Mortality quotients",
    "section": "",
    "text": "Codestopifnot(\n  require(patchwork),\n  require(glue),\n  require(here),\n  require(tidyverse),\n  require(plotly),\n  require(DT),\n  require(GGally),\n  require(ggforce),\n  require(ggfortify)\n)\ntidymodels::tidymodels_prefer(quiet = TRUE)\n\nold_theme &lt;-theme_set(theme_minimal(base_size=9, base_family = \"Helvetica\"))"
  },
  {
    "objectID": "labs/lab-lt.html#data-sources",
    "href": "labs/lab-lt.html#data-sources",
    "title": "Life tables, EDA, Mortality quotients",
    "section": "Data sources",
    "text": "Data sources\nLife tables have been downloaded from https://www.mortality.org.\nWe investigate life tables describing countries from Western Europe (France, Great Britain –actually England and Wales–, Italy, the Netherlands, Spain, and Sweden) and the United States.\nLife tables used here have been doctored and merged so as to simplify discussion.\nWe will use the next lookup tables to recode some factors.\n\nCodecountry_code &lt;- list(fr_t='FRATNP',\n                     fr_c='FRACNP',\n                     be='BEL',\n                     gb_t='GBRTENW',\n                     gb_c='GBRCENW',\n                     nl='NLD',\n                     it='ITA',\n                     swe='SWE',\n                     sp='ESP',\n                     us='USA')\n\ncountries &lt;- c('fr_t', 'gb_t', 'nl', 'it', 'sp', 'swe', 'us')\n\ncountry_names &lt;- list(fr_t='France',     # total population\n                     fr_c='France',      # civilian population\n                     be='Belgium',\n                     gb_t='England & Wales',    # total population\n                     gb_c='England & Wales',    # civilian population\n                     nl='Netherlands',\n                     it='Italy',\n                     swe='Sweden',\n                     sp='Spain',\n                     us='USA')\n\ngender_names &lt;- list('b'='Both',\n                     'f'='Female',\n                     'm'='Male')\n\n\n\nCodedatafile &lt;- 'full_life_table.Rds'\nfpath &lt;- stringr::str_c(\"../DATA/\", datafile) # here::here('DATA', datafile)   # check getwd() if problem \n\nif (! file.exists(fpath)) {\n  download.file(\"https://stephane-v-boucheron.fr/data/full_life_table.Rds\", \n                fpath,\n                mode=\"wb\")\n}\n\nlife_table &lt;- readr::read_rds(fpath)\n\n\n\nCodelife_table &lt;- life_table |&gt;\n  mutate(Country = as_factor(Country)) |&gt;\n  mutate(Country = fct_relevel(Country, \"Spain\", \"Italy\", \"France\", \"England & Wales\", \"Netherlands\", \"Sweden\", \"USA\")) |&gt;\n  mutate(Gender = as_factor(Gender)) \n\nlife_table &lt;- life_table |&gt;\n  mutate(Area = fct_collapse(Country, \n                        SE = c(\"Spain\", \"Italy\", \"France\"), \n                        NE = c(\"England & Wales\", \"Netherlands\", \"Sweden\"), \n                        USA=\"USA\")) \n\n\nDocument Tables de mortalité françaises pour les XIXe et XXe siècles et projections pour le XXIe siècle contains detailed information on the construction of Life Tables for France.\nTwo kinds of Life Tables can be distinguished: Table du moment which contain for each calendar year, the mortality risks at different ages for that very year; and Tables de génération which contain for a given birthyear, the mortality risks at which an individual born during that year has been exposed.\nThe life tables investigated in this lab are Table du moment. According to the document by Vallin and Meslé, building the life tables required decisions and doctoring.\nHave a look at Lexis diagram.\nDefinitions can be obtained from www.lifeexpectancy.org. We translate it into mathematical (rather than demographic) language. Recall that the quantities define a probability distribution over \\(\\mathbb{N}\\). This probability distribution is a construction that reflects the health situation in a population at a given time (year). This probability distribution does not describe the sequence of sanitary situations experienced by a cohort (people born during a specific year).\n\nOne works with a period, or current, life table (table du moment). This summarizes the mortality experience of persons across all ages in a short period, typically one year or three years. More precisely, the death probabilities \\(q(x)\\) for every age \\(x\\) are computed for that short period, often using census information gathered at regular intervals. These \\(q(x)\\)’s are then applied to a hypothetical cohort of \\(100 000\\) people over their life span to produce a life table.\n\n\nCodelife_table |&gt; \n  filter(Country=='France', Year== 2010, Gender=='Female', Age &lt; 10 | Age &gt; 80 & Age &lt;90) |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nAge\nmx\nqx\nax\nlx\ndx\nLx\nTx\nex\nCountry\nGender\nArea\n\n\n\n2010\n0\n0.00325\n0.00324\n0.14\n100000\n324\n99722\n8465207\n84.65\nFrance\nFemale\nSE\n\n\n2010\n1\n0.00032\n0.00032\n0.50\n99676\n32\n99660\n8365484\n83.93\nFrance\nFemale\nSE\n\n\n2010\n2\n0.00015\n0.00015\n0.50\n99645\n15\n99637\n8265824\n82.95\nFrance\nFemale\nSE\n\n\n2010\n3\n0.00011\n0.00011\n0.50\n99630\n11\n99624\n8166187\n81.97\nFrance\nFemale\nSE\n\n\n2010\n4\n0.00008\n0.00008\n0.50\n99619\n8\n99615\n8066563\n80.97\nFrance\nFemale\nSE\n\n\n2010\n5\n0.00005\n0.00005\n0.50\n99611\n5\n99608\n7966948\n79.98\nFrance\nFemale\nSE\n\n\n2010\n6\n0.00008\n0.00008\n0.50\n99606\n8\n99602\n7867339\n78.98\nFrance\nFemale\nSE\n\n\n2010\n7\n0.00008\n0.00008\n0.50\n99598\n8\n99594\n7767737\n77.99\nFrance\nFemale\nSE\n\n\n2010\n8\n0.00008\n0.00008\n0.50\n99590\n8\n99586\n7668143\n77.00\nFrance\nFemale\nSE\n\n\n2010\n9\n0.00007\n0.00007\n0.50\n99582\n7\n99578\n7568557\n76.00\nFrance\nFemale\nSE\n\n\n2010\n81\n0.03516\n0.03455\n0.50\n73367\n2535\n72099\n727802\n9.92\nFrance\nFemale\nSE\n\n\n2010\n82\n0.04059\n0.03978\n0.50\n70832\n2818\n69423\n655702\n9.26\nFrance\nFemale\nSE\n\n\n2010\n83\n0.04754\n0.04644\n0.50\n68014\n3158\n66435\n586280\n8.62\nFrance\nFemale\nSE\n\n\n2010\n84\n0.05536\n0.05386\n0.50\n64856\n3493\n63109\n519845\n8.02\nFrance\nFemale\nSE\n\n\n2010\n85\n0.06295\n0.06103\n0.50\n61362\n3745\n59490\n456736\n7.44\nFrance\nFemale\nSE\n\n\n2010\n86\n0.07246\n0.06993\n0.50\n57617\n4029\n55603\n397246\n6.89\nFrance\nFemale\nSE\n\n\n2010\n87\n0.08256\n0.07929\n0.50\n53588\n4249\n51464\n341643\n6.38\nFrance\nFemale\nSE\n\n\n2010\n88\n0.09660\n0.09215\n0.50\n49339\n4547\n47066\n290180\n5.88\nFrance\nFemale\nSE\n\n\n2010\n89\n0.11088\n0.10505\n0.50\n44792\n4706\n42440\n243114\n5.43\nFrance\nFemale\nSE\n\n\n\n\n\n\n\n\n\n\n\nCheck on http://www.mortality.org the meaning of the different columns.\n\n\n\nIn the sequel, we denote by \\(F_{t}\\) the cumulative distribution function for year \\(t\\). \\(F_t(x)\\) represents the probability of dying at age not larger than \\(x\\).\nWe agree on \\(\\overline{F}_t = 1 - F_t\\) and \\(F_t(-1)=0\\).\nThe life tables are highly redundant. Provided we get the right conventions we can derive almost all columns from column qx.\n\nCodelife_table |&gt; \n  filter( Year&gt;=1948) |&gt; \n  group_by(Country, Year, Gender) |&gt; \n  summarise(m1 =max(abs(lx -dx -lead(lx)), na.rm = T), \n            m2 =max(abs(lx * qx -dx), na.rm=T),\n            m3 =max(abs(Lx -lx * (1 + qx * (ax-1))), na.rm=T),\n            m4 =max(abs(1-exp(-mx)-qx), na.rm=T)) |&gt; \n  select(Year, Country, Gender, m1, m2, m3, m4) |&gt;  \n  ungroup() |&gt; \n  group_by(Country, Gender) |&gt; \n  slice_max(order_by = desc(m4), n = 1)\n\n`summarise()` has grouped output by 'Country', 'Year'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 21 × 7\n# Groups:   Country, Gender [21]\n    Year Country         Gender    m1    m2    m3      m4\n   &lt;int&gt; &lt;fct&gt;           &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1  1948 Spain           Both       1 0.874 2.20  0.00838\n 2  1948 Spain           Female     1 0.789 1.56  0.00816\n 3  1952 Spain           Male       1 0.802 5.5   0.0119 \n 4  2004 Italy           Both       1 0.836 0.968 0.0150 \n 5  2004 Italy           Female     1 0.875 1.03  0.0149 \n 6  1984 Italy           Male       1 0.774 5.56  0.0146 \n 7  2007 France          Both       1 0.887 0.976 0.0152 \n 8  2007 France          Female     1 0.890 0.980 0.0151 \n 9  1979 France          Male       1 0.764 4.97  0.0161 \n10  1992 England & Wales Both       1 0.898 2.42  0.0135 \n# ℹ 11 more rows\n\n\n\nqx\n\n(age-specific) risk of death at age \\(x\\), or mortality quotient at given age \\(x\\) for given year \\(t\\): \\(q_{t,x} = \\frac{\\overline{F}_t(x) - \\overline{F}_t(x+1)}{\\overline{F}_t(x)}\\).\nFor each year, each age, \\(q_{t,x}\\) is determined by data from that year.\n\n\nWe also have \\[\\overline{F}_{t}(x+1) = \\overline{F}_{t}(x) \\times (1-q_{t,x+1})\\, .\\]\n\nmx\n\ncentral death rate at age \\(x\\) during year \\(t\\). This is connected with \\(q_{t,x}\\) by \\[m_{t,x} = -\\log(1- q_{t,x}) \\,, \\] or equivalently \\(q_{t,x} = 1 - \\exp(-m_{t,x})\\).\n\nlx\n\nthe so-called survival function: the scaled proportion of persons alive at age \\(x\\). These values are computed recursively from the \\(q_{t,x}\\) values using the formula \\[l_t(x+1) = l_t(x) \\times (1-q_{t,x}) \\, ,\\] with \\(l_{t,0}\\), the “radix” of the table, arbitrarily set to \\(100000\\). Function \\(l_{t,\\cdot}\\) and \\(\\overline{F}_t\\) are connected by \\[l_{t,x + 1} = l_{t,0} \\times \\overline{F}_t(x)\\,.\\] Note that in Probability theory, \\(\\overline{F}\\) is also called the survival or tail function.\n\ndx\n\n\\(d_{t,x} = q_{t,x} \\times l_{t,x}\\)\n\nTx\n\nTotal number of person-years lived by the cohort from age \\(x\\) to \\(x+1\\). This is the sum of the years lived by the \\(l_{t, x+1}\\) persons who survive the interval, and the \\(d_{t,x}\\) persons who die during the interval. The former contribute exactly \\(1\\) year each, while the latter contribute, on average, approximately half a year, so that \\(L_{t,x} = l_{t,x+1} + 0.5 \\times d_{t,x}\\). This approximation assumes that deaths occur, on average, half way in the age interval x to x+1. Such is satisfactory except at age 0 and the oldest age, where other approximations are often used; We will stick to a simplified vision \\(L_{t,x}= l_{t,x+1}\\)\n\n\nex:\n\nResidual Life Expectancy at age \\(x\\) and year \\(t\\)\n\n\nSee: Demography: Measuring and Modeling Population Processes by SH Preston, P Heuveline, and M Guillot. Blackwell. Oxford. 2001.\n\nChapter 2, on Age-specific rates and Probabilities. The comparison between Sweden and Kazakhstan illustrates the distinction between crude death rates and age-specific death rates, as well as the dependence of crude death rates on the age structure/distribution of the population. Moreover the Sweeden/Kazakhstan comparison offers a clear-cut example of the Yule-Simpson paradox.\n\nChapter contains an important discussion of age standardization for cross country comparisons, why it matters, why it is difficult and remains a matter of taste. The definitions of Life Expectancy at Birth, or Residual Life Expectancies are example of age standardizations."
  },
  {
    "objectID": "labs/lab-lt.html#western-countries-in-1948",
    "href": "labs/lab-lt.html#western-countries-in-1948",
    "title": "Life tables, EDA, Mortality quotients",
    "section": "Western countries in 1948",
    "text": "Western countries in 1948\nSeveral pictures share a common canvas: we plot mortality quotientss against ages using a logarithmic scale on the \\(y\\) axis. Countries are identified by aesthetics (shape, color, linetypes). Abiding to the DRY principle, we define a prototype ggplot (alternatively plotly) object. The prototype will be fed with different datasets and decorated and arranged for the different figures.\n\nCodedummy_data &lt;- dplyr::filter(life_table, FALSE)\n\nproto_plot &lt;- ggplot(dummy_data,\n                     aes(x=Age,\n                         y=qx,\n                         col=Area,\n                         linetype=Country,\n                         shape=Country)) +\n              scale_y_log10() +\n              scale_x_continuous(breaks = c(seq(0, 100, 10), 109)) +\n              ylab(\"Mortality quotients\") +\n              labs(linetype=\"Country\") +\n              theme_bw()\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPlot qx of all Countries at all ages for years 1948 and 2013.\n\n\n\n\n\n\n\n\n\nCodeproto_plt2 &lt;-\n  ggplot() +\n  aes(x=Age, y=qx, colour=Area, frame=Year, linetype=Country) +\n  geom_point(size=.1) +\n  geom_line(size=.1) +\n  scale_y_log10() +\n  labs(linetype=c(\"Country\")) +\n  scale_x_continuous(breaks = c(seq(0, 100, 10), 109)) +\n  xlab(\"Age\") +\n  ylab(\"mortality quotients\") +\n  facet_grid(cols=vars(Gender))\n\nwith(params,\n(proto_plt2 %+%\n  (life_table |&gt; filter(between(Year, year_p, year_e), Gender != 'Both', Age &lt; 90))  +\n  ggtitle(\"mortality quotients 1948-2013: Europe catches up\"))) |&gt;\n  plotly::ggplotly()\n\n\n\n\n\n\n\n\n\n\n\nThe animated plot allows to spot more details. It is useful to use color so as to distinguish threee areas: USA; Northern Europe (NE) comprising England and Wales, the Netherlands, and Sweden; Southern Europe (SE) comprising Spain, Italy, and France. In 1948, NE and the USA exhibit comparable central death reates at all ages for the two genders, the USA looking like a more dangerous place for young adults. Spain lags behind, Italy and Frane showing up at intermediate positions.\nBy year 1962, SE has almost caught up the USA. Italy and Spain still have higher infant mortality while mortality quotients in the USA and France are almost identical at all ages for both genders. mortality quotients attain a minimum around 10-12 for both genders. In Spain the minium central death rate has been divided by almost ten between 1948 and 1962.\nIf we dig further we observe that the shape of the male mortality quotients curve changes over time. In 1962, in the USA and France, mortality quotients exhibit a sharp increase between years 12 and 18, then remain almost constant between 20 and 30 and afterwards increase again. This pattern shows up in other countries but in a less spectacular way.\nTwenty years afterwards, during years 1980-1985, death rates at age 0 have decreased at around \\(1\\%\\) in all countries while it was \\(7\\%\\) in Spain in 1948. The male central death curve exhibits a plateau between ages 20 and 30. mortality quotients at this age look higher in France and the USA.\nBy year 2000, France is back amongst European countries (at least with respect to mortality quotients). Young adult mortality rates are higher in the USA than in Europe. This phenomenon became more pregnant during the last decade.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPlot ratios between mortality quotients (qx) in European countries and mortality quotients in the USA in 1948.\n\n\n\nCodesimplified_life_table &lt;- with(params, \n                              life_table |&gt;\n  dplyr::filter(between(Year, year_p, year_e), Age&lt;90, Gender!=\"Both\") |&gt;\n  dplyr::select(Age, Year, Country, qx, Gender, Area))\n\neur_table &lt;- simplified_life_table |&gt;\n  dplyr::filter(Country!='USA')\n\nus_table &lt;- simplified_life_table |&gt;\n  dplyr::filter(Country=='USA') |&gt;\n  dplyr::select(-Area, -Country)\n\neur_us_table &lt;-  eur_table |&gt;\n  dplyr::inner_join(us_table, by=c('Age', 'Year', 'Gender')) |&gt;\n  dplyr::mutate(Ratio=qx.x/qx.y)\n\n\n\nCodewith(params,\n(eur_us_table  |&gt;\n  ggplot(aes(x=Age,\n             y=Ratio,\n             col=Area,\n             frame=Year,\n             linetype=Country)) +\n  scale_y_log10() +\n  scale_x_continuous(breaks = c(seq(0, 100, 10), 109)) +\n  geom_point(size=.1) +\n  geom_smooth(method=\"loess\", se=FALSE, span=.1, size=.1) +\n  ylab(\"Ratio of mortality quotients with respect to US\") +\n  labs(linetype=\"Country\", color=\"Area\") +\n  ggtitle(label = stringr::str_c(\"European countries with respect to US,\", year_p,'-', year_e, sep = \" \"), subtitle = \"Sweden consistently ahead\") +\n  facet_grid(rows = vars(Gender))) |&gt;\n  ggplotly()\n)\n\n\n\n\n\n\n\n\nThis animation reveals less than the preceding one since we just have ratios with respect to the USA. But the patterns followed by European societies emerge in a more transparent way. The divide between northern and southern Europe at the onset of the period is even more visible. The ratios are important across the continent: there is a factor of 10 between spanish and swedish infant mortality rates. But the ratios at ages 50 and above tend to be similar. By the early 60s, the gap between southern and northern Europe has shrinked. By now, the ratios between mortality quotients tend to be within a factor of 2 across all ages, and even less at ages 50 and above."
  },
  {
    "objectID": "labs/lab-lt.html#death-rates-evolution-since-ww-ii",
    "href": "labs/lab-lt.html#death-rates-evolution-since-ww-ii",
    "title": "Life tables, EDA, Mortality quotients",
    "section": "Death rates evolution since WW II",
    "text": "Death rates evolution since WW II\n\n\n\n\n\n\nQuestion\n\n\n\nPlot mortality quotients (column qx) for both genders as a function of Age for years 1946, 1956, ... up to 2016 .\n\n\n\nCodepost_ww_II &lt;- with(params, seq(year_p, year_e, 10))\n\np &lt;- life_table |&gt;\n  filter(FALSE) |&gt;\n  ggplot(aes(x=Age,\n             y=qx,\n             col=forcats::as_factor(Year),\n             linetype=forcats::as_factor(Year))) +\n  geom_smooth(se=FALSE, method=\"loess\", span= .1, size=.2) +\n  labs(colour=\"Year\", linetype=\"Year\")   +\n  scale_y_log10() +\n  facet_grid(rows=vars(Country), cols=vars(Gender))\n\n\n\nCode(p  %+%\n  (life_table |&gt; dplyr::filter(Year %in% post_ww_II, Gender!=\"Both\",\n                                Age &lt; 90,\n                                Country %in% c('Spain', 'USA'))) +\n  ggtitle(stringr::str_c(\"Mortality quotient per Age\", sep=\", \"),\n          subtitle = \"Post WW II\")) |&gt;\n  ggplotly()\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a function ratio_mortality_rates with signature function(df, reference_year=1946, target_years=seq(1946, 2016, 10)) that takes as input:\n\na dataframe with the same schema as life_table,\na reference year ref_year and\na sequence of years target_years\n\n\nand that returns a dataframe with schema:\n\n\nColumn Name\nColumn Type\n\n\n\nYear\ninteger\n\n\nAge\ninteger\n\n\nqx\ndouble\n\n\nqx.ref_year\ndouble\n\n\nCountry\nfactor\n\n\nGender\nfactor\n\n\n\nwhere (Country, Year, Age, Gender) serves as a primary key, mx denotes the central death rate at Age for Year and Gender in Country whereas qx_ref_year denotes mortality quatient at Age for argument reference_year in Country for Gender.\n\n\n\nCoderatio_mortality_rates &lt;- function(df,\n                                  reference_year=1946,\n                                  target_years=seq(1946, 2016, 10)){\n  dplyr::filter(df, Year %in% target_years, Age &lt;90) |&gt;\n  dplyr::select(\"Age\", \"Area\", \"Gender\", \"Country\", \"qx\", \"Year\") |&gt;\n  dplyr::inner_join(y=df[df$Year==reference_year,\n                         c(\"Age\", \"Gender\", \"Country\", \"qx\")],\n                    by=c(\"Age\", \"Gender\", \"Country\"))\n}\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDraw plots displaying the ratio \\(m_{x,t}/m_{x, 1946}\\) for ages \\(x \\in 1, \\ldots, 90\\) and year \\(t\\) for \\(t \\in 1946, \\ldots, 2016\\) where \\(m_{x,t}\\) is the central death rate at age \\(x\\) during year \\(t\\).\nHandle both genders and countries Spain, Italy, France, England & Wales, USA, Sweden, Netherlands.\n\n\nOne properly facetted plot is enough.\n\nCodedf_ratios &lt;- ratio_mortality_rates(filter(life_table,\n                                          Gender!=\"Both\"),\n                                   reference_year=1948,\n                                   target_years=seq(1948, 2013, 1))\n\n\n\nCodeq &lt;- df_ratios |&gt;\n  dplyr::filter(FALSE) |&gt;\n  ggplot(aes(x=Age,\n             y=qx.x/qx.y,\n             linetype=forcats::as_factor(Year),\n             col=forcats::as_factor(Year))) +\n  geom_smooth(method=\"loess\",\n              se= FALSE,\n              size =.2,\n              span= .1) +\n  scale_y_log10() +\n  ylab(\"Ratio of mortality rates, reference Year 1946\") +\n  labs(linetype=\"Year\", col=\"Year\") +\n  scale_colour_brewer()\n\n\n\nCodeqf &lt;- df_ratios |&gt;\n#  dplyr::filter(FALSE) |&gt;\n  ggplot(aes(x=Age,\n             y=qx.x/qx.y,\n             linetype=Country,\n             frame=Year,\n             col=Area)) +\n  geom_smooth(method=\"loess\",\n              se= FALSE,\n              size =.2,\n              span= .1) +\n  scale_y_log10() +\n  scale_x_continuous(breaks = c(seq(0, 100, 10), 109)) +\n  ylab(\"Ratio of mortality rates, reference Year 1946\") +\n  labs(linetype=\"Country\") +\n  facet_grid(rows=vars(Gender))\n\n\n\nCodeqf |&gt; ggplotly()\n\n\nComment. During the last seventy years, death rates decreased at all ages in all seven countries. This progress has not been uniform across ages, genders and countries. Across most countries, infant mortality dramatically improved during the first post-war decade while death rates at age 50 and above remained stable until the mid seventies."
  },
  {
    "objectID": "labs/lab-lt.html#trends",
    "href": "labs/lab-lt.html#trends",
    "title": "Life tables, EDA, Mortality quotients",
    "section": "Trends",
    "text": "Trends\nWe noticed that mortality quotients did not evolve in the same way across all ages: first, the decay has been much more significant at low ages; second, the decay of mortality quotients at old ages (above 60) mostly took place during the last four decades. It is worth digging separately at what happened for different parts of life.\n\n\n\n\n\n\nQuestion\n\n\n\nPlot mortality quotients at ages \\(0, 1, 5\\) as a function of time. Facet by Gender and Country\n\n\n\nCodeages &lt;- c(0, 1, 5)\n\np_children &lt;- filter(life_table, FALSE) |&gt;\n  ggplot(mapping=aes(x=Year, y=qx,\n                     linetype=forcats::as_factor(Age),\n                     shape=forcats::as_factor(Age),\n                     col=forcats::as_factor(Age))) +\n  geom_line(size=.2) +\n  labs(linetype=\"Age\", col=\"Age\", shape=\"Age\") +\n  scale_y_log10() +\n  scale_x_continuous(breaks=seq(1935,2010,5)) +\n  facet_grid(cols=vars(Gender), rows=vars(Country))\n\np_children %+%\n  filter(life_table,\n            Age %in% ages,\n            Gender != \"Both\",\n            Year %in% 1933:2013) +\n  ggtitle(\"Infant and child, mortality rate\",\n            subtitle = \"Hygiene, Vaccination, Antibiotics\")\n\n\nAll European countries achieved the same infant mortality rates after year 2000. The USA now lag behind.\nDuring years 1940-1945, in the Netherlands and France, gains obtained before 1940 were reversed. Year 1945 was particularly difficult in the Netherlands.\n\n\n\n\n\n\nQuestion\n\n\n\nPlot mortality quotients at ages \\(15, 20, 40, 60\\) as a function of time. Facet by Gender and Country"
  },
  {
    "objectID": "labs/lab-lt.html#life-expectancy",
    "href": "labs/lab-lt.html#life-expectancy",
    "title": "Life tables, EDA, Mortality quotients",
    "section": "Life expectancy",
    "text": "Life expectancy\nWrite a function that takes as input a vector of mortality quotients, as well as an age, and returns the residual life expectancy corresponding to the vector and the given age.\n\nWrite a function that takes as input a dataframe with the same schema as life_table and returns a data frame with columns Country, Gender, Year, Age defining a primary key and a column res_lex containing residual life expectancy corresponding to the pimary key.\n\nThe next window function suffices to compute life expectancy at birth. It computes the logarithm of survival probabilities for each Country, Year, Gender (partition) at each Age. Note that the expression mentions an aggregation function sum and that the correction of the result is ensured by a correct design of the frame argument.\nIn order to compute Residual Life Expectancies at all ages, instead of performing aggregation, we compute a second window function. For each Year, Country, Gender (defining the partition), for each Age, the Residual Life Expectancy is the sum of survival probabilities over the frame defined by the current Age and all ages above.\nDeparting from the official method for computing residual life expectancy, we use the simplified recursion \\[\ne_{t,x} = (1- q_{t,x}) \\times (1 + e_{t,x+1}) \\,.\n\\] That is, we assume that people dying between age \\(x\\) (included) and \\(x+1\\) (non-included) die exactly on their \\(x^{\\text{th}}\\) birthday. The official calculation assume that except at age \\(0\\) and great age, people die uniformly at random between age \\(x\\) and \\(x+1\\): \\[\ne_{t,x} = (1- q_{t,x}) \\times (1 + e_{t,x+1}) + \\frac{1}{2} \\times q_{t,x}\n\\]\n\n\n\n\n\n\nQuestion\n\n\n\nThis recursion suggests a more efficient to compute residual life expectancies at all ages.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nPlot residual life expectancy as a function of Year at ages \\(60\\) and \\(65\\), facet by Gender and Country.\n\n\nR4Data Science Tidy"
  },
  {
    "objectID": "labs/lab-lin-reg-2.html",
    "href": "labs/lab-lin-reg-2.html",
    "title": "Linear regression II",
    "section": "",
    "text": "Code\nstopifnot(\n  require(tidyverse),\n  require(patchwork),\n  require(httr),\n  require(glue),\n  require(broom)\n)\nold_theme &lt;- theme_set(theme_minimal())"
  },
  {
    "objectID": "labs/lab-lin-reg-2.html#build-the-second-diagnostic-plot-using-ggplot",
    "href": "labs/lab-lin-reg-2.html#build-the-second-diagnostic-plot-using-ggplot",
    "title": "Linear regression II",
    "section": "Build the second diagnostic plot using ggplot",
    "text": "Build the second diagnostic plot using ggplot"
  },
  {
    "objectID": "labs/lab-lin-reg-2.html#use-package-patchwork...-to-collect-your-four-diagnostic-plots",
    "href": "labs/lab-lin-reg-2.html#use-package-patchwork...-to-collect-your-four-diagnostic-plots",
    "title": "Linear regression II",
    "section": "Use package patchwork::... to collect your four diagnostic plots",
    "text": "Use package patchwork::... to collect your four diagnostic plots"
  },
  {
    "objectID": "labs/lab-lin-reg-2.html#plot-actual-values-against-fitted-values-for-sal_actuel",
    "href": "labs/lab-lin-reg-2.html#plot-actual-values-against-fitted-values-for-sal_actuel",
    "title": "Linear regression II",
    "section": "Plot actual values against fitted values for SAL_ACTUEL",
    "text": "Plot actual values against fitted values for SAL_ACTUEL"
  },
  {
    "objectID": "labs/lab-lin-reg-2.html#discuss-the-relevance-of-simple-linear-regression-for-analyzing-the-connection-between-sal_actuel-and-age",
    "href": "labs/lab-lin-reg-2.html#discuss-the-relevance-of-simple-linear-regression-for-analyzing-the-connection-between-sal_actuel-and-age",
    "title": "Linear regression II",
    "section": "Discuss the relevance of Simple Linear Regression for analyzing the connection between SAL_ACTUEL and AGE",
    "text": "Discuss the relevance of Simple Linear Regression for analyzing the connection between SAL_ACTUEL and AGE"
  },
  {
    "objectID": "labs/lab-lin-reg-2.html#compute-the-pearson-correlation-coefficient-for-every-pair-of-quantitative-variable-draw-corresponding-scatterplots.",
    "href": "labs/lab-lin-reg-2.html#compute-the-pearson-correlation-coefficient-for-every-pair-of-quantitative-variable-draw-corresponding-scatterplots.",
    "title": "Linear regression II",
    "section": "Compute the Pearson correlation coefficient for every pair of quantitative variable? Draw corresponding scatterplots.",
    "text": "Compute the Pearson correlation coefficient for every pair of quantitative variable? Draw corresponding scatterplots."
  },
  {
    "objectID": "labs/lab-lin-reg-2.html#randomly-select-450-rows-in-the-banque-dataframe.",
    "href": "labs/lab-lin-reg-2.html#randomly-select-450-rows-in-the-banque-dataframe.",
    "title": "Linear regression II",
    "section": "Randomly select \\(450\\) rows in the banque dataframe.",
    "text": "Randomly select \\(450\\) rows in the banque dataframe.\nFunction sample from base R is convenient. You may also enjoy slice_sample() from dplyr. Denote by trainset the vector of of selected indices. Bind the vector of left behind indices to variable testset. Functions match, setdiff or operator %in% may be useful.\n\nLinear fit of SAL_ACTUEL with respect to AGE, on the training set. Call the result lm_3.\nHow do you feel about such a linear fit? (Use diagnostic plots)\n\nInspecting points with high Cook’s distance\n\nUse lm_3 to predict the values of SAL_ACTUEL as an affine function of AGE on the testing set testset (broom::augment() with optional argument newdata may be useful). Compare the data frame with the one obtained from augment(lm_3).\n\n\nCompare training error and testing error\nAnalyse residuals (prediction errors) on the testing set. Compare with training set"
  },
  {
    "objectID": "labs/lab-test-miashs-2.html",
    "href": "labs/lab-test-miashs-2.html",
    "title": "Testing independence",
    "section": "",
    "text": "Codestopifnot(\n  require(patchwork),\n  require(glue),\n  require(here),\n  require(tidyverse),\n  require(vcd),\n  require(vcdExtra),\n  require(ggmosaic),\n  require(skimr),\n  require(plotly),\n  require(DT),\n  require(GGally),\n  require(ggforce),\n  require(ggfortify)\n)\n\ntidymodels::tidymodels_prefer(quiet = TRUE)\n\nold_theme &lt;-theme_set(theme_minimal(base_size=9, base_family = \"Helvetica\"))"
  },
  {
    "objectID": "labs/lab-test-miashs-2.html#confidence-intervals",
    "href": "labs/lab-test-miashs-2.html#confidence-intervals",
    "title": "Testing independence",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nWe start with Confidence Intervals in a simple Gaussian setting. We have \\(X_1, \\ldots, X_n \\sim_{i.i.d.} \\mathcal{N}(\\mu, \\sigma^2)\\) where \\(\\mu\\) and \\(\\sigma\\) are unknown (to be estimated and/or tested).\nThe maximum likelihood estimator for \\((\\mu, \\sigma^2)\\) is \\((\\overline{X}_n, \\widehat{\\sigma}^2)\\) where\n\\[\\overline{X}_n =\\sum_{i=1}^n \\frac{1}{n} X_i\\quad\\text{and}\\quad \\widehat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n (X_i - \\overline{X}_n)^2\\] By Student’s Theorem \\(\\overline{X}_n\\) and \\(\\widehat{\\sigma}^2\\) are stochastically independent \\(\\overline{X}_n \\sim \\mathcal{N}(\\mu, \\widehat{\\sigma}^2/n)\\) and \\(n \\widehat{\\sigma}^2/\\sigma^2 \\sim \\chi^2_{n-1}\\).\n\\[\\sqrt{n} \\frac{\\overline{X}_n - \\mu}{\\widehat{\\sigma}} \\sim t_{n-1}\\]\nwhere \\(t_{n-1}\\) denotes the Student’s \\(t\\) distribution with \\(n-1\\) degrees of freedom.\nWe have the following confidence interval for \\(\\mu\\) at confidence level \\(1-\\alpha\\):\n\\[\\left[\\overline{X}_n - \\frac{\\widehat{\\sigma} t_{n-1,\\alpha/2}}{\\sqrt{n}},   \\overline{X}_n + \\frac{\\widehat{\\sigma} t_{n-1,\\alpha/2}}{\\sqrt{n}}\\right]\\]\n\n\n\n\n\n\nQuestion\n\n\n\nSimulate \\(N=1000\\) Gaussian samples of size \\(n=100\\).\nCompute the empirical coverage of confidence intervals for \\(\\alpha=5\\%\\) and \\(\\alpha=10\\%\\).\nPlot a histogram for replicates of \\(\\frac{\\overline{X}_n - \\mu}{\\widehat{\\sigma}\\sqrt{n}}\\). Overlay the density of \\(t_{n-1}\\)."
  },
  {
    "objectID": "labs/lab-test-miashs-2.html#testing-independence",
    "href": "labs/lab-test-miashs-2.html#testing-independence",
    "title": "Testing independence",
    "section": "Testing independence",
    "text": "Testing independence\n\nIn data gathered from the 2000 General Social Survey (GSS), one cross classiﬁes gender and political party identiﬁcation. Respondents indicated whether they identiﬁed more strongly with the Democratic  or Republican  party or as Independents. This is summarized in the next contingency table (taken from Agresti Introduction to Categorical Data Analysis).\n\n\nCode# GSS &lt;- vcdExtra::GSS\n\nT &lt;- tribble(~ Democrat, ~ Independent, ~ Republican,\n             762, 327, 468,\n             484, 239, 477)\nrownames(T) &lt;- c('Females', 'Males')\n\nWarning: Setting row names on a tibble is deprecated.\n\nCodeT &lt;- as.matrix(T)\nT &lt;- as.table(T)\nnames(dimnames(T)) &lt;- c(\"Gender\", \"Party identification\") \n\n\n\nCodeprop.table(T)\n\n         Party identification\nGender      Democrat Independent Republican\n  Females 0.27638738  0.11860718 0.16974973\n  Males   0.17555314  0.08668843 0.17301415\n\nCodemargin.table(T, 1)\n\nGender\nFemales   Males \n   1557    1200 \n\nCodemargin.table(T, 2)\n\nParty identification\n   Democrat Independent  Republican \n       1246         566         945 \n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nDraw mosaicplot for the cross classification table\nCompute the Pearson chi-square statistic for testing independence\nComment"
  },
  {
    "objectID": "labs/lab-test-miashs-2.html#visualizing-multiway-categorical-data",
    "href": "labs/lab-test-miashs-2.html#visualizing-multiway-categorical-data",
    "title": "Testing independence",
    "section": "Visualizing multiway categorical data",
    "text": "Visualizing multiway categorical data\nConsider the celebrated UCBAdmissions dataset\nAccording to R documentation, this dataset is made of\n\nAggregate data on applicants to graduate school at Berkeley for the six largest departments in 1973 classified by admission and sex.\n\nThis is a compilation of 4526 application files.\nFor each application, three variables have been reported: the department , the gender of the applicant, and whether the applicant has been admitted.\nThe dataset is a trivariate sample, which is summarized by a 3-way contingency table.\n\nCodedata(\"UCBAdmissions\")\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nTurn the 3-way contingency table into a dataframe/tibble with columns Gender, Dept, Admit, n, where the first columns are categorical, and the last column counts the number of co-occurrences of the values in the first three columns amongst the UCB applicants.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nMake it a bivariate sample by focusing on Gender and Admit: compute the margin table\nDraw the corresponding mosaicplot and compute the chi-square independence statistic.\nComment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nVisualize the three-way contingency table using double-decker plots from vcd\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nViewing the UCBAdmissions dataset, which variable would you call a response variable? Which variable would you call covariates?\nTest independence between Gender and Dept.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nFor each department of application (Dept), extract the partial two-way table for Gender and Admit. Test each two-way table for independence. How many departments pass the test at significance level \\(1\\%\\), \\(5\\%\\)?\n\n\nNote that the two-way cross-sectional slices of the three-way table are called partial tables.\nWhat we observed has a name.\n\n\n\n\n\n\nSimpson’s paradox\n\n\n\nThe result that a marginal association can have different direction from the conditional associations is called Simpson’s paradox. This result applies to quantitative as well as categorical variables."
  },
  {
    "objectID": "labs/lab-whiteside.html#packages-installation-and-loading-again",
    "href": "labs/lab-whiteside.html#packages-installation-and-loading-again",
    "title": "Linear regression, diagnostics, variable selection",
    "section": "Packages installation and loading (again)",
    "text": "Packages installation and loading (again)\nWe will use the following packages. If needed, we install them.\n\nCodestopifnot(\n  require(tidyverse), \n                  require(broom),\n                  require(magrittr),\n                  require(lobstr),\n                  require(ggforce),\n#                  require(cowplot),\n                  require(patchwork), \n                  require(glue),\n                  require(DT), \n                  require(viridis)\n)"
  },
  {
    "objectID": "labs/lab-ca-hesitancy.html",
    "href": "labs/lab-ca-hesitancy.html",
    "title": "Correspondence Analysis of Survey Data",
    "section": "",
    "text": "M1 MIDS/MFA\nUniversité Paris Cité\nAnnée 2024-2025\n\nCourse Homepage\n\nMoodle\n\n\n\n\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\n\n\nContext\nThis notebook is based on\nReference: Social inequalities in hostility toward vaccination against Covid-19\nby Alexis Spire, Nathalie Bajos, Léna Silberzan , for the EPICOV study group\n\nIn recent decades, France has appeared as a country particularly hostile to vaccination in general. When asked in November 2020 about the intention to take the Covid-19 vaccine, the French public showed, once again, reluctance. Therefore, France appeared as an ideal case study to analyze whether the hostility toward the Covid-19 vaccine has its own reasons or whether it is related to the reluctance to the principle of vaccination itself. Our objective was to determine the specificity of the social determinants of the intention to get the Covid-19 vaccine. Thanks to the use of a large random sample of the general population in France (86,000 individuals), the reluctant to Covid-19 vaccine could be clearly distinguished from the hesitant and the convinced, and thereby thoroughly analyzed. Our analysis highlighted a gendered reluctance toward vaccination in general but even more so regarding vaccination against Covid-19. It might refer to women being more concerned about the possible effects of an injection in their body, especially at the age of maternity and a differentiated socialization making them more sensitive than men to long-term risks and more apprehensive toward rapid technological change. We also found that people at the bottom of the social hierarchy, in terms of level of education, financial resources, and immigration status, were more likely to refuse the Covid-19 vaccine. Nevertheless, this reluctance was less prominent than for vaccination in general, reflecting the actual spread of the epidemic in various social milieux. Finally, our analysis showed that trust in the government’s actions was significantly associated with reluctance toward the Covid-19 vaccine, even more than toward vaccination in general.\n\nRecension dans Le Monde (Juin 2021)\nSurexposition\nHésitants et réfractaires\nThe authors distinguish two skeptical attitudes towards vaccination.\nVaccine reluctance:\nVaccine hesitancy:\nThe EPICOV investigation\n\nThe EpiCoV (Epidémiologie et Conditions de Vie) cohort was set-up in April 2020, with the general aim of understanding the main epidemiological, social and behavioural issues related to the Covid-19 epidemic in France. The survey was approved by the CNIL (French independent administrative authority responsible for data protection) on April 25th 2020 (ref: MLD/MFI/AR205138) and by the “Comité de protection des personnes” (French equivalent of the Research Ethics Committee) on April 24th. The survey also obtained an agreement from the Comité du Label de la statistique publique, proving its adequacy to statistical quality standards.\n\nSee https://doi.org/10.1101/2021.02.24.21252316 for more the EpiCov cohort\n\nThis study was based on a large-scale random survey of 107,808 people conducted between October 26 and December 9, 2020, a pivotal time, as Pfizer announced on November 9, 2020, that it would be able to produce a \\(90\\%\\) effective vaccine on a large scale.\n\nDemographic variables\n\nTo describe the sample, six sociodemographic variables were considered: age, gender, ethno-racial status (based on migration history), social class (based on current or last occupation), standard of living (based on decile of household income per consumption unit), and formal education level. Ethno-racial status was defined by combining the criteria of place of birth, nationality, and status of the individual and both parents.\n\n\nThe analysis was conducted from an intersectional perspective [10] that simultaneously took into account gender, class, age, and ethno-racial social characteristics, as well as respondents’ level of trust in the government.\n\nAttitudinal variables: general versus specific vaccination hesitancy\n\nTo study attitudes toward vaccination in the EpiCoV survey in November 2020, two questions were available.\n\n\n\nAbout vaccination in general: Are you strongly; somewhat; somewhat not; or not at all in favor of vaccinations in general?\n\n\n\n\nAbout the Covid-19 vaccine: If a free vaccine against coronavirus were offered by the Sécurité Sociale, would you be willing to get vaccinated: Yes probably; Yes maybe; Probably not; Certainly not; or you do not know.\n\n\n\nAttitudes towards vaccination are reported on a Likert/rating scale. Is there any difference between the rating scales for the two questions?\nIf you find any difference, can you guess the motivation?\nRating scales\nLikert scale/Rating scale\n\nCodextab_general_covid |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\nYes Probably\nYes Maybe\nDo not know\nProbably Not\nCertainly Not\n\n\n\nStrongly in Favor\n16062\n3411\n993\n468\n386\n\n\nSomewhat in Favor\n12607\n16190\n6234\n3901\n2705\n\n\nSomewhat Not in Favor\n928\n3321\n3408\n3947\n3242\n\n\nNot at all in Favor\n227\n524\n1344\n1144\n4723\n\n\n\n\n\nMosaiplot vaccination against covid vaccination\n\n\n\n\n\n\n\n\n\nUse tools from package vcd to display mosaicplots with different stylings\n\n\nCodevcd::mosaic(~ vaccine + covid_vaccine, xtab_general_covid, shade=T) \n\n\n\n\n\n\n\n\nCodechisq.test(xtab_general_covid) |&gt; \n  broom::tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                    \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1    45356.       0        12 Pearson's Chi-squared test\n\n\nCA using low-levels funtions\n\nCodeX &lt;-  as.matrix(xtab_general_covid)\n# P &lt;- \n# Dc &lt;- \n# Dr &lt;- \n# R &lt;- \n# ...\n# svd()\n\n\nCompare your own CA with FactoMineR::CA()\n\nCorrespondence analysis : Screeplot\n\n\n\n\n\n\n\n\nBiplot\n\n\n\n\n\n\n\n\n\nCodemosaicplot(ls_tables[[5]])\n\n\n\n\n\n\n\nAge\n\n\n\n\n\n\n\n\nAge mosaicplots\n\n\n\n\n\n\n\n\nCSP\n\n\n\n\n\n\n\n\n\nCSP mosaicplots\n\n\n\n\n\n\n\n\nEducation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEducation mosaicplots\n\n\n\n\n\n\n\n\nIncome\nIncome mosaicplots\n\n\n\n\n\n\n\n\nEthnicity\n\n\n\n\n\n\n\n\nEthnicity mosaicplots\n\n\n\n\n\n\n\n\nGender\n\n\n\n\n\n\n\n\n#Family\n\n\n\n\n\n\n\n\nIn government we trust (or not)\n\n\n\n\n\n\n\n\n\nCodenames(ca_analyses)\n\n [1] \"afraid_covid\"           \"age_covid\"              \"age_vaccine\"           \n [4] \"child_vaccine\"          \"children_covid\"         \"comorbidities_covid\"   \n [7] \"csp_covid\"              \"csp_vaccine\"            \"education_covid\"       \n[10] \"education_vaccine\"      \"ethnic_vaccine\"         \"ethno_covid\"           \n[13] \"gender_covid\"           \"gender_vaccine\"         \"income_covid\"          \n[16] \"income_vaccine\"         \"trust_government_covid\" \"xtab_general_covid\""
  },
  {
    "objectID": "projects/homework02.html",
    "href": "projects/homework02.html",
    "title": "Propaganda, start the spark session",
    "section": "",
    "text": "For SQL users, Spark SQL provides state-of-the-art SQL performance and maintains compatibility with Shark/Hive. In particular, like Shark, Spark SQL supports all existing Hive data formats, user-defined functions (UDF), and the Hive metastore.\n\n\nFor Spark users, Spark SQL becomes the narrow-waist for manipulating (semi-) structured data as well as ingesting data from sources that provide schema, such as JSON, Parquet, Hive, or EDWs. It truly unifies SQL and sophisticated analysis, allowing users to mix and match SQL and more imperative programming APIs for advanced analytics.\n\n\nFor open source hackers, Spark SQL proposes a novel, elegant way of building query planners. It is incredibly easy to add new optimizations under this framework.\n\n\nInternally, a structured query is a Catalyst tree of (logical and physical) relational operators and expressions.\n\n\n# import the usual suspects\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom pathlib import Path\nimport sys\nimport timeit\n\n%matplotlib inline\nimport seaborn as sns\n\nsns.set_context(\"notebook\", font_scale=1.2)\n\nDuring the session, we will use classes and functions exported by pyspark\n\n# spark\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import col\nimport pyspark.sql.functions as fn\nfrom pyspark.sql.catalog import Catalog\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import IntegerType, StringType\n\nStart the SparkSession\n\nconf = SparkConf().setAppName(\"Spark SQL Illustrations\")\nsc = SparkContext(conf=conf)\n\nspark = (SparkSession\n    .builder\n    .appName(\"Spark SQL\")\n    .getOrCreate()\n)\n\n24/08/09 15:32:51 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 192.168.10.120 instead (on interface wlp0s20f3)\n24/08/09 15:32:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/08/09 15:32:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable"
  },
  {
    "objectID": "projects/homework02.html#using-spark.sql",
    "href": "projects/homework02.html#using-spark.sql",
    "title": "Propaganda, start the spark session",
    "section": "Using spark.sql()",
    "text": "Using spark.sql()\nIn order to use mode sql, create a temporary view from the DataFrame.\n\nWhat are temporary views made of?\nAre there other kind of views in spark’s world?\n\n\nCatalog(spark).listTables()\n\n[]\n\n\n\n# TODO: \n\nCatalog(spark).listTables()\n\n[]"
  },
  {
    "objectID": "projects/homework02.html#a-query-is-a-plain-sql-query-embodied-in-a-string.",
    "href": "projects/homework02.html#a-query-is-a-plain-sql-query-embodied-in-a-string.",
    "title": "Propaganda, start the spark session",
    "section": "A query is a plain SQL query embodied in a string.",
    "text": "A query is a plain SQL query embodied in a string.\n\nquery = \"\"\"TODO: \"\"\"\n\n# spark.sql(query)\n\n\nThis phrasing is not consistent with the DRY principle. Fix this using formatted strings."
  },
  {
    "objectID": "projects/homework02.html#using-the-dataframedataset-api",
    "href": "projects/homework02.html#using-the-dataframedataset-api",
    "title": "Propaganda, start the spark session",
    "section": "Using the dataframe/dataset API",
    "text": "Using the dataframe/dataset API\nThis can also be done using Spark SQL API.\n\nPedestrian approach\n\nFirst select 10 most popular names for girls in year 2000, define spark dataframe top10_2000_f.\nDoes the definition of top10_2000_f involve transformations, actions or both?\nWhat is the type of the result returned by top10_2000_f.take(2)? the type of elements of the result?\n\n\n# top10_2000_f = TODO:\n\n\nDo the same thing for boys.\n\n\n# top10_2000_m = TODO:\n\n\nCompute the union of the two spark dataframes. Store the result in dataframe top10_2000\n\n\n# top10_2000 = TODO:\n\n\n\nDo it again, complying with DRY principle\n\n# TODO:"
  },
  {
    "objectID": "labs/lab-lin-reg.html#setup",
    "href": "labs/lab-lin-reg.html#setup",
    "title": "Linear regression I",
    "section": "Setup",
    "text": "Setup\n\nCodestopifnot(\n  require(broom),\n  require(corrr),\n  require(DT), \n  require(GGally),\n  require(ggforce),\n  require(glue),\n  require(gt),\n  require(httr),\n  require(kableExtra),\n  require(lobstr),\n  require(magrittr),\n  require(patchwork),\n  require(rlang),\n  require(skimr),\n  require(fs),\n  require(tidyverse),\n  require(viridis)\n)\n\nold_theme &lt;- theme_set(theme_minimal())\n\nknitr::opts_chunk$set(\n  message = FALSE,\n  warning = FALSE,\n  comment=NA,\n  prompt=FALSE,\n  cache=FALSE,\n  echo=TRUE,\n  results='asis'\n)"
  },
  {
    "objectID": "labs/lab-lin-reg.html#dataset",
    "href": "labs/lab-lin-reg.html#dataset",
    "title": "Linear regression I",
    "section": "Dataset",
    "text": "Dataset\nDataset Banque.csv contains information on clerical officers in the Banking sector. We aim at investigating connections between variables."
  },
  {
    "objectID": "labs/lab-whiteside-anova.html",
    "href": "labs/lab-whiteside-anova.html",
    "title": "Linear regression: ANOVA",
    "section": "",
    "text": "Codetheme_set(theme_minimal())"
  },
  {
    "objectID": "labs/lab-whiteside-anova.html#challenges",
    "href": "labs/lab-whiteside-anova.html#challenges",
    "title": "Linear regression: ANOVA",
    "section": "Challenge(s)",
    "text": "Challenge(s)"
  },
  {
    "objectID": "labs/lab-whiteside-anova.html#comparing-weekly-average-temperatures-over-two-seasons",
    "href": "labs/lab-whiteside-anova.html#comparing-weekly-average-temperatures-over-two-seasons",
    "title": "Linear regression: ANOVA",
    "section": "Comparing weekly average temperatures over two seasons",
    "text": "Comparing weekly average temperatures over two seasons\nWe address the following question: was the external temperature distributed in the same way during the two heating seasons? When we raise this question, we silently make modeling assumptions. Spell them out.\nWhat kind of hypothesis are we testing in the next two chunks? Interpret the results.\n\nCodelm_temp &lt;- lm(Temp ~ Insul, whiteside)\n\nlm_temp |&gt; \n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    5.35      0.537      9.96 7.80e-14\n2 InsulAfter    -0.887     0.734     -1.21 2.32e- 1\n\n\n\nCodelm_temp |&gt; \n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0263       0.00830  2.74      1.46   0.232     1  -135.  276.  282.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nDisplay parallel boxplots, overlayed cumulative distribution functions and a quantile-quantile plot (QQ-plot) to compare the temperature distributions during the two heating seasons. Comment"
  },
  {
    "objectID": "labs/lab-whiteside-anova.html#perform-a-wilcoxon-test-to-assess-change-of-temperature-between-the-two-seasons",
    "href": "labs/lab-whiteside-anova.html#perform-a-wilcoxon-test-to-assess-change-of-temperature-between-the-two-seasons",
    "title": "Linear regression: ANOVA",
    "section": "Perform a Wilcoxon test to assess change of Temperature between the two seasons",
    "text": "Perform a Wilcoxon test to assess change of Temperature between the two seasons"
  },
  {
    "objectID": "labs/lab-whiteside-anova.html#compare-gas-consumption-before-and-after-leaving-temperature-aside",
    "href": "labs/lab-whiteside-anova.html#compare-gas-consumption-before-and-after-leaving-temperature-aside",
    "title": "Linear regression: ANOVA",
    "section": "Compare Gas consumption before and after (leaving Temperature aside)",
    "text": "Compare Gas consumption before and after (leaving Temperature aside)\nDraw a qqplot to compare Gas consumptions before and after insulation.\nCompare ECDFs of Gas consumption before and after insulation.\nDo Insulation and Temperature additively matter?\nThis consists in assessing whether the Intercept is modified after Insulation while the slope is left unchanged. Which models should be used to assess this hypothesis?\nDraw the disgnostic plots for this model\nDo Insulation and Temperature matter and interact?\nFind the formula and build the model.\nDo Insulation and powers of temperature interact?\nInvestigate formulae Gas ~ poly(Temp, 2, raw=T)*Insul, Gas ~ poly(Temp, 2)*Insul, Gas ~ (Temp +I(Temp*2))*Insul, Gas ~ (Temp +I(Temp*2))| Insul"
  },
  {
    "objectID": "labs/lab-whiteside-anova.html#higher-degree-polynomials",
    "href": "labs/lab-whiteside-anova.html#higher-degree-polynomials",
    "title": "Linear regression: ANOVA",
    "section": "Higher degree polynomials",
    "text": "Higher degree polynomials\nPlay it with degree 10 polynomials"
  },
  {
    "objectID": "labs/lab-whiteside-anova.html#drying-model-exploration",
    "href": "labs/lab-whiteside-anova.html#drying-model-exploration",
    "title": "Linear regression: ANOVA",
    "section": "Drying model exploration",
    "text": "Drying model exploration\nCollecting the models a posteriori\nMake a named list with the models constructed so far"
  },
  {
    "objectID": "projects/homework01-2023.html",
    "href": "projects/homework01-2023.html",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "",
    "text": "Due date : 2023-02-24 @23h55 (this is a hard deadline)\n\n\n\nName, First Name, Informatique/Mathématique-Informatique\nName, First Name, Informatique/Mathématique-Informatique\n\n\n\n\nIf you don’t: no evaluation!\nWrite in English or French\nThe deliverable is a file\n\nxxx_yyy.ipynb file (jupyter notebook) or\nxxx_yyy.py file (if you are using jupytext) or\nxxx_yyy.qmd file (if you are using quarto)\n\nwhere xxx and yyy are your names, for example lagarde_michard.ipynb.\nThe deliverable is not meant to contain cell outputs.\nThe data files used to execute cells are meant to sit in the same directory as the deliverable. Use relative filepaths or urls to denote the data files.\nWe will execute the code in your notebook: make sure that running all the cells works well.\n\n\n\nHere is the way we’ll assess your work\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nSpelling and syntax\n3\nEnglish/French\n\n\nPlots correction\n3\nClarity / answers the question\n\n\nPlot style and cleanliness\n3\nTitles, legends, labels, breaks …\n\n\nTable wrangling\n4\nETL, SQL like manipulations\n\n\nComputing Statistics\n5\nSQL goup by and aggregation\n\n\nDRY compliance\n2\nDRY principle at Wikipedia\n\n\n\nIf we see a single (or more) for loop in your code: -5 points. Everything can be done using high-level pandas methods"
  },
  {
    "objectID": "projects/homework01-2023.html#fill-this-cell-with-your-names",
    "href": "projects/homework01-2023.html#fill-this-cell-with-your-names",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "",
    "text": "Name, First Name, Informatique/Mathématique-Informatique\nName, First Name, Informatique/Mathématique-Informatique"
  },
  {
    "objectID": "projects/homework01-2023.html#carefully-follow-instructions",
    "href": "projects/homework01-2023.html#carefully-follow-instructions",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "",
    "text": "If you don’t: no evaluation!\nWrite in English or French\nThe deliverable is a file\n\nxxx_yyy.ipynb file (jupyter notebook) or\nxxx_yyy.py file (if you are using jupytext) or\nxxx_yyy.qmd file (if you are using quarto)\n\nwhere xxx and yyy are your names, for example lagarde_michard.ipynb.\nThe deliverable is not meant to contain cell outputs.\nThe data files used to execute cells are meant to sit in the same directory as the deliverable. Use relative filepaths or urls to denote the data files.\nWe will execute the code in your notebook: make sure that running all the cells works well."
  },
  {
    "objectID": "projects/homework01-2023.html#grading",
    "href": "projects/homework01-2023.html#grading",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "",
    "text": "Here is the way we’ll assess your work\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nSpelling and syntax\n3\nEnglish/French\n\n\nPlots correction\n3\nClarity / answers the question\n\n\nPlot style and cleanliness\n3\nTitles, legends, labels, breaks …\n\n\nTable wrangling\n4\nETL, SQL like manipulations\n\n\nComputing Statistics\n5\nSQL goup by and aggregation\n\n\nDRY compliance\n2\nDRY principle at Wikipedia\n\n\n\nIf we see a single (or more) for loop in your code: -5 points. Everything can be done using high-level pandas methods"
  },
  {
    "objectID": "projects/homework01-2023.html#notebooks-modus-operandi",
    "href": "projects/homework01-2023.html#notebooks-modus-operandi",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Notebooks: Modus operandi",
    "text": "Notebooks: Modus operandi\n\nThis is a Jupyter Notebook.\nWhen you execute code within the notebook, the results appear beneath the code.\nJupytext\nQuarto"
  },
  {
    "objectID": "projects/homework01-2023.html#packages",
    "href": "projects/homework01-2023.html#packages",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Packages",
    "text": "Packages\n\nBase Python can do a lot. But the full power of Python comes from a fast growing collection of packages/modules.\nPackages are first installed (that is using pip install or conda install), and if needed, imported during a session.\nThe docker image you are supposed to use already offers a lot of packages. You should not need to install new packages.\nOnce a package has been installed on your drive, if you want all objects exported by the package to be available in your session, you should import the package, using from pkg import *.\nIf you just want to pick some subjects from the package, you can use qualified names like pkg.object_name to access the object (function, dataset, class…)\n\n\n\nCode\n# importing basic tools\nimport numpy as np\nimport pandas as pd\n\nfrom pandas.api.types import CategoricalDtype\n\nimport os            # file operations\nimport requests      # networking\nimport zipfile\nimport io\nfrom pathlib import Path\n\nfrom datetime import date  # if needed\n\n\n\n\nCode\n# importing plotting packages\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\n\n\n\n\nCode\n# make pandas plotly-friendly\nnp.set_printoptions(precision=2, suppress=True)\n%matplotlib inline\npd.options.plotting.backend = \"plotly\""
  },
  {
    "objectID": "projects/homework01-2023.html#french-data",
    "href": "projects/homework01-2023.html#french-data",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "French data",
    "text": "French data\nThe French data are built and made available by INSEE (French Governement Statistics Institute)\nPrénoms: - https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip\nThis dataset has been growing for a while. It has been considered by social scientists for decades. Given names are meant to give insights into a variety of phenomena, including religious observance.\n\nA glimpse at the body of work can be found in L’archipel français by Jérome Fourquet, Le Seuil, 2019\nRead the File documentation"
  },
  {
    "objectID": "projects/homework01-2023.html#us-data",
    "href": "projects/homework01-2023.html#us-data",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "US data",
    "text": "US data\nUS data may be gathered from\nBaby Names USA from 1910 to 2021 (SSA)\nSee https://www.ssa.gov/oact/babynames/background.html"
  },
  {
    "objectID": "projects/homework01-2023.html#british-data",
    "href": "projects/homework01-2023.html#british-data",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "British data",
    "text": "British data\nEnglish and Welsh data can be gathered from\nhttps://www.ons.gov.uk/"
  },
  {
    "objectID": "projects/homework01-2023.html#download-the-french-data",
    "href": "projects/homework01-2023.html#download-the-french-data",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Download the French data",
    "text": "Download the French data\nQUESTION: Download the data into a file which relative path is './nat2021_csv.zip'\nHints:\n\nHave a look at package requests.\nUse magic commands to navigate across the file hierarchy and create subdirectories when needed\n\n\n\nCode\n# for French data \n\nparams = dict(\n    url = 'https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip',\n    dirpath = './',\n    timecourse = '',\n    datafile = 'nat2021.hdf',\n    fpath = 'nat2021_csv.zip'\n)\n\n\n\n\nCode\n# modify location  make sure you are in the right directory\n# %cd\n# %pwd  #\n# %ls\n# %mkdir # if needed\n\n\n\n\nCode\nurl = params['url']      # 'https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip'\nfpath = params['fpath']  # './nat2021_csv.zip'\n\n\n\n\nCode\n# your code here\n\nif not Path(params['fpath']).exists():\n    r = requests.get(params['url'])                # What is the type of `r` ?\n    z = zipfile.ZipFile((io.BytesIO(r.content)))   # What is the type of `z` ? \n    z.extractall(path='./')"
  },
  {
    "objectID": "projects/homework01-2023.html#download-us-and-british-data",
    "href": "projects/homework01-2023.html#download-us-and-british-data",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Download US and British data",
    "text": "Download US and British data\n\n\nCode\n# your code here\n# https://www.ssa.gov/oact/babynames/limits.html\nurl_us = 'https://www.ssa.gov/oact/babynames/names.zip'\nurl_states = 'https://www.ssa.gov/oact/babynames/state/namesbystate.zip'\n\nfpath = './names.zip'\nPath('fpath').exists()\n\n\nFalse\n\n\n\n\nCode\ndef download(url, fpath):\n    if not Path(fpath).exists():\n        r = requests.get(url)                # What is the type of `r` ?\n        z = zipfile.ZipFile((io.BytesIO(r.content)))   # What is the type of `z` ? \n        z.extractall(path='./')\n    else:\n        print(f'File {fpath} already exists!')\n\n\n\n\nCode\ndownload(url_us, fpath)"
  },
  {
    "objectID": "projects/homework01-2023.html#load-the-french-data-in-memory",
    "href": "projects/homework01-2023.html#load-the-french-data-in-memory",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Load the French data in memory",
    "text": "Load the French data in memory\nQUESTION: Load the data in a pandas DataFrame called data\nHints:\n\nYou should obtain a Pandas dataframe with 4 columns.\nMind the conventions used to build the csv file.\nPackage pandas provides the convenient tools.\nThe dataset, though not too large, is already demanding.\nDon’t hesitate to test your methods on a sample of rows method sample() from class DataFrame can be helpful.\n\n\n\nCode\n# your code here"
  },
  {
    "objectID": "projects/homework01-2023.html#load-us-and-british-data-in-memory",
    "href": "projects/homework01-2023.html#load-us-and-british-data-in-memory",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Load US and British data in memory",
    "text": "Load US and British data in memory\n\n\nCode\n# your code here"
  },
  {
    "objectID": "projects/homework01-2023.html#explore-the-data",
    "href": "projects/homework01-2023.html#explore-the-data",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Explore the data",
    "text": "Explore the data\nQUESTION: Look at the data, Use the attributes columns, dtypes and the methods head, describe, to get a feeling of the data.\n\nThis dataset is supposed to report all given names used for either sex during a year in France since 1900\nThe file is made of 652 056 lines and 4 columns.\n\n|-- preusuel : object\n|-- nombre: int64\n|-- sexe: int64\n|-- annais: object\nEach row indicates for a given preusuel (prénom usuel, given name), sexe (sex), and annais (année naissance, birthyear) the nombre (number) of babies of the given sex who were given that name during the given year.\n\n\n\nsexe\npreusuel\nannais\nnombre\n\n\n\n\n2\nSYLVETTE\n1953\n577\n\n\n1\nBOUBOU\n1979\n4\n\n\n1\nNILS\n1959\n3\n\n\n2\nNICOLE\n2003\n36\n\n\n1\nJOSÉLITO\n2013\n4\n\n\n\nQUESTION: Compare memory usage and disk space used by data\nHints:\n\nThe method info prints a concise summary of a DataFrame.\nWith optional parameter memory_usage, you can get an estimate of the amount of memory used by the DataFrame.\nBeware that the resulting estimate depends on the argument fed.\n\n\n\nCode\n# your code here\n\n\nQUESTION: Display the output of .describe() with style.\n\n\nCode\n# your code here\n\n\nQUESTION: For each column compute the number of distinct values\n\n\nCode\n# your code here"
  },
  {
    "objectID": "projects/homework01-2023.html#improving-the-data-types",
    "href": "projects/homework01-2023.html#improving-the-data-types",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Improving the data types",
    "text": "Improving the data types\nQUESTION: Make sexe a category with two levels Female and Male. Call the new column gender. Do you see any reason why this factor should be ordered?\nHint: Read Pandas and categorical variables\n\n\nCode\n# your code here\n\n\nQUESTION: Compare memory usage of columns sexe and gender\n\n\nCode\n# your code here\n\n\nQUESTION: Would it be more memory-efficient to recode sexe using modalities F and M instead of Male and Female ?\nInsert your answer here\n\n…"
  },
  {
    "objectID": "projects/homework01-2023.html#dealing-with-missing-values",
    "href": "projects/homework01-2023.html#dealing-with-missing-values",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Dealing with missing values",
    "text": "Dealing with missing values\nQUESTION: Variable annais class is object. Make annais of type float. Note that missing years are encoded as “XXXX”, find a way to deal with that.\nHint: As of releasing this Homework (2023-01-18), Pandas is not very good at managing missing values, see roadmap. Don’t try to convert annais into an integer column.\n\n\nCode\n# your code here"
  },
  {
    "objectID": "projects/homework01-2023.html#rename-and-remove-columns",
    "href": "projects/homework01-2023.html#rename-and-remove-columns",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Rename and remove columns",
    "text": "Rename and remove columns\nQUESTION: Remove useless columns (now that you’ve created new ones, and rename them). You should end up with a dataframe with columns called \"gender\", \"year\", \"count\", \"firstname” with the following dtypes:\ngender        category\nfirstname     object\ncount         int64\nyear          float64\n\n\nCode\n# your code here\n\n\nQuestion: Do the same thing for British and US data. You should eventually obtain dataframes with the same schema.\nQUESTION: How many missing values (NA) have been introduced? How many births are concerned?\n\n\nCode\n# your code here\n\n\nQUESTION: Read the documentation and describe the origin of rows containing the missing values.\n\n\nCode\n# your code here"
  },
  {
    "objectID": "projects/homework01-2023.html#checkpointing-save-your-transformed-dataframes",
    "href": "projects/homework01-2023.html#checkpointing-save-your-transformed-dataframes",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Checkpointing: save your transformed dataframes",
    "text": "Checkpointing: save your transformed dataframes\nQUESTION: Save the transformed dataframe (retyped and renamed) to ./nat2021_csv.zip. Try several compression methods.\n\n\nCode\n# your code here\n\n\nQUESTION: Save the transformed dataframes (retyped and renamed) to ./nat2021.hdf using .hdf format\n\n\nCode\n# your code here\n\n\nAt that point your working directory should look like:\n├── homework01.py      # if you use `jupytext`\n|── homework01.qmd     # if you use `quarto`\n├── homework01.ipynb   # if you use `jupyter` `notebook`\n├── babies-fr.hdf\n├── babies-fr.zip\n├── babies-us.hdf\n├── babies-us.zip\n├── babies-ew.hdf\n├── babies-ew.zip\n├── births-fr.csv\n├── births-fr.hdf\nQUESTION: Reload the data using read_hdf(...) so that the resulting dataframes are properly typed with meaningful and homogeneous column names.\nHint: use try: ... except to handle exceptions such as FileNotFoundError\n\n\nCode\n# your code here"
  },
  {
    "objectID": "projects/homework01-2023.html#some-data-analytics-and-visualization",
    "href": "projects/homework01-2023.html#some-data-analytics-and-visualization",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Some data “analytics” and visualization",
    "text": "Some data “analytics” and visualization\nQUESTION: For each year, compute the total number of Female and Male births and the proportion of Female births among total births\nHints:\n\nGroupby operations using several columns for the groups return a dataframe with a MultiIndex index see Pandas advanced\nHave a look at MultiIndex, reset_index, pivot, columns.droplevel\n\n\n\nCode\n# your code here\n\n\nQUESTION: Plot the proportion of female births as a function of year and French, US, en British babynames data. Compare with what you get from births-fr.hdf.\nDon’t forget: title, axes labels, ticks, scales, etc.\nBecause of what we did before, the plot method of a DataFrame with be rendered using plotly, so you can use this. But you can use also seaborn or any other available plotting library that you want.\nHint: Mind the missing values in the year column\n\n\nCode\n# your code here\n\n\nQUESTION: Make any sensible comment about these plots.\nInsert your answer here\n\n…\n\nQUESTION: Explore the fluctuations of sex ratio around its mean value since 1945 in the US, in France and in the Great Britain.\nPlot deviations of sex ratio around its mean since 1945 as a function of time.\n\n\nCode\n# your code here\n\n\nQUESTION: Assume that baby gender is chosen at random according to a Bernoulli distribution with success probability \\(.48\\), that baby genders are i.i.d. Perform simulations for sex ratios for French and US data since 1945.\nPlot the results, compare with your plots above."
  },
  {
    "objectID": "projects/homework01-2023.html#rare-firstnames",
    "href": "projects/homework01-2023.html#rare-firstnames",
    "title": "Homework 1 (2023): Data Wrangling and Visualization",
    "section": "Rare firstnames",
    "text": "Rare firstnames\nQUESTION: In the French data, for each sex, plot the proportion of births given _PRENOMS_RARES as a function of the year.\n\n\nCode\n# your code here"
  },
  {
    "objectID": "labs/lab-gapminder.html#grammar-of-graphics",
    "href": "labs/lab-gapminder.html#grammar-of-graphics",
    "title": "Data visualization",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nWe will use the Grammar of Graphics approach to visualization\nThe expression Grammar of Graphics was coined by Leiland Wilkinson to describe a principled approach to visualization in Data Analysis (EDA)\nA plot is organized around tabular data (a table with rows (observations) and columns (variables))\nA plot is a graphical object that can be built layer by layer\nBuilding a graphical object consists in chaining elementary operations\nThe acclaimed TED presentation by Hans Rosling illustrates the Grammar of Graphics approach\n\nWe will reproduce the animated demonstration using\n\n\nggplot2: an implementation of grammar of graphics in `R\n\nplotly: a bridge between R and the javascript library D3.js\n\nUsing plotly, opting for html ouput, brings the possibility of interactivity and animation"
  },
  {
    "objectID": "labs/lab-gapminder.html#setup",
    "href": "labs/lab-gapminder.html#setup",
    "title": "Data visualization",
    "section": "Setup",
    "text": "Setup\nWe will use the following packages. If needed, we install them.\n\nCodestopifnot(\n  require(tidyverse), \n  require(patchwork), \n  require(glue), \n  require(ggforce), \n  require(plotly),\n  require(ggthemes),\n  require(gapminder),\n  require(ggrepel)\n)\n\n\nThe data we will use can be obtained by loading package gapminder\n\n\n\n\n\n\nTip\n\n\n\nIf the packages have not yet been installed on your hard drive, install them.\nYou can do that using base R install.packages() function:\ninstall.packages(\"tidyverse\")\nIt is often faster to use functions from package pak\ninstall.packages(\"pak\")\npak::pkg_install(\"tidyverse\")\n\n\nYou need to understand the difference between installing and loading a package\n\n\n\n\n\n\nQuestion\n\n\n\n\nHow do we get the list of installed packages?\nHow do we get the list of loaded packages?\nWhich objects are made available by a package?"
  },
  {
    "objectID": "labs/lab-gapminder.html#have-a-look-at-gapminder-dataset",
    "href": "labs/lab-gapminder.html#have-a-look-at-gapminder-dataset",
    "title": "Data visualization",
    "section": "Have a look at gapminder dataset",
    "text": "Have a look at gapminder dataset\nThe gapminder table can be found at gapminder::gapminder\n\nA table has a schema: a list of named columns, each with a given type\nA table has a content: rows. Each row is a collection of items, corresponding to the columns\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplore gapminder::gapminder, using glimpse() and head()\n\n\nglimpse() allows to see the schema and the first rows\n\nhead() allows to see the first rows\nUse the pipe |&gt; to chain operations"
  },
  {
    "objectID": "labs/lab-gapminder.html#get-a-feeling-of-the-dataset",
    "href": "labs/lab-gapminder.html#get-a-feeling-of-the-dataset",
    "title": "Data visualization",
    "section": "Get a feeling of the dataset",
    "text": "Get a feeling of the dataset\n\n\n\n\n\n\nQuestion\n\n\n\nPick two random rows for each continent using slice_sample()\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat makes a table tidy?\n\n\n\n\n\n\n\n\nTip\n\n\n\nHave a look at Data tidying in R for Data Science (2nd ed.)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs the gapminder table redundant?"
  },
  {
    "objectID": "labs/lab-gapminder.html#gapminder-tibble-extract",
    "href": "labs/lab-gapminder.html#gapminder-tibble-extract",
    "title": "Data visualization",
    "section": "Gapminder tibble (extract)",
    "text": "Gapminder tibble (extract)\n\n\n\n\n\n\nQuestion\n\n\n\nExtract/filter a subset of rows using dplyr::filter(...)\n\nAll rows concerning a given country\nAll rows concerning a year\nAll rows concerning a given continnent and a year"
  },
  {
    "objectID": "labs/lab-gapminder.html#filtering-selection-σ-from-database-theory-picking-one-year-of-data",
    "href": "labs/lab-gapminder.html#filtering-selection-σ-from-database-theory-picking-one-year-of-data",
    "title": "Data visualization",
    "section": "Filtering (selection \\(σ\\) from database theory) : Picking one year of data",
    "text": "Filtering (selection \\(σ\\) from database theory) : Picking one year of data\nThere is simple way to filter rows satisfying some condition. It consists in mimicking indexation in a matrix, leaving the colum index empty, replacing the row index by a condition statement (a logical expression) also called a mask.\n\nCodegapminder_2002 &lt;- gapminder[gapminder$year==2002, ]\n\n\nHave a look at gapminder$year==2002. What is the type/class of this expression?\nThis is possible in base R and very often convenient.\nNevertheless, this way of performing row filtering does not emphasize the connection between the dataframe and the condition. Any logical vector with the right length could be used as a mask. Moreover, this way of performing filtering is not very functional.\n\n\n\n\n\n\nIn the parlance of Relational Algebra, filter performs a selection of rows. Relational expression \\[σ_{\\text{condition}}(\\text{Table})\\] translates to\nfilter(Table, condition)\nwhere \\(\\text{condition}\\) is a boolean expression that can be evaluated on each row of \\(\\text{Table}\\). In SQL, the relational expression would translate into\nSELECT *\nFROM Table\nWHERE condition\nCheck Package dplyr docs\nThe posit cheatsheet on dplyr is an unvaluable resource for table manipulation.\n\n\n\nUse dplyr::filter() to perform row filtering"
  },
  {
    "objectID": "labs/lab-gapminder.html#static-plotting-first-attempt",
    "href": "labs/lab-gapminder.html#static-plotting-first-attempt",
    "title": "Data visualization",
    "section": "Static plotting: First attempt",
    "text": "Static plotting: First attempt\n\n\n\n\n\n\nQuestion\n\n\n\nDefine a plot with respect to gapminder_2002 along the lines suggested by Rosling’s presentation.\n\n\n\n\n\n\n\n\nYou should define a ggplot object with data layer gapminder_2022 and call this object p for further reuse.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nMap variables gdpPercap and lifeExp to axes x and y. Define the axes. In ggplot2 parlance, this is called aesthetic mapping. Use aes().\n\n\n\n\n\n\n\n\nUse ggplot object p and add a global aesthetic mapping gdpPercap and lifeExp to axes x and y (using + from ggplot2) .\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nFor each row, draw a point at coordinates defined by the mapping. You need to add a geom_ layer to your ggplot object, in this case geom_point() will do.\n\n\n\n\n\n\n\n\nWhat’s up?\n\n\n\nWe are building a graphical object (a ggplot object) around a data frame (gapminder)\nWe supply aesthetic mappings (aes()) that can be either global or bound to some geometries (geom_point())or statistics\nThe global aesthetic mapping defines which columns are\n\nmapped to which axes,\npossibly mapped to colours, linetypes, shapes, …\n\nGeometries and Statistics describe the building blocks of graphics\n\n\nWhat’s missing here?\nwhen comparing to the Gapminder demonstration, we can spot that\n\ncolors are missing\nbubble sizes are all the same. They should reflect the population size of the country\ntitles and legends are missing. This means the graphic object is useless.\n\nWe will add other layers to the graphical object to complete the plot"
  },
  {
    "objectID": "labs/lab-gapminder.html#second-attempt-display-more-information",
    "href": "labs/lab-gapminder.html#second-attempt-display-more-information",
    "title": "Data visualization",
    "section": "Second attempt: display more information",
    "text": "Second attempt: display more information\n\n\n\n\n\n\nQuestion\n\n\n\n\nMap continent to color (use aes())\nMap pop to bubble size (use aes())\nMake point transparent by tuning alpha (inside geom_point() avoid overplotting)"
  },
  {
    "objectID": "labs/lab-gapminder.html#scaling",
    "href": "labs/lab-gapminder.html#scaling",
    "title": "Data visualization",
    "section": "Scaling",
    "text": "Scaling\nIn order to pay tribute to Hans Rosling, we need to take care of two scaling issues:\n\nthe gdp per capita axis should be logarithmic scale_x_log10()\n\nthe area of the point should be proportional to the population scale_size_area()\n\n\n\n\n\n\n\n\nComplete the graphical object accordingly\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nMotivate the proposed scalings.\n\nWhy is it important to use logarithmic scaling for gdp per capita?\nWhen is it important to use logarithmic scaling on some axis (in other contexts)?\nWhy is it important to specify scale_size_area() ?"
  },
  {
    "objectID": "labs/lab-gapminder.html#in-perspective",
    "href": "labs/lab-gapminder.html#in-perspective",
    "title": "Data visualization",
    "section": "In perspective",
    "text": "In perspective\n\n\n\n\n\n\nQuestion\n\n\n\n\nAdd a plot title\nMake axes titles\n\nexplicit\nreadable\n\n\nUse labs(...)\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat should be the respective purposes of Title, Subtitle, Caption, … ?"
  },
  {
    "objectID": "labs/lab-gapminder.html#theming-using-ggthemes-or-not",
    "href": "labs/lab-gapminder.html#theming-using-ggthemes-or-not",
    "title": "Data visualization",
    "section": "Theming using ggthemes (or not)",
    "text": "Theming using ggthemes (or not)\n\nCodestopifnot(\n  require(\"ggthemes\")\n)\n\n\nA theme defines the look and feel of plots\nWithin a single document, we should use only one theme\nSee Getting the theme for a gallery of available themes\n\nCodep +\n  theme_economist()"
  },
  {
    "objectID": "labs/lab-gapminder.html#tuning-scales",
    "href": "labs/lab-gapminder.html#tuning-scales",
    "title": "Data visualization",
    "section": "Tuning scales",
    "text": "Tuning scales\n\n\n\n\n\n\nQuestion\n\n\n\nUse scale_color_manual(...) to fine tune the color aesthetic mapping.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nChoosing a color scale is a difficult task\nviridis is often a good pick."
  },
  {
    "objectID": "labs/lab-gapminder.html#zooming-on-a-continent",
    "href": "labs/lab-gapminder.html#zooming-on-a-continent",
    "title": "Data visualization",
    "section": "Zooming on a continent",
    "text": "Zooming on a continent\n\nCodezoom_continent &lt;- 'Europe'  # choose another continent at your convenience \n\n\n\n\n\n\n\n\nUse facet_zoom() from package ggforce"
  },
  {
    "objectID": "labs/lab-gapminder.html#adding-labels",
    "href": "labs/lab-gapminder.html#adding-labels",
    "title": "Data visualization",
    "section": "Adding labels",
    "text": "Adding labels\n\n\n\n\n\n\nQuestion\n\n\n\nAdd labels to points. This can be done by aesthetic mapping. Use aes(label=..)\nTo avoid text cluttering, package ggrepel offers interesting tools."
  },
  {
    "objectID": "labs/lab-gapminder.html#facetting",
    "href": "labs/lab-gapminder.html#facetting",
    "title": "Data visualization",
    "section": "Facetting",
    "text": "Facetting\nSo far we have only presented one year of data (2002)\nRosling used an animation to display the flow of time\nIf we have to deliver a printable report, we cannot rely on animation, but we can rely on facetting\nFacets are collections of small plots constructed in the same way on subsets of the data\n\n\n\n\n\n\nQuestion\n\n\n\nAdd a layer to the graphical object using facet_wrap()\n\n\n\nAs all rows in gapminder_2002 are all related to year 2002, we need to rebuild the graphical object along the same lines (using the same graphical pipeline) but starting from the whole gapminder dataset.\nShould we do this using cut and paste?\n No!!!\n\n\n\n\n\n\n\nDon’t Repeat Yoursel (DRY)\n\n\n\n\nAbide to the DRY principle using operator %+%: the ggplot2 object p can be fed with another dataframe and all you need is proper facetting."
  },
  {
    "objectID": "labs/lab-gapminder.html#animate-for-free-with-plotly",
    "href": "labs/lab-gapminder.html#animate-for-free-with-plotly",
    "title": "Data visualization",
    "section": "Animate for free with plotly\n",
    "text": "Animate for free with plotly\n\n\n\n\n\n\n\nQuestion\n\n\n\nUse plotly::ggplotly() to create a Rosling like animation.\nUse frame aesthetics."
  },
  {
    "objectID": "labs/lab-gapminder.html#more-material",
    "href": "labs/lab-gapminder.html#more-material",
    "title": "Data visualization",
    "section": "More material",
    "text": "More material\n\nRead Visualization in R for Data Science"
  },
  {
    "objectID": "labs/lab-pca.html",
    "href": "labs/lab-pca.html",
    "title": "PCA II: Swiss fertility data",
    "section": "",
    "text": "Code\nstopifnot(\n  require(broom),\n  require(DT),\n  require(GGally),\n  require(ggforce),\n  require(ggfortify),\n  require(ggvoronoi),\n  require(glue),\n  require(httr),\n  require(magrittr),\n  require(patchwork),\n  require(skimr),\n  require(tidymodels),\n  require(tidyverse)\n)\n\nold_theme &lt;- theme_set(theme_minimal())"
  },
  {
    "objectID": "labs/lab-pca.html#perform-pca-on-covariates",
    "href": "labs/lab-pca.html#perform-pca-on-covariates",
    "title": "PCA II: Swiss fertility data",
    "section": "Perform PCA on covariates",
    "text": "Perform PCA on covariates\nPairwise analysis did not provide us with a clear and simple picture of the French-speaking districts.\nPlay with centering and scaling\nProject the dataset on the first two principal components (perform dimension reduction) and build a scatterplot. Colour the points according to the value of original covariates."
  },
  {
    "objectID": "labs/lab-pca.html#compare-standardized-and-non-standardized-pca",
    "href": "labs/lab-pca.html#compare-standardized-and-non-standardized-pca",
    "title": "PCA II: Swiss fertility data",
    "section": "Compare standardized and non-standardized PCA",
    "text": "Compare standardized and non-standardized PCA\nPay attention to the correlation circles.\n\nHow well are variables represented?\nWhich variables contribute to the first axis?\n\nExplain the contrast between the two correlation circles.\nIn the sequel we focus on standardized PCA.\n\nInvestigate eigenvalues of covariance matrix\nHow many axes should we keep?\n\n\nProvide an interpretation of the first two principal axes\n\nWhich variables contribute to the two first principal axes?\n\n\nAnalyze the signs of correlations between variables and axes?\n\n\n\nAdd the Fertility variable\nPlot again the correlation circle using the same principal axes as before, but add the Fertility variable. How does Fertility relate with covariates? with principal axes?\n\n\nDisplay individuals (districts)\n\n\nComment\n\n\nBiplot"
  },
  {
    "objectID": "labs/lab-tables.html#setup",
    "href": "labs/lab-tables.html#setup",
    "title": "Tables manipulation II",
    "section": "Setup",
    "text": "Setup\nWe will use the following packages. If needed, we install them.\n\nCodeold_theme &lt;- theme_set(theme_minimal())\n\n\nCheck nycflights13 for any explanation concerning the tables and their columns."
  },
  {
    "objectID": "labs/lab-tables.html#data-loading",
    "href": "labs/lab-tables.html#data-loading",
    "title": "Tables manipulation II",
    "section": "Data loading",
    "text": "Data loading\n\nCodeflights &lt;- nycflights13::flights\nweather &lt;- nycflights13::weather\nairports &lt;- nycflights13::airports\nairlines &lt;- nycflights13::airlines\nplanes &lt;- nycflights13::planes\n\n\n\nCodecon &lt;- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\nflights_lite &lt;- copy_to(con, nycflights13::flights)\nairports_lite &lt;- copy_to(con, nycflights13::airports)\nplanes_lite &lt;-  copy_to(con, nycflights13::planes)\nweather_lite &lt;- copy_to(con, nycflights13::weather)\nairlines_lite &lt;- copy_to(con, nycflights13::airlines)\n\n\n\nCodeflights_lite %&gt;%\n  select(contains(\"delay\")) %&gt;%\n  show_query()\n\n&lt;SQL&gt;\nSELECT `dep_delay`, `arr_delay`\nFROM `nycflights13::flights`\n\n\nView data in spreadsheet style.\n\nCodeView(flights)\n\n\nAsk for help about table flights"
  },
  {
    "objectID": "labs/lab-tables.html#first-queries-the-dplyr-way",
    "href": "labs/lab-tables.html#first-queries-the-dplyr-way",
    "title": "Tables manipulation II",
    "section": "First Queries (the dplyr way)",
    "text": "First Queries (the dplyr way)\nFind all flights that\n\nHad an arrival delay of two or more hours\n\n\nFlew to Houston (IAH or HOU)\n\n\nWere operated by United, American, or Delta\n\n\n\n\n\n\n\nPackage stringr could be useful.\n\nCodeairlines %&gt;% \n  filter(stringr::str_starts(name, \"United\") |\n        stringr::str_starts(name, \"American\") |\n        stringr::str_starts(name, \"Delta\"))\n\n# A tibble: 3 × 2\n  carrier name                  \n  &lt;chr&gt;   &lt;chr&gt;                 \n1 AA      American Airlines Inc.\n2 DL      Delta Air Lines Inc.  \n3 UA      United Air Lines Inc. \n\nCodeairlines %&gt;% \n  filter(stringr::str_detect(name, (\"United|American|Delta\"))) %&gt;% \n  pluck(\"carrier\")\n\n[1] \"AA\" \"DL\" \"UA\"\n\n\n\nCodeairlines_lite %&gt;% \n  filter(stringr::str_starts(name, \"United\") |\n        stringr::str_starts(name, \"American\") |\n        stringr::str_starts(name, \"Delta\")) %&gt;% \n  show_query()\n\n\nSELECT *\nFROM `nycflights13::airlines`\nWHERE \"name\" LIKE 'United%' OR \n      \"name\" LIKE 'American%' OR \n      \"name\" LIKE 'Delta%' ;\nstringr is part of tidyverse\n\n\n\n\nDeparted in summer (July, August, and September)\n\n\n\n\n\n\n\nWhen manipulating temporal information (date, time, duration), keep an eye on what lubridate offers. The API closely parallels what RDMS and Python offer.\n\n\n\n\nArrived more than two hours late, but didn’t leave late\n\n\nWere delayed by at least an hour, but made up over 30 minutes in flight\n\n\nDeparted between midnight and 6am (inclusive)\n\n\n\n\n\n\n\nRead filter() in R for Data Science 1st Ed\nRead Chapter Transform in R for Data Science 2nd Ed"
  },
  {
    "objectID": "labs/lab-tables.html#missing-data",
    "href": "labs/lab-tables.html#missing-data",
    "title": "Tables manipulation II",
    "section": "Missing data",
    "text": "Missing data\n\nHow many flights per origin have a missing dep_time?\n\n\nWhat other variables are missing?\n\n\n\n\n\n\n\nThe introduction to tidyselect is a must read.\n\n\n\n\nWhat might these rows with missing data represent?\n\n\nCodenot_cancelled &lt;-  flights %&gt;% \n  filter(!is.na(dep_time))\n\n\n\n\nMore questions: for each column in flight report the number of missing values."
  },
  {
    "objectID": "labs/lab-tables.html#arrange",
    "href": "labs/lab-tables.html#arrange",
    "title": "Tables manipulation II",
    "section": "Arrange",
    "text": "Arrange\n\nHow could you use arrange() to sort all missing values to the start? (Hint: use is.na()).\n\n\nSort flights to find the most delayed flights.\n\n\nPick the ten most delayed flights (with finite dep_delay)\n\n\nFind the flights that left earliest.\n\n\nSort flights to find the fastest (highest speed) flights.\n\n\nWhich flights travelled the farthest?\n\n\n\n\n\n\n\nThe database provides all we need with columns distance and air_time. Otherwise, with the positions of airports from table airports, we should be able to compute distances using :\n\n‘Haversine’ formula.\n\nhttps://en.wikipedia.org/wiki/Haversine_formula\n\n\n\n\nWhich travelled the shortest?"
  },
  {
    "objectID": "labs/lab-tables.html#projection",
    "href": "labs/lab-tables.html#projection",
    "title": "Tables manipulation II",
    "section": "Projection",
    "text": "Projection\n\nBrainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.\n\n\n\nWhat happens if you include the name of a variable multiple times in a select() call?\n\n\nWhat does the any_of() function do? Why might it be helpful in conjunction with this vector?\n\nvars &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\nDoes the result of running the following code surprise you?\n\n\nCodeselect(flights, contains(\"TIME\", ignore.case =TRUE))  %&gt;% \n  head()\n\n# A tibble: 6 × 6\n  dep_time sched_dep_time arr_time sched_arr_time air_time time_hour          \n     &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;dbl&gt; &lt;dttm&gt;             \n1      517            515      830            819      227 2013-01-01 05:00:00\n2      533            529      850            830      227 2013-01-01 05:00:00\n3      542            540      923            850      160 2013-01-01 05:00:00\n4      544            545     1004           1022      183 2013-01-01 05:00:00\n5      554            600      812            837      116 2013-01-01 06:00:00\n6      554            558      740            728      150 2013-01-01 05:00:00\n\n\n\nHow do the select helpers deal with case by default?\n\n\nHow can you change that default?"
  },
  {
    "objectID": "labs/lab-tables.html#mutations",
    "href": "labs/lab-tables.html#mutations",
    "title": "Tables manipulation II",
    "section": "Mutations",
    "text": "Mutations\n\nCurrently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight.\n\n\nCompare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?\n\n\nCompare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?\n\n\nFind the 10 most delayed flights using a ranking function. How do you want to handle ties?\n\n\nCarefully read the documentation for min_rank().\nWindowed rank functions."
  },
  {
    "objectID": "labs/lab-tables.html#aggregations",
    "href": "labs/lab-tables.html#aggregations",
    "title": "Tables manipulation II",
    "section": "Aggregations",
    "text": "Aggregations\n\nBrainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. Consider the following scenarios:\n\nA flight is 15 minutes early 50% of the time, and 15 minutes late 10% of the time.\nA flight is always 10 minutes late.\nA flight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time.\n99% of the time a flight is on time. 1% of the time it’s 2 hours late.\n\n\n\n\nCodeflights %&gt;% \n  group_by(dest) %&gt;% \n  summarise(n_cancelled = sum(is.na(dep_time)))\n\n# A tibble: 105 × 2\n   dest  n_cancelled\n   &lt;chr&gt;       &lt;int&gt;\n 1 ABQ             0\n 2 ACK             0\n 3 ALB            20\n 4 ANC             0\n 5 ATL           317\n 6 AUS            21\n 7 AVL            12\n 8 BDL            31\n 9 BGR            15\n10 BHM            25\n# ℹ 95 more rows\n\n\n\nCodeflights_lite %&gt;% \n  group_by(dest) %&gt;% \n  summarise(n_cancelled = sum(is.na(dep_time))) %&gt;% \n  show_query()\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n\n&lt;SQL&gt;\nSELECT `dest`, SUM((`dep_time` IS NULL)) AS `n_cancelled`\nFROM `nycflights13::flights`\nGROUP BY `dest`\n\n\n\nWhich is more important: arrival delay or departure delay?\n\n\nCome up with another approach that will give you the same output as not_cancelled %&gt;% count(dest) and (without usingcount()`).\n\n\nOur definition of cancelled flights (is.na(dep_delay) | is.na(arr_delay) ) is slightly suboptimal. Why? Which is the most important column?\n\n\nLook at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the average delay?\n\n\nWhich carrier has the worst delays?\n\nChallenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights %&gt;% group_by(carrier, dest) %&gt;% summarise(n()))\n\nWhat does the sort argument to count() do. When might you use it?"
  },
  {
    "objectID": "labs/lab-tables.html#miscellanea",
    "href": "labs/lab-tables.html#miscellanea",
    "title": "Tables manipulation II",
    "section": "Miscellanea",
    "text": "Miscellanea\n\nWhich carriers serve all destination airports (in the table) ?\n\n\nRefer back to the lists of useful mutate and filtering functions.\nDescribe how each operation changes when you combine it with grouping.\n\n\nWhich plane (tailnum) has the worst on-time record amongst planes with at least ten flights?\n\n\nWhat time of day should you fly if you want to avoid delays as much as possible?\n\n\nFor each destination, compute the total minutes of delay.\n\n\nFor each flight, compute the proportion of the total positive arrival delays for its destination.\n\nUsing dplyr, it is easy. See A second look at group_by\n\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag(), explore how the delay of a flight is related to the delay of the immediately preceding flight.\n\n\n\n\n\n\n\nlag() is an example of window function. If we were using SQL, we would define a WINDOW using an expression like\nWINDOW w As (PARTITION BY origin ORDER BY year, month, day, sched_dep_time)\nSomething still needs fixing here: some flights never took off (is.na(dep_time)). Should they be sided out? assigned an infinite departure delay?\n\n\n\n\nLook at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?\n\nConsider all flights with average speed above \\(950\\text{km/h}\\) as suspicious.\nLet us visualize destinations and origins of the speedy flights.\n\nFind all destinations that are flown by at least two carriers. Use that information to rank the carriers.\n\n\nFor each plane, count the number of flights before the first delay greater than 1 hour.\n\n\n\n\n\n\n\nAssume a plane is characterized by tailnum. Some flights have no tailnum. We ignore them."
  },
  {
    "objectID": "labs/lab-tables.html#references",
    "href": "labs/lab-tables.html#references",
    "title": "Tables manipulation II",
    "section": "References",
    "text": "References\n\nData transformation cheatsheet\nR4Data Science Tidy\nBenchmarking\ndplyr and vctrs\nPosts on dplyr\nWindow functions on dplyr"
  },
  {
    "objectID": "labs-solutions/lab-r-intro.html#packages",
    "href": "labs-solutions/lab-r-intro.html#packages",
    "title": "R language: a tour",
    "section": "Packages",
    "text": "Packages\nBase R can do a lot. But the full power of R comes from a fast growing collection of packages.\nPackages are first installed (that is downloaded from cran and copied somewhere on the hard drive), and if needed, loaded during a session.\n\nInstallation can usually be performed using command install.packages(). In some circumstances, ad hoc installation commands (often from packages devtools) are needed\nPackage pak offers an insteresting alternative to base R install.packages()\nOnce a package has been installed/downloaded on your drive\n\nif you want all objects exported by the package to be available in your session, you should load the package, using library() or require() (what’s the difference?). Technically, this loads the NameSpace defined by the package.\nif you just want to pick some objects exported from the package, you can use qualified names like package_name::object_name to access the object (function, dataset, …).\n\n\nFor example. when we write\ngapminder &lt;- gapminder::gapminder\nwe assign dataframe/tibble gapminder from package gapminder to identifier \"gapminder\" in global environment .\nFunction p_load() from pacman (package manager) blends installation and loading: if the package named in the argument of p_load() is not installed (not among the installed.packages()), p_load() attempts to install the package. If installation is successful, the package is loaded.\n\nif (! require(pak)){\n  install.packages(\"pak\")\n}\n\n\nto_be_loaded &lt;- c(\"devtools\",\n                  \"tidyverse\", \n                  \"lobstr\",\n                  \"ggforce\",\n                  \"nycflights13\",\n                  \"patchwork\", \n                  \"glue\",\n                  \"DT\", \n                  \"kableExtra\",\n                  \"viridis\")\n\nfor (pck in to_be_loaded) {\n  if (!require(pck, character.only = T)) {\n    pak::pkg_install(pck, repos=\"http://cran.rstudio.com/\")\n    stopifnot(require(pck, character.only = T))\n  }  \n}\n\n\n\n\n\n\n\nOptional arguments\n\n\n\nA very nice feature of R is that functions from base R as well as from packages have optional arguments with sensible default values. Look for example at documentation of require() using expression ?require.\nOptional settings may concern individual functions or the collection of functions exported by some packages. In the next chunk, we reset the default color scales used by graphical functions from ggplot2.\n\nopts &lt;- options()  # save old options\n\noptions(ggplot2.discrete.colour=\"viridis\")\noptions(ggplot2.continuous.colour=\"viridis\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou shall not confuse installing (on your hard-drive) and loading (in session) a package.\n\n\n\n\n\n\n\n\nQuestion for Pythonistas\n\n\n\n\nIn  what is the analogue of install.packages()?\nIn  what is the analogue of require()/library()?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nIn , you can install a package pck using pip install pck or conda install pck (for example).\nIn , the analogue of require(pck) could be\nfrom pck import *\nNote that in R, once a package in installed on the hard drive, you do not need to write something like\nimport pck\nto be able to use objects exported by pck using qualified names (like pck.ze_object), you just need to use R qualified names:\npck::ze_object"
  },
  {
    "objectID": "labs-solutions/lab-r-intro.html#vector-creation-and-assignment",
    "href": "labs-solutions/lab-r-intro.html#vector-creation-and-assignment",
    "title": "R language: a tour",
    "section": "Vector creation and assignment",
    "text": "Vector creation and assignment\nThe next three lines create three numerical atomic vectors.\nIn IDE Rstudio, have a look at the environment pane on the right before running the chunk, and after.\nUse ls() to investigate the environment before and after the execution of the three assignments.\n\nls()\nx &lt;- c(1, 2, 12)\ny &lt;- 5:7\nz &lt;- 10:1\nx ; y ; z \nls()\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat are the identifiers known in the global environment before execution of lines 2-4?\nWhat are the identifiers known in the global environment after execution of lines 2-4?\nWhich objects are attached to identifiers x, y, and z?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe chunks adds three identifiers x,y,z to the global environment. Identifiers are bound to R objects which turn out to be numerical vectors.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat does the next chunk?\n\nls()\nw &lt;- y\nls()\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe chunk inserts a new identifier w in the global environment. This identifier is associated with the same object as y.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nIs the content of object denoted by y copied to a new object bound to w?\nInterpret the result of w == y.\nInterpret the result of identical(w,y) (use help(\"identical\") if needed).\n\n\nw == y \nidentical(w,y)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPackage lobstr lets us explore low-level aspects of R (and much more). Function lobstr::obj_addr() returns the address of the object denoted by the argument.\n\nlobstr::obj_addr(w)\n\n[1] \"0x5689b87f2c18\"\n\nlobstr::obj_addr(y)\n\n[1] \"0x5689b87f2c18\"\n\n\nNow, if we modify either y or w\n\ny &lt;- y + 1\nidentical(y, w)\n\n[1] FALSE\n\nc(lobstr::obj_addr(w), lobstr::obj_addr(y))\n\n[1] \"0x5689b87f2c18\" \"0x5689b814ce98\"\n\n\nThe adress associated with y has changed!\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe meaning of assignment in R differs from its countrepart in Python"
  },
  {
    "objectID": "labs-solutions/lab-r-intro.html#indexation-slicing-modification",
    "href": "labs-solutions/lab-r-intro.html#indexation-slicing-modification",
    "title": "R language: a tour",
    "section": "Indexation, slicing, modification",
    "text": "Indexation, slicing, modification\nSlicing a vector can be done in two ways:\n\nproviding a vector of indices to be selected. Indices need not be consecutive\nproviding a Boolean mask, that is a logical vector to select a set of positions\n\n\nx &lt;- c(1, 2, 12) ; y &lt;- 5:7 ; z &lt;- 10:1\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next lines\n\nz[1]   # slice of length 1\nz[0]   # What did you expect?\nz[x]   # slice of length ??? index error ?\nz[y]\nz[x %% 2]   # what happens with x[0] ?\nz[0 == (x %% 2)] # masking\nz[c(2, 1, 1)]\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nIndices start at 1 (not like in C, Java, or Python)\nz[0] does not return an Error message. It returns an empty vector with the same basetype as x\nz[x] returns a vector made of z[x[1]], z[x[2]] and z[x[3]]==z[12]. Note again that z[12] does not raise an exception. It is simply not available (NA).\nx %% 2 returns 1 0 0 as %% stands for mod. z[x %% 2] returns the same thing as z[1]\n\nc( ) stands for combine, or concatenate.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIf the length of mask and and the length of the sliced vector do not coincide, what happens?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNo error is signalled, the returned sequence is as long as the number of truthies in the mask.\nOut of bound truthies show up as NA\n\nz[rep(c(TRUE, FALSE), 6)]\n\n[1] 10  8  6  4  2 NA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA scalar is just a vector of length \\(1\\)!\n\nclass(z)\n\n[1] \"integer\"\n\nclass(z[1])\n\n[1] \"integer\"\n\nclass(z[c(2,1)])\n\n[1] \"integer\"\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next lines\n\ny[2:3] &lt;- z[2:3]\ny == z[-10]\n\nz[-11]\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can assign a slice of a vector to a slice of identical size of another vector.\nWhat is the result of z[-11], z[-c(11:7)]?\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next line\n\nz[-(1:5)]\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe pick all positions in z but the ones in 1:5, that is 6, 7, 8, 9, 10\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you select the last element from a vector (say z)?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nz[length(z)]\n\n[1] 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n is not  (reminder)!\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nReverse the entries of a vector. Find two ways to do that.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nz[seq(length(z), 1, by=-1)]\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nz[length(z):1]\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nrev(z)   # the simplest way, once you know rev()\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\n\nIn statistics, machine learning, we are often faced with the task of building grid of regularly spaced elements (these elements can be numeric or not). R offers a collection of tools to perform this. The most basic tool is rep().\n\n\n\n\n\n\nQuestion\n\n\n\n\nRepeat a vector \\(2\\) times\nRepeat each element of a vector twice\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nw &lt;- c(1, 7, 9)\nrep(w, 2)\n\n[1] 1 7 9 1 7 9\n\nrep(w, rep(2, length(w)))\n\n[1] 1 1 7 7 9 9\n\n\nNow, we can try something more fancy.\n\nrep(w, 1:3)\n\n[1] 1 7 7 9 9 9\n\n\nWhat are the requirements on the second (times) argument?\n\n\n\nLet us remove objects from the global environment.\n\nrm(w, x, y ,z)"
  },
  {
    "objectID": "labs-solutions/lab-r-intro.html#numbers",
    "href": "labs-solutions/lab-r-intro.html#numbers",
    "title": "R language: a tour",
    "section": "Numbers",
    "text": "Numbers\nSo far, we told about numeric vectors. Numeric vectors are vectors of floating point numbers. R distinguishes several kinds of numbers.\n\nIntegers\nFloating point numbers (double)\n\nTo check whether a vector is made of numeric or of integer, use is.numeric() or is.integer(). Use as.integer, as.numeric() to enforce type conversion.\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the outcome of the next chunks\n\nclass(113L) ; class(113) ; class(113L + 113) ; class(2 * 113L) ; class(pi) ; as.integer(pi)\n\n[1] \"integer\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] \"numeric\"\n\n\n[1] 3\n\n\n\nclass(as.integer(113))\n\n[1] \"integer\"\n\n\n\npi ; class(pi)\n\n[1] 3.141593\n\n\n[1] \"numeric\"\n\n\n\nfloor(pi) ; class(floor(pi)) # mind the floor\n\n[1] 3\n\n\n[1] \"numeric\""
  },
  {
    "objectID": "labs-solutions/lab-r-intro.html#integer-arithmetic",
    "href": "labs-solutions/lab-r-intro.html#integer-arithmetic",
    "title": "R language: a tour",
    "section": "Integer arithmetic",
    "text": "Integer arithmetic\n\n29L * 31L ; 899L %/% 32L ; 899L %% 30L\n\n[1] 899\n\n\n[1] 28\n\n\n[1] 29\n\n\n\n\n\n\n\n\nCaution\n\n\n\nR integers are not the natural numbers from Mathematics\nR numerics are not the real numbers from Mathematics\n\n.Machine$double.eps\n\n[1] 2.220446e-16\n\n.Machine$double.xmax\n\n[1] 1.797693e+308\n\n.Machine$sizeof.longlong\n\n[1] 8\n\nu &lt;- double(19L)\nv &lt;- numeric(5L)\nw &lt;- integer(7L)\nlapply(list(u, v, w), typeof)\n\n[[1]]\n[1] \"double\"\n\n[[2]]\n[1] \"double\"\n\n[[3]]\n[1] \"integer\"\n\nlength(c(u, v, w))\n\n[1] 31\n\ntypeof(c(u, v, w))\n\n[1] \"double\"\n\n\n\n\nR is (sometimes) able to make sensible use of Infinite.\n\nlog(0)\n\n[1] -Inf\n\nlog(Inf)\n\n[1] Inf\n\n1/0\n\n[1] Inf\n\n0/0\n\n[1] NaN\n\nmax(c( 0/0,1,10))\n\n[1] NaN\n\nmax(c(NA,1,10))\n\n[1] NA\n\nmax(c(-Inf,1,10))\n\n[1] 10\n\nis.finite(c(-Inf,1,10))\n\n[1] FALSE  TRUE  TRUE\n\nis.na(c(NA,1,10))\n\n[1]  TRUE FALSE FALSE\n\nis.nan(c(NaN,1,10))\n\n[1]  TRUE FALSE FALSE"
  },
  {
    "objectID": "labs-solutions/lab-r-intro.html#computing-with-vectors",
    "href": "labs-solutions/lab-r-intro.html#computing-with-vectors",
    "title": "R language: a tour",
    "section": "Computing with vectors",
    "text": "Computing with vectors\nSumming, scalar multiplication\n\nx &lt;- 1:3\ny &lt;- 9:7\n\nsum(x) ; prod(x)\n\n[1] 6\n\n\n[1] 6\n\nz &lt;- cumsum(1:3)\nw &lt;- cumprod(3:5)\n\nx + y\n\n[1] 10 10 10\n\nx + z\n\n[1] 2 5 9\n\n2 * w\n\n[1]   6  24 120\n\n2 + w\n\n[1]  5 14 62\n\nw / 2\n\n[1]  1.5  6.0 30.0\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you compute a factorial?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nn &lt;- 10\ncumprod(1:n)\n\n [1]       1       2       6      24     120     720    5040   40320  362880\n[10] 3628800\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nApproximate \\(\\sum_{n=1}^\\infty 1/n^2\\) within \\(10^{-3}\\)?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\\sum_{n &gt; N} \\frac{1}{n^2} &lt; \\sum_{n &gt; N} \\frac{1}{n(n-1)} = \\sum_{n &gt; N} \\left(\\frac{1}{n-1}-\\frac{1}{n}\\right) = \\frac{1}{N}\\] So we may pick \\(N=1000\\).\n\nsum(x*y) # inner product\n\n[1] 46\n\nprod(1:5) # factorial(n) as prod(1:n)\n\n[1] 120\n\nN &lt;- 1000L\nsum(1/((1:N)^2)) ; pi^2/6 # grand truth\n\n[1] 1.643935\n\n\n[1] 1.644934\n\n(pi^2/6 - sum(1/((1:N)^2))) &lt; 1e-3\n\n[1] TRUE\n\n# N &lt;- 999L\n# (pi^2/6 - sum(1/((1:N)^2))) &lt; 1e-3\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you compute the inner product between two (atomic numeric) vectors?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nInner product between two vectors can be computed as a matrix product between a row vector and a column vector using %*%. Is this a good idea.\n\nmatrix(w, ncol=3) %*% matrix(y, nrow=3) == sum(w * y)\n\n     [,1]\n[1,] TRUE\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat we have called vectors so far are indeed atomic vectors.\n\nRead Chapter on Vectors in R advanced Programming\nKeep an eye on package vctrs for getting insights into the R vectors."
  },
  {
    "objectID": "labs-solutions/lab-r-intro.html#creation-transposition-and-reshaping",
    "href": "labs-solutions/lab-r-intro.html#creation-transposition-and-reshaping",
    "title": "R language: a tour",
    "section": "Creation, transposition and reshaping",
    "text": "Creation, transposition and reshaping\nA vector can be turned into a column matrix.\n\nv &lt;- as.matrix(1:5)\nv\n\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n[4,]    4\n[5,]    5\n\n\nA matrix can be transposed\n\nt(v)  # transpose \n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n\ncat(dim(v), ' ', dim(t(v)), '\\n')\n\n5 1   1 5 \n\n\n\nA &lt;- matrix(1, nrow=5, ncol=2) ; A\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n[3,]    1    1\n[4,]    1    1\n[5,]    1    1\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nlobstr::mem_used() allows us to keep track of the amount of memory used by our R session. lobstr::obj_size() tells us the amount of memory used by the representation of an object.\nComment the next chunk\n\nm1 &lt;-lobstr::mem_used()\nA &lt;- matrix(rnorm(100000L), nrow=1000L)\nm2 &lt;- lobstr::mem_used()\nlobstr::obj_size(A)\n\n800,216 B\n\nB &lt;- t(A)\nlobstr::obj_size(B)\n\n800,216 B\n\nm3 &lt;- lobstr::mem_used()\nm2-m1 ; m3-m2\n\n796,080 B\n\n\n986,984 B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nIs there a difference between the next two assignments?\nHow would you assign value to all entries of a matrix?\n\n\nA &lt;- matrix(rnorm(16), nrow=4)\nA[] &lt;- 0 ; A\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    0    0    0    0\n\nA   &lt;- 0 ; A\n\n[1] 0\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere is!\nThe first assignment assigns 0 to every entry in A.\nThe second assignment binds 0 to name A\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the final shape of A?\n\nA &lt;- matrix(1, nrow=5, ncol=2) \nA\nA[] &lt;- 1:15 \nA\n\n\n\nWe can easily generate diagonal matrices and constant matrices.\n\ndiag(1, 3)  # building identity matrix\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\nmatrix(0, 3, 3) # building null matrix\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0    0\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs there any difference between the next two assignments?\n\nB &lt;- A[]\nB ; A\n\n[1] 0\n\n\n[1] 0\n\nlobstr::obj_addr(B) ; lobstr::obj_addr(A)\n\n[1] \"0x5689b8e2f3c8\"\n\n\n[1] \"0x5689b9c15940\"\n\nB &lt;- A"
  },
  {
    "objectID": "labs-solutions/lab-r-intro.html#indexation-slicing-modification-1",
    "href": "labs-solutions/lab-r-intro.html#indexation-slicing-modification-1",
    "title": "R language: a tour",
    "section": "Indexation, slicing, modification",
    "text": "Indexation, slicing, modification\nIndexation consists in getting one item from a vector/list/matrix/array/dataframe.\nSlicing and subsetting consists in picking a substructure:\n\nsubsetting a vector returns a vector\nsubsetting a list returns a list\nsubsetting a matrix/array returns a matrix/array (beware of implicit simplifications and dimension dropping)\nsubsetting a dataframe returns a dataframe or a vector (again, beware of implicit simplifications).\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplain the next results\n\nA &lt;- matrix(1, nrow=5, ncol=2)\n\ndim(A[sample(5, 3), -1])\ndim(A[sample(5, 3), 1])\nlength(A[sample(5, 3), 1])\nis.vector(A[sample(5, 3), 1])\nA[10:15]\nA[60]\ndim(A[])\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nNULL\n\n\nNULL\n\n\n[1] 3\n\n\n[1] TRUE\n\n\n[1]  1 NA NA NA NA NA\n\n\n[1] NA\n\n\n[1] 5 2\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you create a fresh copy of a matrix?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nA &lt;- matrix(rnorm(10), ncol=2L)\nB &lt;- matrix(0, nrow=5L, ncol=2L)\n\nB[] &lt;- A\nall(B==A) ; identical(A, B) ; lobstr::obj_addrs(list(A, B))\n\n[1] TRUE\n\n\n[1] TRUE\n\n\n[1] \"0x5689b9a902a8\" \"0x5689b9a90ae8\""
  },
  {
    "objectID": "labs-solutions/lab-r-intro.html#computing-with-matrices",
    "href": "labs-solutions/lab-r-intro.html#computing-with-matrices",
    "title": "R language: a tour",
    "section": "Computing with matrices",
    "text": "Computing with matrices\n\n* versus %*%\n\n%*% stands for matrix multiplication. In order to use it, the two matrices should have conformant dimensions.\n\n\n\nt(v) %*% A\n\n          [,1]      [,2]\n[1,] -5.973767 -5.241166\n\n\nThere are a variety of reasonable products around. Some of them are available in R.\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you compute the Hilbert-Schmidt inner product between two matrices?\n\\[\\langle A, B\\rangle_{\\text{HS}} = \\text{Trace} \\big(A \\times B^\\top\\big)\\]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn R, trace() does not return the trace of a matrix! Function is used for debugging.\nJust remember that the trace of a matrix is the sum of its diagonal elements.\n\nA &lt;- matrix(runif(6), 2, 3)\nB &lt;- matrix(runif(6), 2, 3)\nfoo &lt;- sum(diag(A %*% t(B)))\nbar &lt;- sum(A * B)\nfoo ; bar\n\n[1] 0.325935\n\n\n[1] 0.325935\n\n\nAre you surprised?\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow can you invert a square (invertible) matrix?\n\n\nUse solve(A) which is a shorthand for solve(A, diag(1, nrow(3)))."
  },
  {
    "objectID": "labs-solutions/lab-r-intro.html#handling-three-valued-logic",
    "href": "labs-solutions/lab-r-intro.html#handling-three-valued-logic",
    "title": "R language: a tour",
    "section": "Handling three-valued logic",
    "text": "Handling three-valued logic\n\n\n\n\n\n\nQuestion\n\n\n\n\nTRUE &  (1&gt; (0/0))\n(1&gt; (0/0)) | TRUE\n(1&gt; (0/0)) | FALSE\nTRUE || (1&gt; (0/0))\nTRUE |  (1&gt; (0/0))\nTRUE || stopifnot(4&lt;3)\n# TRUE |  stopifnot(4&lt;3)  \nFALSE && stopifnot(4&lt;3)\n# FALSE & stopifnot(4&lt;3)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nTRUE &  (1&gt; (0/0))\n\n[1] NA\n\n(1&gt; (0/0)) | TRUE\n\n[1] TRUE\n\n(1&gt; (0/0)) | FALSE\n\n[1] NA\n\nTRUE || (1&gt; (0/0))\n\n[1] TRUE\n\nTRUE |  (1&gt; (0/0))\n\n[1] TRUE\n\nTRUE || stopifnot(4&lt;3)\n\n[1] TRUE\n\n# TRUE |  stopifnot(4&lt;3)  \nFALSE && stopifnot(4&lt;3)\n\n[1] FALSE\n\n# FALSE & stopifnot(4&lt;3)\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the difference between logical operators || and | ?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n|| is lazy. It does not evaluate its second argument if the first one evaluates to TRUE.\n&& is also lazy.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemark: favor &, | over &&, ||."
  },
  {
    "objectID": "labs-solutions/lab-r-intro.html#all-and-any",
    "href": "labs-solutions/lab-r-intro.html#all-and-any",
    "title": "R language: a tour",
    "section": "all and any",
    "text": "all and any\nLook at the definition of all and any.\n\n\n\n\n\n\nQuestion\n\n\n\n\nHow would you check that a square matrix is symmetric?\nHow would you check that a matrix is diagonal?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA square matrix is symmteric iff it is equal to its transpose. Recall that t(A) denotes the transpose of matrix A.\n\nA &lt;- matrix(rnorm(9), nrow=3, ncol=3) # a.s. non-symmetric\nall(A == t(A))\n\n[1] FALSE\n\nA &lt;- A %*% t(A)  # build a symmetric matrix, A + t(A) would work also\nall(A == t(A))\n\n[1] TRUE\n\n\nA == t(A) returns a matrix a logical matrix, whose entries are all TRUE iff A is symmetric.\nall() works for matrices as well as for vectors. This is sensible as matrices can be considered as vectors with some additional structure."
  },
  {
    "objectID": "labs-solutions/lab-r-intro.html#if-then-else",
    "href": "labs-solutions/lab-r-intro.html#if-then-else",
    "title": "R language: a tour",
    "section": "If () then {} else {}",
    "text": "If () then {} else {}\nIf expressions yes_expr and no_expr are complicated it makes sense to use the if (...) {...} else {...} construct\nThere is also a conditional statement with an optional else {}\n#| eval: false\n#| collapse: false\nif (condition) {\n  ...\n} else {\n  ...\n}\n\n\n\n\n\n\nQuestion\n\n\n\nIs there an elif construct in R?\n\n\nNope!\n R also offers a switch\nswitch (object,\n  case1 = {action1}, \n  case2 = {action2}, \n  ...\n)\n\n\n\n\n\n\nNote\n\n\n\nThere exists a selection function ifelse(test, yes_expr, no_expr).\n\nifelse(test, yes, no)\n\nNote that ifelse(...) is vectorized.\n\nx &lt;-  1L:6L\ny &lt;-  rep(\"odd\", 6)\nz &lt;- rep(\"even\", 6)\n\nifelse(x %% 2L, y, z)\n\n[1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n\n\n This is a vectorized function"
  },
  {
    "objectID": "labs-solutions/lab-r-intro.html#iterations-for-it-in-iterable-...",
    "href": "labs-solutions/lab-r-intro.html#iterations-for-it-in-iterable-...",
    "title": "R language: a tour",
    "section": "Iterations for (it in iterable) {...}",
    "text": "Iterations for (it in iterable) {...}\nHave a look at Iteration section in R for Data Science\n\n\n\n\n\n\nQuestion\n\n\n\nCreate a lower triangular matrix which represents the 5 first lines of the Pascal triangle.\n\n\nRecall\n\\[\\binom{n}{k} = \\binom{n-1}{k-1} + \\binom{n-1}{k}\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nT &lt;- matrix(0L, nrow=6, ncol=6)\nT[1,1] &lt;- 1L\n\nfor (i in 2:ncol(T))\n  T[i, 1:i] &lt;- c(0L, T[i-1, 2:i-1]) + T[i-1, 1:i]\n\ncolnames(T) &lt;- 0L:5L\nrownames(T) &lt;- 0L:5L\n\nT\n\n  0 1  2  3 4 5\n0 1 0  0  0 0 0\n1 1 1  0  0 0 0\n2 1 2  1  0 0 0\n3 1 3  3  1 0 0\n4 1 4  6  4 1 0\n5 1 5 10 10 5 1\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nLocate the smallest element in a numerical vector\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nv &lt;- sample(1:100, 100)\nv[1:10]\n\n [1] 70 38 87 21 54 22 26 98 49 50\n\npmin &lt;- 1\n\nfor (i in seq_along(v)) {\n  if (v[i]&lt;v[pmin]) {\n    pmin &lt;- i\n  }\n}\n\nprint(stringr::str_c('minimum is at ', pmin, ', it is equal to ', v[pmin]))\n\n[1] \"minimum is at 71, it is equal to 1\"\n\n\nThere are some redundant braces {}"
  },
  {
    "objectID": "labs-solutions/lab-r-intro.html#while-condition",
    "href": "labs-solutions/lab-r-intro.html#while-condition",
    "title": "R language: a tour",
    "section": "While (condition) {…}",
    "text": "While (condition) {…}\n\n\n\n\n\n\nQuestion\n\n\n\nFind the location of the minimum in a vector v\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nv &lt;- sample(100, 100)\n\npmin &lt;- 1   # Minimum in v[1:1]\ni &lt;- 2\n\nwhile (i &lt;= length(v)) {\n  # loop invariant: v[pmin] == min(v[1:i])\n  if (v[i]&lt;v[pmin]) {\n    pmin &lt;- i\n  }\n  i &lt;- i + 1\n}\n\nprint(stringr::str_c('minimum is at ', pmin, ', it is equal to ', v[pmin]))\n\n[1] \"minimum is at 69, it is equal to 1\"\n\nwhich.min(v); v[which.min(v)]\n\n[1] 69\n\n\n[1] 1\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a loop that checks whether vector v is non-decreasing.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nresult &lt;- TRUE\n\nfor (i in 2:length(v))\n  if (v[i] &lt; v[i-1]) {\n    result &lt;- FALSE\n    break\n  }\n\nif (result) {\n  print(\"non-decreasing\")\n} else {\n  print(\"not non-decreasing\")\n}\n\n[1] \"not non-decreasing\""
  },
  {
    "objectID": "labs-solutions/lab-r-intro.html#operators-purrrmap_",
    "href": "labs-solutions/lab-r-intro.html#operators-purrrmap_",
    "title": "R language: a tour",
    "section": "Operators purrr::map_???",
    "text": "Operators purrr::map_???\n\n\n\n\n\n\nQuestion\n\n\n\nWrite truth tables for &, |, &&, ||, ! and xor\nHint: use purrr::map, function outer()\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nvals &lt;- c(TRUE, FALSE, NA)\nops &lt;- c(`&`, `|`, `xor`)\n\ntruth &lt;- purrr::map(ops, \\(x) outer(vals,vals, x))\n\nnames(truth) &lt;- (ops)\ntruth\n\n$`.Primitive(\"&\")`\n      [,1]  [,2]  [,3]\n[1,]  TRUE FALSE    NA\n[2,] FALSE FALSE FALSE\n[3,]    NA FALSE    NA\n\n$`.Primitive(\"|\")`\n     [,1]  [,2] [,3]\n[1,] TRUE  TRUE TRUE\n[2,] TRUE FALSE   NA\n[3,] TRUE    NA   NA\n\n$`function (x, y) \\n{\\n    (x | y) & !(x & y)\\n}`\n      [,1]  [,2] [,3]\n[1,] FALSE  TRUE   NA\n[2,]  TRUE FALSE   NA\n[3,]    NA    NA   NA\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a function that takes as input a square matrix and returns TRUE if it is lower triangular.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlt &lt;- function(A){\n  n &lt;- nrow(A)\n  all(purrr::map_lgl(1:(n-1), \\(x) all(0== A[x, (.+1):n])))\n}\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nUse map , choose and proper use of pronouns to deliver the n first lines of the Pascal triangle using one line of code.\nAs far as the total number of operations is concerned, would you recommend this way of computing the Pascal triangle?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nn &lt;- 5\n\ntp5 &lt;- matrix(unlist(map(0:n,\n           \\(x) c(choose(x, 0:x), rep(0L, n-x)))),\n       nrow=n+1,\n       byrow=T)\n\nrownames(tp5) &lt;- 0:n\n\ncolnames(tp5) &lt;- 0:n\n\ntp5\n\n  0 1  2  3 4 5\n0 1 0  0  0 0 0\n1 1 1  0  0 0 0\n2 1 2  1  0 0 0\n3 1 3  3  1 0 0\n4 1 4  6  4 1 0\n5 1 5 10 10 5 1\n\n\nNo. Using map and choose, we do not reuse previous computations. The total number of arithmetic operations is \\(\\Omega(n^3)\\), it should be \\(O(n^2)\\).\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nRead Chapter on Functional Programming in Advanced R"
  },
  {
    "objectID": "labs-solutions/lab-gapminder.html#grammar-of-graphics",
    "href": "labs-solutions/lab-gapminder.html#grammar-of-graphics",
    "title": "Data visualization",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nWe will use the Grammar of Graphics approach to visualization\nThe expression Grammar of Graphics was coined by Leiland Wilkinson to describe a principled approach to visualization in Data Analysis (EDA)\nA plot is organized around tabular data (a table with rows (observations) and columns (variables))\nA plot is a graphical object that can be built layer by layer\nBuilding a graphical object consists in chaining elementary operations\nThe acclaimed TED presentation by Hans Rosling illustrates the Grammar of Graphics approach\n\nWe will reproduce the animated demonstration using\n\n\nggplot2: an implementation of grammar of graphics in `R\n\nplotly: a bridge between R and the javascript library D3.js\n\nUsing plotly, opting for html ouput, brings the possibility of interactivity and animation"
  },
  {
    "objectID": "labs-solutions/lab-gapminder.html#setup",
    "href": "labs-solutions/lab-gapminder.html#setup",
    "title": "Data visualization",
    "section": "Setup",
    "text": "Setup\nWe will use the following packages. If needed, we install them.\n\nCodestopifnot(\n  require(tidyverse), \n  require(patchwork), \n  require(glue), \n  require(ggforce), \n  require(plotly),\n  require(ggthemes),\n  require(gapminder),\n  require(ggrepel)\n)\n\n\nThe data we will use can be obtained by loading package gapminder\n\n\n\n\n\n\nTip\n\n\n\nIf the packages have not yet been installed on your hard drive, install them.\nYou can do that using base R install.packages() function:\ninstall.packages(\"tidyverse\")\nIt is often faster to use functions from package pak\ninstall.packages(\"pak\")\npak::pkg_install(\"tidyverse\")\n\n\nYou need to understand the difference between installing and loading a package\n\n\n\n\n\n\nQuestion\n\n\n\n\nHow do we get the list of installed packages?\nHow do we get the list of loaded packages?\nWhich objects are made available by a package?\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\nThe (usually very long) list of installed packages can be obtained by a simple function call.\n\nCodedf &lt;- installed.packages()\nhead(df)\n##             Package       LibPath                                            \n## abind       \"abind\"       \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n## arkhe       \"arkhe\"       \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n## arrow       \"arrow\"       \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n## ash         \"ash\"         \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n## AsioHeaders \"AsioHeaders\" \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n## askpass     \"askpass\"     \"/home/boucheron/R/x86_64-pc-linux-gnu-library/4.4\"\n##             Version    Priority Depends       \n## abind       \"1.4-5\"    NA       \"R (&gt;= 1.5.0)\"\n## arkhe       \"1.6.0\"    NA       \"R (&gt;= 3.5)\"  \n## arrow       \"16.1.0\"   NA       \"R (&gt;= 4.0)\"  \n## ash         \"1.0-15\"   NA       NA            \n## AsioHeaders \"1.22.1-2\" NA       NA            \n## askpass     \"1.2.0\"    NA       NA            \n##             Imports                                                                                                                \n## abind       \"methods, utils\"                                                                                                       \n## arkhe       \"graphics, methods, stats, utils\"                                                                                      \n## arrow       \"assertthat, bit64 (&gt;= 0.9-7), glue, methods, purrr, R6, rlang\\n(&gt;= 1.0.0), stats, tidyselect (&gt;= 1.0.0), utils, vctrs\"\n## ash         NA                                                                                                                     \n## AsioHeaders NA                                                                                                                     \n## askpass     \"sys (&gt;= 2.1)\"                                                                                                         \n##             LinkingTo         \n## abind       NA                \n## arkhe       NA                \n## arrow       \"cpp11 (&gt;= 0.4.2)\"\n## ash         NA                \n## AsioHeaders NA                \n## askpass     NA                \n##             Suggests                                                                                                                                                                                                            \n## abind       NA                                                                                                                                                                                                                  \n## arkhe       \"tinytest\"                                                                                                                                                                                                          \n## arrow       \"blob, curl, cli, DBI, dbplyr, decor, distro, dplyr, duckdb\\n(&gt;= 0.2.8), hms, jsonlite, knitr, lubridate, pillar, pkgload,\\nreticulate, rmarkdown, stringi, stringr, sys, testthat (&gt;=\\n3.1.0), tibble, tzdb, withr\"\n## ash         NA                                                                                                                                                                                                                  \n## AsioHeaders NA                                                                                                                                                                                                                  \n## askpass     \"testthat\"                                                                                                                                                                                                          \n##             Enhances License                   License_is_FOSS\n## abind       NA       \"LGPL (&gt;= 2)\"             NA             \n## arkhe       NA       \"GPL (&gt;= 3)\"              NA             \n## arrow       NA       \"Apache License (&gt;= 2.0)\" NA             \n## ash         NA       \"GPL (&gt;= 2)\"              NA             \n## AsioHeaders NA       \"BSL-1.0\"                 NA             \n## askpass     NA       \"MIT + file LICENSE\"      NA             \n##             License_restricts_use OS_type MD5sum NeedsCompilation Built  \n## abind       NA                    NA      NA     \"no\"             \"4.4.0\"\n## arkhe       NA                    NA      NA     \"no\"             \"4.4.0\"\n## arrow       NA                    NA      NA     \"yes\"            \"4.4.0\"\n## ash         NA                    NA      NA     \"yes\"            \"4.4.0\"\n## AsioHeaders NA                    NA      NA     \"no\"             \"4.4.0\"\n## askpass     NA                    NA      NA     \"yes\"            \"4.4.0\"\n\n\nNote that the output is tabular (it is a matrix and an array) that contains much more than the names of installed packages. If we just want the names of the installed packages, we can extract the column named Package.\n\nCodedf[1:5, c(\"Package\", \"Version\") ]\n##             Package       Version   \n## abind       \"abind\"       \"1.4-5\"   \n## arkhe       \"arkhe\"       \"1.6.0\"   \n## arrow       \"arrow\"       \"16.1.0\"  \n## ash         \"ash\"         \"1.0-15\"  \n## AsioHeaders \"AsioHeaders\" \"1.22.1-2\"\n\n\nMatrices and arrays represent mathematical object and are fit for computations. They are not so convenient as far as querying is concerned. Dataframes which are also tabular objects can be queried like tables in a relational database.\nLoading a package amounts to make a number of objects available in the current session. The objects are made available though Namespaces.\n\nCodeloadedNamespaces()\n##  [1] \"methods\"     \"graphics\"    \"plotly\"      \"utf8\"        \"generics\"   \n##  [6] \"tidyr\"       \"stringi\"     \"hms\"         \"digest\"      \"magrittr\"   \n## [11] \"evaluate\"    \"grid\"        \"timechange\"  \"grDevices\"   \"fastmap\"    \n## [16] \"jsonlite\"    \"ggrepel\"     \"tidyverse\"   \"ggthemes\"    \"httr\"       \n## [21] \"purrr\"       \"fansi\"       \"viridisLite\" \"scales\"      \"tweenr\"     \n## [26] \"codetools\"   \"lazyeval\"    \"cli\"         \"rlang\"       \"polyclip\"   \n## [31] \"munsell\"     \"withr\"       \"utils\"       \"yaml\"        \"stats\"      \n## [36] \"tools\"       \"base\"        \"tzdb\"        \"dplyr\"       \"colorspace\" \n## [41] \"ggplot2\"     \"forcats\"     \"vctrs\"       \"R6\"          \"lifecycle\"  \n## [46] \"lubridate\"   \"stringr\"     \"htmlwidgets\" \"MASS\"        \"pkgconfig\"  \n## [51] \"pillar\"      \"gtable\"      \"glue\"        \"data.table\"  \"Rcpp\"       \n## [56] \"ggforce\"     \"xfun\"        \"tibble\"      \"tidyselect\"  \"knitr\"      \n## [61] \"farver\"      \"datasets\"    \"gapminder\"   \"htmltools\"   \"patchwork\"  \n## [66] \"rmarkdown\"   \"readr\"       \"compiler\"\n\n\nNote that we did not load explicitly some of the loadedNamespaces. Many of the loaded packages were loaded while loading other packages, for example metapackages like tidyverse."
  },
  {
    "objectID": "labs-solutions/lab-gapminder.html#have-a-look-at-gapminder-dataset",
    "href": "labs-solutions/lab-gapminder.html#have-a-look-at-gapminder-dataset",
    "title": "Data visualization",
    "section": "Have a look at gapminder dataset",
    "text": "Have a look at gapminder dataset\nThe gapminder table can be found at gapminder::gapminder\n\nA table has a schema: a list of named columns, each with a given type\nA table has a content: rows. Each row is a collection of items, corresponding to the columns\n\n\n\n\n\n\n\nQuestion\n\n\n\nExplore gapminder::gapminder, using glimpse() and head()\n\n\nglimpse() allows to see the schema and the first rows\n\nhead() allows to see the first rows\nUse the pipe |&gt; to chain operations\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\nDataframes\n\nCodegapminder &lt;- gapminder::gapminder\n\nglimpse(gapminder)\n## Rows: 1,704\n## Columns: 6\n## $ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n## $ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …\n## $ lifeExp   &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8…\n## $ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…\n## $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, …\n\ngapminder |&gt;  \n  glimpse()\n## Rows: 1,704\n## Columns: 6\n## $ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n## $ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …\n## $ lifeExp   &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8…\n## $ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…\n## $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, …\n\ngapminder |&gt; \n  head()\n## # A tibble: 6 × 6\n##   country     continent  year lifeExp      pop gdpPercap\n##   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n## 1 Afghanistan Asia       1952    28.8  8425333      779.\n## 2 Afghanistan Asia       1957    30.3  9240934      821.\n## 3 Afghanistan Asia       1962    32.0 10267083      853.\n## 4 Afghanistan Asia       1967    34.0 11537966      836.\n## 5 Afghanistan Asia       1972    36.1 13079460      740.\n## 6 Afghanistan Asia       1977    38.4 14880372      786.\n\n\nEven an empty dataframe has a scheme:\n\nCodegapminder |&gt; \n  head(0) |&gt; \n  glimpse()\n\nRows: 0\nColumns: 6\n$ country   &lt;fct&gt; \n$ continent &lt;fct&gt; \n$ year      &lt;int&gt; \n$ lifeExp   &lt;dbl&gt; \n$ pop       &lt;int&gt; \n$ gdpPercap &lt;dbl&gt; \n\nCode# glimpse(head(gapminder, 0))\n\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\nThe schema of a dataframe/tibble is the list of column names and classes. The content of a dataframe is made of the rows. A dataframe may have null content\n\nCodegapminder |&gt; \n  filter(FALSE) |&gt; \n  glimpse()\n## Rows: 0\n## Columns: 6\n## $ country   &lt;fct&gt; \n## $ continent &lt;fct&gt; \n## $ year      &lt;int&gt; \n## $ lifeExp   &lt;dbl&gt; \n## $ pop       &lt;int&gt; \n## $ gdpPercap &lt;dbl&gt;"
  },
  {
    "objectID": "labs-solutions/lab-gapminder.html#get-a-feeling-of-the-dataset",
    "href": "labs-solutions/lab-gapminder.html#get-a-feeling-of-the-dataset",
    "title": "Data visualization",
    "section": "Get a feeling of the dataset",
    "text": "Get a feeling of the dataset\n\n\n\n\n\n\nQuestion\n\n\n\nPick two random rows for each continent using slice_sample()\n\n\n\n\n\n\n\n\nsolution\n\n\n\nTo pick a slice at random, we can use function slice_sample. We can even perform sampling within groups defined by the value of a column.\n\nCodegapminder |&gt; \n  slice_sample(n=2, by=continent)\n\n# A tibble: 10 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Mongolia    Asia       1957    45.2   882134      913.\n 2 Pakistan    Asia       1967    49.8 60641899      942.\n 3 Ireland     Europe     1992    75.5  3557761    17559.\n 4 Netherlands Europe     1967    73.8 12596822    15363.\n 5 Zimbabwe    Africa     1987    62.4  9216418      706.\n 6 Chad        Africa     1967    43.6  3495967     1197.\n 7 Canada      Americas   1962    71.3 18985849    13462.\n 8 Canada      Americas   1967    72.1 20819767    16077.\n 9 New Zealand Oceania    2002    79.1  3908037    23190.\n10 New Zealand Oceania    1977    72.2  3164900    16234.\n\nCode#&lt; or equivalently \n# gapminder |&gt; \n#   group_by(continent) |&gt; \n#   slice_sample(n=2)\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat makes a table tidy?\n\n\n\n\n\n\n\n\nTip\n\n\n\nHave a look at Data tidying in R for Data Science (2nd ed.)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs the gapminder table redundant?\n\n\n\n\n\n\n\n\nsolution\n\n\n\ngapminder is redundant: column country completely determines the content of column continent. In database parlance, we have a functional dependancy: country → continent whereas the key of the table is made of columns country, year.\nTable gapminder is not in Boyce-Codd Normal Form (BCNF), not even in Third Normal Form (3NF)."
  },
  {
    "objectID": "labs-solutions/lab-gapminder.html#gapminder-tibble-extract",
    "href": "labs-solutions/lab-gapminder.html#gapminder-tibble-extract",
    "title": "Data visualization",
    "section": "Gapminder tibble (extract)",
    "text": "Gapminder tibble (extract)\n\n\n\n\n\n\nQuestion\n\n\n\nExtract/filter a subset of rows using dplyr::filter(...)\n\nAll rows concerning a given country\nAll rows concerning a year\nAll rows concerning a given continnent and a year\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodegapminder |&gt; \n  filter(country=='France') |&gt; \n  head()\n\n# A tibble: 6 × 6\n  country continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 France  Europe     1952    67.4 42459667     7030.\n2 France  Europe     1957    68.9 44310863     8663.\n3 France  Europe     1962    70.5 47124000    10560.\n4 France  Europe     1967    71.6 49569000    13000.\n5 France  Europe     1972    72.4 51732000    16107.\n6 France  Europe     1977    73.8 53165019    18293.\n\n\n\n\n\n\n\n\n\n\nEquality testing is performed using ==, not = (which is used to implement assignment)"
  },
  {
    "objectID": "labs-solutions/lab-gapminder.html#filtering-selection-σ-from-database-theory-picking-one-year-of-data",
    "href": "labs-solutions/lab-gapminder.html#filtering-selection-σ-from-database-theory-picking-one-year-of-data",
    "title": "Data visualization",
    "section": "Filtering (selection \\(σ\\) from database theory) : Picking one year of data",
    "text": "Filtering (selection \\(σ\\) from database theory) : Picking one year of data\nThere is simple way to filter rows satisfying some condition. It consists in mimicking indexation in a matrix, leaving the colum index empty, replacing the row index by a condition statement (a logical expression) also called a mask.\n\nCodegapminder_2002 &lt;- gapminder[gapminder$year==2002, ]\n\n\nHave a look at gapminder$year==2002. What is the type/class of this expression?\nThis is possible in base R and very often convenient.\nNevertheless, this way of performing row filtering does not emphasize the connection between the dataframe and the condition. Any logical vector with the right length could be used as a mask. Moreover, this way of performing filtering is not very functional.\n\n\n\n\n\n\nIn the parlance of Relational Algebra, filter performs a selection of rows. Relational expression \\[σ_{\\text{condition}}(\\text{Table})\\] translates to\nfilter(Table, condition)\nwhere \\(\\text{condition}\\) is a boolean expression that can be evaluated on each row of \\(\\text{Table}\\). In SQL, the relational expression would translate into\nSELECT *\nFROM Table\nWHERE condition\nCheck Package dplyr docs\nThe posit cheatsheet on dplyr is an unvaluable resource for table manipulation.\n\n\n\nUse dplyr::filter() to perform row filtering\n\n\n\n\n\n\nsolution\n\n\n\n\nCode# filter(gapminder, year==2002)\n\ngapminder |&gt; \n  filter(year==2002)\n\n# A tibble: 142 × 6\n   country     continent  year lifeExp       pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;     &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       2002    42.1  25268405      727.\n 2 Albania     Europe     2002    75.7   3508512     4604.\n 3 Algeria     Africa     2002    71.0  31287142     5288.\n 4 Angola      Africa     2002    41.0  10866106     2773.\n 5 Argentina   Americas   2002    74.3  38331121     8798.\n 6 Australia   Oceania    2002    80.4  19546792    30688.\n 7 Austria     Europe     2002    79.0   8148312    32418.\n 8 Bahrain     Asia       2002    74.8    656397    23404.\n 9 Bangladesh  Asia       2002    62.0 135656790     1136.\n10 Belgium     Europe     2002    78.3  10311970    30486.\n# ℹ 132 more rows\n\n\n\n\n\n\n\n\n\n\nData masking\n\n\n\nNote that in stating the condition, we simply write year==2002 even though year is not the name of an object in our current session. This is possible because filter( ) uses data masking, year is meant to denote a column in gapminder.\nThe ability to use data masking is one of the great strengths of the R programming language."
  },
  {
    "objectID": "labs-solutions/lab-gapminder.html#static-plotting-first-attempt",
    "href": "labs-solutions/lab-gapminder.html#static-plotting-first-attempt",
    "title": "Data visualization",
    "section": "Static plotting: First attempt",
    "text": "Static plotting: First attempt\n\n\n\n\n\n\nQuestion\n\n\n\nDefine a plot with respect to gapminder_2002 along the lines suggested by Rosling’s presentation.\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodep &lt;- gapminder_2002 |&gt;\n  ggplot() \n\n\n\n\n\n\n\n\n\n\nYou should define a ggplot object with data layer gapminder_2022 and call this object p for further reuse.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nMap variables gdpPercap and lifeExp to axes x and y. Define the axes. In ggplot2 parlance, this is called aesthetic mapping. Use aes().\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodep &lt;- p +\n  aes(x=gdpPercap, y=lifeExp)\n\np \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse ggplot object p and add a global aesthetic mapping gdpPercap and lifeExp to axes x and y (using + from ggplot2) .\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nFor each row, draw a point at coordinates defined by the mapping. You need to add a geom_ layer to your ggplot object, in this case geom_point() will do.\n\n\n\n\n\n\n\n\nsolution\n\n\n\nWe add another layer to our graphical object.\n\nCodep &lt;- p +\n  geom_point()\n\np\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s up?\n\n\n\nWe are building a graphical object (a ggplot object) around a data frame (gapminder)\nWe supply aesthetic mappings (aes()) that can be either global or bound to some geometries (geom_point())or statistics\nThe global aesthetic mapping defines which columns are\n\nmapped to which axes,\npossibly mapped to colours, linetypes, shapes, …\n\nGeometries and Statistics describe the building blocks of graphics\n\n\nWhat’s missing here?\nwhen comparing to the Gapminder demonstration, we can spot that\n\ncolors are missing\nbubble sizes are all the same. They should reflect the population size of the country\ntitles and legends are missing. This means the graphic object is useless.\n\nWe will add other layers to the graphical object to complete the plot"
  },
  {
    "objectID": "labs-solutions/lab-gapminder.html#second-attempt-display-more-information",
    "href": "labs-solutions/lab-gapminder.html#second-attempt-display-more-information",
    "title": "Data visualization",
    "section": "Second attempt: display more information",
    "text": "Second attempt: display more information\n\n\n\n\n\n\nQuestion\n\n\n\n\nMap continent to color (use aes())\nMap pop to bubble size (use aes())\nMake point transparent by tuning alpha (inside geom_point() avoid overplotting)\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodep &lt;- p +\n  aes(color=continent, size=pop) +\n  geom_point(alpha=.5) \n\np\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\nIn this enrichment of the graphical object, guides have been automatically added for two aesthetics: color and size. Those two guides are deemed necessary since the reader has no way to guess the mapping from the five levels of continent to color (the color scale), and the reader needs help to connect population size and bubble size.\nggplot2 provides us with helpers to fine tune guides.\nThe scalings on the x and y axis do not deserve guides: the ticks along the coordinate axes provide enough information."
  },
  {
    "objectID": "labs-solutions/lab-gapminder.html#scaling",
    "href": "labs-solutions/lab-gapminder.html#scaling",
    "title": "Data visualization",
    "section": "Scaling",
    "text": "Scaling\nIn order to pay tribute to Hans Rosling, we need to take care of two scaling issues:\n\nthe gdp per capita axis should be logarithmic scale_x_log10()\n\nthe area of the point should be proportional to the population scale_size_area()\n\n\n\n\n\n\n\n\nComplete the graphical object accordingly\n\n\n\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodep &lt;- p +\n  scale_x_log10() +\n  scale_size_area()\n\np\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nMotivate the proposed scalings.\n\nWhy is it important to use logarithmic scaling for gdp per capita?\nWhen is it important to use logarithmic scaling on some axis (in other contexts)?\nWhy is it important to specify scale_size_area() ?\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodep +\n  scale_radius()\n\nScale for size is already present.\nAdding another scale for size, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\nWe use package patchwork to collect and present several graphical objects.\n\nCodeptchwrk &lt;- (p + ggtitle(\"scale_size_area\")) + (p + scale_size() + ggtitle(\"scale\")) \n\nScale for size is already present.\nAdding another scale for size, which will replace the existing scale.\n\nCodeptchwrk + plot_annotation(\n  title='Comparing scale_size_area and scale_size', \n  caption='In the current setting, scale_size_area() should be favored'\n)"
  },
  {
    "objectID": "labs-solutions/lab-gapminder.html#in-perspective",
    "href": "labs-solutions/lab-gapminder.html#in-perspective",
    "title": "Data visualization",
    "section": "In perspective",
    "text": "In perspective\n\n\n\n\n\n\nQuestion\n\n\n\n\nAdd a plot title\nMake axes titles\n\nexplicit\nreadable\n\n\nUse labs(...)\n\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodeyoi &lt;- 2002\n\np &lt;-  p + \n  labs(\n    title=glue('The world in year {yoi}'),\n    x=\"Gross Domestic Product per capita (US$ 2009, corrected for PPP)\",\n    y=\"Life expectancy at birth\"\n  )\n\np\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\nWe should also fine tune the guides: replace pop by Population and titlecase continent.\n\nCode# TODO: fine tune the guides: replace `pop` by `Population` and titlecase `continent`.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat should be the respective purposes of Title, Subtitle, Caption, … ?"
  },
  {
    "objectID": "labs-solutions/lab-gapminder.html#theming-using-ggthemes-or-not",
    "href": "labs-solutions/lab-gapminder.html#theming-using-ggthemes-or-not",
    "title": "Data visualization",
    "section": "Theming using ggthemes (or not)",
    "text": "Theming using ggthemes (or not)\n\nCodestopifnot(\n  require(\"ggthemes\")\n)\n\n\nA theme defines the look and feel of plots\nWithin a single document, we should use only one theme\nSee Getting the theme for a gallery of available themes\n\nCodep +\n  theme_economist()"
  },
  {
    "objectID": "labs-solutions/lab-gapminder.html#tuning-scales",
    "href": "labs-solutions/lab-gapminder.html#tuning-scales",
    "title": "Data visualization",
    "section": "Tuning scales",
    "text": "Tuning scales\n\n\n\n\n\n\nQuestion\n\n\n\nUse scale_color_manual(...) to fine tune the color aesthetic mapping.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nCode```{r}\n#| label: theme_scale\nneat_color_scale &lt;-\n      c(\"Africa\" = \"#01d4e5\",\n        \"Americas\" = \"#7dea01\" ,\n        \"Asia\" = \"#fc5173\",\n        \"Europe\" = \"#fde803\",\n        \"Oceania\" = \"#536227\")\n```\n\n\n\nCodep &lt;- p +\n  scale_size_area(max_size = 15) + #&lt;&lt;\n  scale_color_manual(values = neat_color_scale) #&lt;&lt;\n\nScale for size is already present.\nAdding another scale for size, which will replace the existing scale.\n\nCodep\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nChoosing a color scale is a difficult task\nviridis is often a good pick.\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\nMimnimalist themes are often a good pick.\n\nCodeold_theme &lt;- theme_set(theme_minimal())\n\n\n\nCodep &lt;- p +\n   scale_size_area(max_size = 15,\n                  labels= scales::label_number(scale=1/1e6,\n                                               suffix=\" M\")) +\n   scale_color_manual(values = neat_color_scale) +\n    labs(title= glue(\"Gapminder  {min(gapminder$year)}-{max(gapminder$year)}\"),\n         x = \"Yearly Income per Capita\",\n         y = \"Life Expectancy\",\n       caption=\"From sick  and poor (bottom left) to healthy and rich (top right)\")   \n\nScale for size is already present.\nAdding another scale for size, which will replace the existing scale.\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\nCodep + theme(legend.position = \"none\")"
  },
  {
    "objectID": "labs-solutions/lab-gapminder.html#zooming-on-a-continent",
    "href": "labs-solutions/lab-gapminder.html#zooming-on-a-continent",
    "title": "Data visualization",
    "section": "Zooming on a continent",
    "text": "Zooming on a continent\n\nCodezoom_continent &lt;- 'Europe'  # choose another continent at your convenience \n\n\n\n\n\n\n\n\nUse facet_zoom() from package ggforce\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodestopifnot(\n  require(\"ggforce\") #&lt;&lt;\n)\n\np_zoom_continent &lt;- p + \n  facet_zoom( #&lt;&lt;\n    xy= continent==zoom_continent, #&lt;&lt;\n    zoom.data= continent==zoom_continent #&lt;&lt;\n    ) #&lt;&lt;\n\np_zoom_continent"
  },
  {
    "objectID": "labs-solutions/lab-gapminder.html#adding-labels",
    "href": "labs-solutions/lab-gapminder.html#adding-labels",
    "title": "Data visualization",
    "section": "Adding labels",
    "text": "Adding labels\n\n\n\n\n\n\nQuestion\n\n\n\nAdd labels to points. This can be done by aesthetic mapping. Use aes(label=..)\nTo avoid text cluttering, package ggrepel offers interesting tools.\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodestopifnot(\nrequire(ggrepel) #&lt;&lt;\n)\n\np +\n   aes(label=country) + #&lt;&lt;\n   ggrepel::geom_label_repel(max.overlaps = 5) + #&lt;&lt;\n   scale_size_area(max_size = 15,\n                  labels= scales::label_number(scale=1/1e6,\n                                               suffix=\" M\")) +\n   scale_color_manual(values = neat_color_scale) +\n   theme(legend.position = \"none\") +\n    labs(title= glue(\"Gapminder  {min(gapminder$year)}-{max(gapminder$year)}\"),\n         x = \"Yearly Income per Capita\",\n         y = \"Life Expectancy\",\n       caption=\"From sick  and poor (bottom left) to healthy and rich (top right)\")\n\n\n\nGapminder 2002 layer by layer"
  },
  {
    "objectID": "labs-solutions/lab-gapminder.html#facetting",
    "href": "labs-solutions/lab-gapminder.html#facetting",
    "title": "Data visualization",
    "section": "Facetting",
    "text": "Facetting\nSo far we have only presented one year of data (2002)\nRosling used an animation to display the flow of time\nIf we have to deliver a printable report, we cannot rely on animation, but we can rely on facetting\nFacets are collections of small plots constructed in the same way on subsets of the data\n\n\n\n\n\n\nQuestion\n\n\n\nAdd a layer to the graphical object using facet_wrap()\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodep &lt;- p +\n  aes(text=country) +\n  guides(color = guide_legend(title = \"Continent\",\n                              override.aes = list(size = 5),\n                              order = 1),\n         size = guide_legend(title = \"Population\",\n                             order = 2)) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1)) +\n  facet_wrap(vars(year), ncol=6) +\n  ggtitle(\"Gapminder 1952-2007\")\n\np\n\n\n\n\n\n\n\n\n\n\nAs all rows in gapminder_2002 are all related to year 2002, we need to rebuild the graphical object along the same lines (using the same graphical pipeline) but starting from the whole gapminder dataset.\nShould we do this using cut and paste?\n No!!!\n\n\n\n\n\n\n\nDon’t Repeat Yoursel (DRY)\n\n\n\n\nAbide to the DRY principle using operator %+%: the ggplot2 object p can be fed with another dataframe and all you need is proper facetting.\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCodep %+% gapminder"
  },
  {
    "objectID": "labs-solutions/lab-gapminder.html#animate-for-free-with-plotly",
    "href": "labs-solutions/lab-gapminder.html#animate-for-free-with-plotly",
    "title": "Data visualization",
    "section": "Animate for free with plotly\n",
    "text": "Animate for free with plotly\n\n\n\n\n\n\n\nQuestion\n\n\n\nUse plotly::ggplotly() to create a Rosling like animation.\nUse frame aesthetics.\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCode```{r}\n#| label: animate\n#| eval: !expr knitr::is_html_output()\n#| code-annotations: hover\n\nq &lt;- filter(gapminder, FALSE) |&gt;\n   ggplot() +\n   aes(x = gdpPercap) +\n   aes(y = lifeExp) +\n   aes(size = pop) +\n   aes(text = country) +                   #\n   aes(fill = continent) +\n   # aes(frame = year) +                     #\n  geom_point(alpha=.5, colour='black') +\n  scale_x_log10() +\n  scale_size_area(max_size = 15,\n                  labels= scales::label_number(scale=1/1e6,\n                                               suffix=\" M\")) +\n  scale_fill_manual(values = neat_color_scale) +\n  theme(legend.position = \"none\") +\n  labs(title= glue(\"Gapminder  {min(gapminder$year)}-{max(gapminder$year)}\"),\n       x = \"Yearly Income per Capita\",\n       y = \"Life Expectancy\",\n       caption=\"From sick  and poor (bottom left) to healthy and rich (top right)\")\n\n\n(q %+% gapminder) |&gt;\n  plotly::ggplotly(height = 500, width=750)   \n```\n\n\n\n\n\n\n\ntext will be used while hovering\n\n\nframe is used by plotly to drive the animation. One frame per year\n\n\n\n\n\n\n\n\n\nsolution\n\n\n\n\nCode```{r}\n#| eval: !expr knitr::is_html_output()\n\n(p %+% gapminder +\n facet_null() +\n aes(frame=year)) |&gt;\n plotly::ggplotly(height = 500, width=750)\n```"
  },
  {
    "objectID": "labs-solutions/lab-gapminder.html#more-material",
    "href": "labs-solutions/lab-gapminder.html#more-material",
    "title": "Data visualization",
    "section": "More material",
    "text": "More material\n\nRead Visualization in R for Data Science"
  },
  {
    "objectID": "labs-solutions/lab-whiteside.html#packages-installation-and-loading-again",
    "href": "labs-solutions/lab-whiteside.html#packages-installation-and-loading-again",
    "title": "Linear regression, diagnostics, variable selection",
    "section": "Packages installation and loading (again)",
    "text": "Packages installation and loading (again)\nWe will use the following packages. If needed, we install them.\n\nCodestopifnot(\n  require(tidyverse), \n                  require(broom),\n                  require(magrittr),\n                  require(lobstr),\n                  require(ggforce),\n#                  require(cowplot),\n                  require(patchwork), \n                  require(glue),\n                  require(DT), \n                  require(viridis)\n)"
  },
  {
    "objectID": "quarto-format.html#a-translator",
    "href": "quarto-format.html#a-translator",
    "title": "Quarto",
    "section": "A translator",
    "text": "A translator",
    "crumbs": [
      "Support",
      "Quarto"
    ]
  },
  {
    "objectID": "quarto-format.html#quarto-and-rstudio",
    "href": "quarto-format.html#quarto-and-rstudio",
    "title": "Quarto",
    "section": "Quarto and rstudio",
    "text": "Quarto and rstudio",
    "crumbs": [
      "Support",
      "Quarto"
    ]
  },
  {
    "objectID": "quarto-format.html#quarto-and-vs-code",
    "href": "quarto-format.html#quarto-and-vs-code",
    "title": "Quarto",
    "section": "Quarto and vs code",
    "text": "Quarto and vs code",
    "crumbs": [
      "Support",
      "Quarto"
    ]
  },
  {
    "objectID": "quarto-format.html#command-line-tool",
    "href": "quarto-format.html#command-line-tool",
    "title": "Quarto",
    "section": "Command Line Tool",
    "text": "Command Line Tool",
    "crumbs": [
      "Support",
      "Quarto"
    ]
  }
]