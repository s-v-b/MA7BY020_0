---
date: "`r Sys.time()`"
title: "Bivariate analysis: simple linear regression"
categories: [Bivariate analysis, Simple Linear regression, Correlation]
 

execute:
  echo: true
  eval: true
  collapse: true

format:
  html:
    output-file: lab-slr.html
  pdf:
    output-file: lab-slr.pdf

params:
  truc: html
  year: 2024 
  curriculum: "M1 MIDS/MFA"
  university: "Université Paris Cité"
  homepage: "https://stephane-v-boucheron.fr/courses/scidon"
  moodle: "https://moodle.u-paris.fr/course/view.php?id=13227"
  
engine: knitr
---

```{r}
#| include: false
#| message: false
#| warning: false

stopifnot(
  require(tidyverse),
  require(patchwork),
  require(httr),
  require(glue),
  require(gssr)
)
old_theme <- theme_set(theme_minimal())
```

::: {layout="[80,20]"}
::: {#first-column}
-   **`r stringr::str_glue('{params$curriculum}')`**
-   **`r stringr::str_glue('[{params$university}](https://www.u-paris.fr)')`**
-   `r stringr::str_glue("Année {params$year}-{params$year+1}")`
-   `r stringr::str_glue("[Course Homepage]({params$homepage})")`\
-   `r stringr::str_glue("[Moodle]({params$moodle})")`
:::

::: {#second-column}
![](./IMG/UniversiteParis*monogramme_couleur*RVB.png){align="right" style="size:50px;" width="75"}
:::
:::


::: {.callout-important}

### Objectives

:::

# Quantitative bivariate samples and Simple linear regression

## Numerical summaries

The numerical summary of a numerical bivariate sample consists of an *empirical mean*

$$\begin{pmatrix}\overline{X}_n \\ \overline{Y}_n \end{pmatrix} = \frac{1}{n} \sum_{i=1}^n \begin{pmatrix} x_i \\ y_i \end{pmatrix}$$

and an empirical covariance matrix

$$\begin{pmatrix}\operatorname{var}_n(X) & \operatorname{cov}_n(X, Y) \\ \operatorname{cov}_n(X, Y) & \operatorname{var}_n(Y)\end{pmatrix}$$

with

$$\operatorname{var}_n(X, Y) = \frac{1}{n}\sum_{k=1}^n \Big(x_i-\overline{X}_n\Big)^2$$

and

$$\operatorname{cov}_n(X, Y) = \frac{1}{n}\sum_{k=1}^n \Big(x_i-\overline{X}_n\Big)\times \Big(y_i-\overline{Y}_n\Big)$$

## Covariance matrices have properties

The empirical covariance matrix is the *covariance matrix of the joint empirical distribution*.

As a covariance matrix, the empirical covariance matrix is *symmetric*, *semi-definite positive (SDP)*

::: callout-note

### Semi-definiteness

-   A square $n \times n$ matrix $A$ is semi-definite positive (SDP) iff

$$\forall u \in \mathbb{R}^n, \qquad  u^T \times A u = \langle u, Au \rangle \geq 0$$

-   A square $n \times n$ matrix $A$ is definite positive (DP) iff

$$\forall u \in \mathbb{R}^n \setminus \{0\}, \qquad  u^T \times A u = \langle u, Au \rangle > 0$$
:::

::: callout-note

### Linear correlation coefficient

The *linear correlation coefficient* is defined from the covariance matrix as

$$\rho = \frac{\operatorname{cov}_n(X, Y)}{\sqrt{\operatorname{var}_n(X)  \operatorname{var}_n(Y)}}$$
:::

{{< fa hand-point-right >}} By the Cauchy-Schwarz inequality, we always have

$$-1 \leq \rho \leq 1$$

{{< fa hand-point-right >}} Translating and/or rescaling the columns does not modify the linear correlation coefficient!

Functions `cov` and `cor` from base `r fontawesome::fa("r-project")` perform the computations

### Visualizing quantitative bivariate samples

Suppose now, we want to visualize a quantitative bivariate sample of length $n$.

This bivariate sample (a dataframe) may be handled as a *real matrix* with $n$ rows and $2$ columns

Geometric concepts come into play

### Exploring column space

We may attempt to visualize the two columns, that is the two $n$-dimensional vectors or the rows, that is $n$ points on the real plane.

{{< fa lightbulb >}} If we try to visualize the two columns, we simplify the problem by *projecting on the plane generated by the two columns*

Then what matters is the *angle* between the two vectors.

Its *cosine* is precisely the *linear correlation coefficient* defined above

### Exploring row space

If we try visualize the rows, the most basic visualization of a quantitative bivariate sample is the *scatterplot*.

In the grammar of graphics parlance, it consists in mapping the two variables on the two axes, and mapping rows to points using `geom_point` and `stat_identity`

### A Gaussian cloud

We build an artificial bivariate sample, by first building a covariance matrix `COV` (it is randomly generated). Then we build a bivariate normal sample `s` of length `n` and turn it into a dataframe `u`. The dataframe is then fed to `ggplot`.

```{r}
set.seed(1515) # for the sake of reproducibility

n <- 100
V <- matrix(rnorm(4, 1, 1), nrow = 2)
COV <- V %*% t(V)         # a random covariance matrix, COV is symmetric and SDP

s <- t(V %*% matrix(rnorm(2 * 10 * n), ncol=10*n))
u <- as_tibble(list(X=s[,1], Y=s[, 2]))                      # a bivariate normal sample

emp_mean <- as_tibble(t(colMeans(u)))
```

### Numerical summary

-   Mean vector (Empirical mean)

```{r}
t(colMeans(u)) %>%
  knitr::kable(digits = 3, col.names = c("$\\overline{X_n}$", "$\\overline{Y_n}$"))
```

-   Covariance matrix (Empirical covariance matrix)

```{r}
cov(u) %>% as.data.frame() %>% knitr::kable(digits = 3)

```

### Code

```{r gaussiancloud}
p_scatter_gaussian <- u %>%
  ggplot() +
  aes(x = X, y = Y) +
  geom_point(alpha = .25, size = 1) +
  geom_point(data = emp_mean, color = 2, size = 5) +
  stat_ellipse(type = "norm", level = .9) +
  stat_ellipse(type = "norm", level = .5) +
  stat_ellipse(type = "norm", level = .95) +
  annotate(geom="text", x=emp_mean$X+1.5, y= emp_mean$Y+1, label="Empirical mean")+
  geom_segment(aes(x=emp_mean$X, y=emp_mean$Y, xend=emp_mean$X+1.5, yend=emp_mean$Y+1)) +
  coord_fixed() +
  ggtitle(stringr::str_c("Gaussian cloud, cor = ",
    round(cor(u$X, u$Y), 2),
    sep = ""
  ))

p_scatter_gaussian
```

## Qualitative and quantitative variables

### Conditional summaries

For each modality $i \in \mathcal{X}$, we define:

-   Conditional Mean of $X$ given $\{ X = i \}$

$$\overline{Y}_{n\mid i}  = \frac{1}{n_i} \sum_{k\leq n} \mathbb{I}_{x_k =i} \times  y_k$$

-   Conditional Variance $Y$ given $\{ X= i\}$

$$\sigma^2_{Y\mid i}  = \frac{1}{n_i} \sum_{k \leq n}  \mathbb{I}_{x_k =i} \times \bigg( y_k  - \overline{Y}_{n \mid i}\bigg)^2$$

### Huygens-Pythagoras formula

$$\sigma^2_{Y} =  \underbrace{\sum_{i\in \mathcal{X}} \frac{n_i}{n} \sigma^2_{Y \mid i}}_{\text{mean of conditional variances}}  + \underbrace{\sum_{i\in \mathcal{X}} \frac{n_i}{n} \big(\overline{Y}_{n \mid i} - \overline{Y}_{n}\big)^2}_{\text{variance of conditional means}}$$

{{< fa brain >}} Check this

### Robust bivariate summaries

It is also possible and fruitful to compute

-   conditional quantiles (median, quartiles) and
-   conditional interquartile ranges (IQR)

Conditional mean, variance, median, IQR (`r fontawesome::fa("database")`)

```{r}
tit <-  readr::read_csv("../DATA/titanic/train.csv")

tit <-  tit |>
    mutate(across(c(Survived, Pclass, Name, Sex, Embarked), as.factor)) 

tit |>  
  glimpse()
```

```{r}
tit %>%
  dplyr::select(Survived, Fare) %>%
  dplyr::group_by(Survived) %>%
  dplyr::summarise(cmean=mean(Fare, na.rm=TRUE), #<<
                   csd=sd(Fare,na.rm = TRUE),
                   cmedian=median(Fare, na.rm = TRUE),
                   cIQR=IQR(Fare,na.rm = TRUE))
```

### Visualization of mixed bivariate samples

Visualization of qualitative/quantitative bivariate samples

consists in displaying visual summaries of conditional distribution of $Y$ given $X=i, i \in \mathcal{X}$

`Boxplots` and `violinplots` are relevant here

### Mixed bivariate samples from Titanic (violine plots)

```{r}
filtered_tit <- tit %>%
  dplyr::select(Pclass, Survived, Fare) %>%
  dplyr::filter(Fare > 0 )

v <- filtered_tit %>%
  ggplot() +
  aes(y=Fare) +
  scale_y_log10()

# vv <- v + geom_violin()

filtered_tit |> 
  glimpse()
```

```{r}
p <- v +
  aes(x=Pclass) + 
  geom_violin() +
  ggtitle("Titanic: Fare versus Passenger Class")

q <- v +
  aes(x=Survived) +
  geom_violin() +
  ggtitle("Titanic: Fare versus Survival")

p + q
```

### Mixed bivariate samples from Titanic (boxplots)

```{r}
(
v + aes(x=Pclass) +
  geom_boxplot() +
  ggtitle("Titanic: Fare versus Passenger Class")
) + (
v +
  aes(x=Survived) +
  geom_boxplot() +
  ggtitle("Titanic: Fare versus Survival")
)
```

### Dataset `whiteside` (from package `MASS` of {{< fa brands r-project >}})

> Mr Derek Whiteside of the UK Building Research Station recorded the weekly gas consumption and average external temperature at his own house in south-east England for two heating seasons, one of 26 weeks before, and one of 30 weeks after cavity-wall insulation was installed. The object of the exercise was to assess the effect of the insulation on gas consumption.

### Dataset `whiteside`

`Gas` and `Temp` are both quantitative variables while `Insul` is qualitative with two modalities (`Before`, `After`).

`Insul`

:   A factor, before or after insulation.

`Temp`

:   Purportedly the average outside temperature in degrees Celsius. (These values is far too low for any 56-week period in the 1960s in South-East England. It might be the weekly average of daily minima.)

`Gas`

:   The weekly gas consumption in 1000s of cubic feet.

```{r}
MASS::whiteside %>%
  ggplot(aes(x=Insul, y=Temp)) +
  geom_violin() +
  ggtitle("Whiteside data: violinplots")
```

## Simple linear regression

-   We now explore association between *two* quantitative variables

-   We investigate the association between two quantitative variables as a *prediction* problem

-   We aim at predicting the value of $Y$ as a function of $X$.

-   We restrict our attention to linear/affine prediction.

We look for $a, b \in \mathbb{R}$ such that $y_i \approx a x_i +b$

Making $\approx$ meaningful compels us to choose a *goodness of fit* criterion.

Several criteria are possible, for example:

$$
\begin{array}{rl}
\text{Mean absolute deviation} & = \frac{1}{n}\sum_{i=1}^n \big|y_i - a x_i -b \big| \\ \text{Mean quadratic deviation} & = \frac{1}{n}\sum_{i=1}^n \big|y_i - a x_i -b \big|^2 
\end{array}
$$

In their days, Laplace championed the mean absolute deviation, while Gauss advocated the mean quadratic deviation. For computational reasons, we focus on minimizing the mean quadratic deviation.

::: {layout="[50,50]"}

::: column

> The fourth chapter of Laplace treatise includes an exposition of the *method of least squares*, a remarkable testimony to Laplace's command over the processes of analysis.

> In 1805 Legendre had published the *method of least squares*, making no attempt to tie it to the theory of probability.
:::

::: column



> In 1809 Gauss had derived the *normal distribution* from the principle that the arithmetic mean of observations gives the most probable value for the quantity measured; then, turning this argument back upon itself, he showed that, if the errors of observation are normally distributed, the *least squares estimates* give the most probable values for the coefficients in *regression* situations

:::

:::



## Least Square Regression

### Minimizing a cost function

The *Least Square Regression* problem consists of minimizing with respect to $(a,b)$ :

$$
\begin{array}{rl} \ell_n(a,b)  & = \sum_{i=1}^n \big(y_i - a x_i -b \big)^2  \\ & = \sum_{i=1}^n \big((y_i - \overline{Y}_n) - a (x_i - \overline{X}_n) + \overline{Y}_n - a \overline{X}_n-b \big)^2 \\ & = \sum_{i=1}^n \big((y_i - \overline{Y}_n) - a (x_i - \overline{X}_n) \big)^2 + n \big(\overline{Y}_n - a \overline{X}_n-b\big)^2 
\end{array}
$$

### Deriving the solution

The function to be minimized is smooth and strictly convex over $\mathbb{R}^2$ : a unique minimum is attained where the gradient vanishes

It is enough to compute the partial derivatives.

$$\begin{array}{rl}\frac{\partial \ell_n}{\partial a} & = - 2  \operatorname{cov}(X,Y) + 2 a \operatorname{var}(X) -2 n \big(\overline{Y}_n - a \overline{X}_n-b\big) \overline{X}_n \\  \frac{\partial \ell_n}{\partial b} & = -2 n \big(\overline{Y}_n - a \overline{X}_n-b\big)\end{array}$$

### A closed-form solution

Zeroing partial derivatives leads to

$$
\begin{array}{rl}
  \widehat{a} & = \frac{\operatorname{cov}(X,Y)}{\operatorname{var}(X)} \\
  \widehat{b} & = \overline{Y}_n - \frac{\operatorname{cov}(X,Y)}{\operatorname{var}(X)} \overline{X}_n
\end{array}
$$

or

$$
\begin{array}{rl}
  \widehat{a} & = \rho \frac{\sigma_y}{\sigma_x} \\
  \widehat{b} & = \overline{Y}_n - \rho\frac{\sigma_y}{\sigma_x} \overline{X}_n
\end{array}
$$

{{< fa hand-point-right >}} If the sample were standardized, that is, if $X$ (resp. $Y$) were divided by $\sigma_X$ (resp. $\sigma_Y$), the slope of the regression line would be the correlation coefficient

### Overplotting the Gaussian cloud

-   The *slope* and *intercept* can be computed from the sample summary (empirical mean and covariance matrix)

-   In higher dimension, coefficients are from `lm(...)`

```{r}
p_scatter_gaussian +
  geom_smooth(method="lm", se=FALSE)

```

### `lm(formula, data)`

```{r}
mod <- lm(formula=Y ~ X, data=u)

mod %>% summary()
```

```{r}
sqrt(sum((mod$residuals)^2)/(mod$df.residual))
cor(u)^2
```

### Residuals

The *residuals* are the prediction errors $\left(y_i - \widehat{a}x_i - \widehat{b}\right)_{i\leq n}$

Residuals play a central role in *regression diagnostic*

The `Residual Standard Error`, is the square root of the normalized sum of squared residuals:

$$\frac{1}{n-2}\sum_{i=1}^n \left(y_i - \widehat{a}x_i - \widehat{b}\right)^2$$

The normalization coefficient is the number of rows $n$ diminished by the number of adjusted parameters (the so-called *degrees of freedom*)

```{r}
sqrt(sum((mod$residuals)^2)/(mod$df.residual))
```

{{< fa hand-point-right >}} This makes sense if we adopt a modeling perspective, if we accept the *Gaussian Linear Models* assumptions from the Statistical Inference course

```{r scatplot-residuals}
p_scatter_gaussian %+%
  broom::augment(lm(Y ~ X, u)) +  #<<
  geom_line(aes(x=X, y=.fitted)) +
  geom_segment(aes(x=X, xend=X, y=.fitted, yend=Y,
                   color=forcats::as_factor(sign(.resid))),
               alpha=.2) +
  theme(legend.position = "None") +
  ggtitle("Gaussian cloud",subtitle = "with residuals")
```

The residuals are the lengths of the segments connecting sample points to their projections on the regression line

Technically, the `Multiple R-squared` or

:   *coefficient of determination* is the squared empirical correlation coefficient $\rho^2$ between the explanatory and the response variables (in simple linear regression)

$$1 - \frac{\sum_{i=1}^n \left(y_i - \widehat{a}x_i - \widehat{b}\right)^2}{\sum_{i=1}^n \left(y_i - \overline{Y}_n\right)^2}= 1 - \frac{\sum_{i=1}^n \left(y_i - \widehat{y}_i \right)^2}{\sum_{i=1}^n \left(y_i - \overline{Y}_n\right)^2}$$

```{r}
cor(u$X, u$Y)^2

(sum(u$X*u$Y)/nrow(u) - mean(u$X)* mean(u$Y))*(nrow(u)/(nrow(u)-1))
((988/999)*cov(u$X, u$Y)/sqrt(var(u$X)*var(u$Y)))^2
```

It is also understood as the share of the variance of the response variable that is *explained* by the explanatory variable

The `Adjusted R-squared` is a deflated version of `Multiple R-squared`

$$1 - \frac{\sum_{i=1}^n \left(y_i - \widehat{a}x_i - \widehat{b}\right)^2/(n-p-1)}{\sum_{i=1}^n \left(y_i - \overline{Y}_n\right)^2/(n-1)}$$

It is useful when comparing the merits of several competing models (this takes us beyond the scope of this lesson)

<div>

```{r flip-scatplot-residuals}
p_scatter_gaussian %+%
  broom::augment(lm(Y ~ X, u)) +  #<<
  geom_line(aes(x=X, y=.fitted)) +
  geom_segment(aes(x=X,
                   xend=X,
                   y=.fitted,
                   yend=Y,
                   color=as_factor(sign(.resid))),
               alpha=.2) +
  theme(legend.position = "None") +
  ggtitle("Gaussian cloud",
          subtitle = "with residuals!")
```

</div>

## $y = x^T \beta + \sigma \epsilon$ : The biggest lie?

::: callout-caution

-   Any numeric bivariate sample can be fed to `lm`

-   Whatever the bivariate dataset, you will obtain a linear prediction model

-   It is not wise to rely exclusively on the `Multiple R-squared` to assess a linear model

-   {{< fa hand-point-right >}} Different datasets can lead to the same regression line and the same `Multiple R-squared` and the same `Adjusted R-squared`
   
:::

### Anscombe quartet

4 simple linear regression problems packaged in dataframe `datasets::anscombe`

-   `y1 ~ x1`
-   `y2 ~ x2`
-   `y3 ~ x3`
-   `y4 ~ x4`

```{r}
anscombe <- datasets::anscombe

anscombe %>% 
    gt::gt()
```

### Anscombe quartet: 4 datasets, 1 linear fit with almost identical goodness of fits

```{r}
lm(y1 ~ x1, anscombe) %>% summary

lm(y2 ~ x2, anscombe) %>% summary

lm(y3 ~ x3, anscombe) %>% summary

lm(y4 ~ x4, anscombe) %>% summary
```

All four numerical summaries look similar:

-   `Intercept` $\approx 3.0017$
-   `slope` $\approx 0.5$
-   Residual standard error $\approx 1.236$
-   Multiple R-squared $\approx .67$
-   F-statistic $\approx 18$

$n$ is equal to 11

The number of adjusted parameters $p$ is 2 The number of degrees of freedom is $n-p=9$

How is RSE computed ?

$$\frac{1}{n-p}\sum_{i=1}^n \left(y_j[i] - \widehat{y}_j[i] \right)^2$$

::: {.callout-important}

Visual inspection of the data reveals that some linear models are more relevant than others

:::

This is the message of the Anscombe quartet.

It is made of four bivariate samples with $n=11$ individuals.

```{r}
datasets::anscombe %>%
  pivot_longer(everything(),  #<<
    names_to = c(".value", "group"), #<<
    names_pattern = "(.)(.)" #<<
  )  %>%
  rename(X=x, Y=y) %>%
  arrange(group)-> anscombe_long
```

From <https://tidyr.tidyverse.org/articles/pivot.html>

### Performing regression per group

For each value of `group` we perform a linear regression of `Y` versus `X`

```{r}
list_lm <- purrr::map(anscombe_long$group ,
                      .f = \(x) lm(Y ~ X,
                                   anscombe_long,
                                   subset = anscombe_long$group==x))
```

{{< fa magic >}} Don't Repeat Yourself (DRY)

We use *functional programming*: `purrr::map(.l, .f)` where

-   `.l` is a list

-   `.f` is a function to be applied to each item of list `.l` or a `formula` to be evaluated on each list item

[`purrr` package](https://purrr.tidyverse.org/reference/map.html)

### Inspecting summaries

All four regressions lead to the same intercept and the same slope

All four regressions have the same Sum of Squared Residuals

All four regressions have the same Adjusted R-square

We are tempted to conclude that

> all four linear regressions are equally relevant

Plotting points and lines helps dispel this illusion

### Unveiling points



```{r, warning=FALSE}
p <- anscombe_long %>%
  ggplot(aes(x=X, y=Y)) +
  geom_smooth(method="lm", se=FALSE) +  #<<
  facet_wrap(~ group) +                 #<<
  ggtitle("Anscombe quartet: linear regression Y ~ X")

(p + (p + geom_point()))
```

Among the four datasets, only the two left ones are righteously handled using simple linear regression

The bottom left dataset outlines the impact of *outliers* on Least Squares Minimization


## Regression on the Whiteside data 


```{r}
whiteside <- MASS::whiteside
lm0 <- whiteside %>% 
  lm(Gas ~ Temp, .)
```

```{r}
lm0 |> 
  broom::tidy() |>
  gt::gt() |>
  gt::fmt_number(
    columns = -term,
    decimals=2
  )
```

```{r}
p <- lm0 |> 
  broom::augment(data=whiteside) |>
  ggplot() +
  aes(x=Temp, y=Gas) +
  geom_point(aes(shape=Insul)) 
  
p +
  geom_smooth(
    formula = y ~ x,
    method="lm",
    se=FALSE
    ) +
  ggtitle("Gas ~ Temp, whiteside data")
```


```{r}
lm_before <- whiteside %>% 
  filter(Insul=="Before") %>% 
  lm(Gas ~ Temp, .)

lm_after <- whiteside %>% 
  filter(Insul=="After") %>% 
  lm(Gas ~ Temp, .)
```

```{r}
p +
  geom_smooth(
    formula = y ~ x,
    method="lm",
    se=FALSE,
    color="red",
    ) +
  geom_abline(
    intercept=coefficients(lm_before)[1],
    slope=coefficients(lm_before)[2],
    color='blue'
  ) +
  geom_abline(
    intercept=coefficients(lm_after)[1],
    slope=coefficients(lm_after)[2],
    color='blue'
  ) +
  labs(
    title="Gas ~ Temp, whiteside data",
    subtitle="Regressions per group in blue"
    )
```

::: {.callout-note}

### Questions

- Which regression should we trust?
- Can we build confidence interval for estimated coefficients?
- Can we estimate noise intensity?
- Can we trust the homoschedasticity assumption?
- Can we trust 

:::

### Using diagnostic plots

```{r}
require(ggfortify)

autoplot(lm0, data=whiteside, shape='Insul')
```


```{r}
autoplot(lm_before)
```



```{r}
autoplot(lm_after)
```